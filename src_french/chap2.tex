Dans ce chapitre, nous proposons
l'étude des modèles $\pi$-PDMOM 
dans le but de résoudre 
de très grands problèmes de planification
lorsqu'ils sont structurés.
Inspirés par l'algorithme \textit{Stochastic Planning Using Decision Diagrams} (\textit{SPUDD})
construit pour résoudre les PDM probabilistes factorisés, 
nous avons mis en place un algorithme symbolique
appelé PPUDD conçu pour résoudre les $\pi$-PDMOM. 
Tandis que le nombre de feuilles
des arbres de décision utilisés par \textit{SPUDD}
peuvent devenir aussi grand que la taille de l'espace des états
puisque leurs valeurs sont des nombres réels 
agrégés par des additions et des multiplications,
le nombre de feuilles de \textit{PPUDD} 
est borné par le nombre d'éléments dans $\mathcal{L}$
car leurs valeurs restent dans l'échelle finie $\mathcal{L}$
via les opérations $\min$ et $\max$ seulement.
Enfin, nous présentons un $\pi$-PDMOM
satisfaisant certaines hypothèses d'indépendance 
sur les variables visibles, cachées,
et d'observation.
Ce dernier résulte en un $\pi$-PDM factorisé,
sur lequel \textit{PPUDD} peut être lancé.
Nos résultats expérimentaux montrent que 
le temps de calcul de \textit{PPUDD} 
est beaucoup plus petit que 
\textit{Symbolic-HSVI} et \textit{APPL}
pour les versions possibilistes 
et probabilistes du même benchmark, 
tout en fournissant des stratégies de bonne qualité.
Les performances des stratégies
calculées par \textit{PPUDD} ont été testées
pendant la compétition internationale de planification probabiliste (\textit{IPPC} 2014)
dont les résultats sont exposés ici.

\section{Introduction}
Les travaux sur les $\pi$-PDM(MO)
présentés précédemment ne tirent pas totalement avantage
de la structure du problème, \textit{i.e.}
les parties visibles ou cachées de l'état peuvent être elles-même
factorisées en plusieurs variables d'états. 
Dans le cadre probabiliste, 
les PDM factorisés et les méthodes de calcul symboliques 
\cite{BoutilierDG00,Hoey99spuddstochastic} ont été étudiés
intensivement dans le but de raisonner directement au niveau des variables
plutôt que sur l'espace d'état.
Un travail récent sur ces problématiques est par exemple \cite{RadoszyckiPS14}. 
Le célèbre algorithme \textit{SPUDD}
\cite{Hoey99spuddstochastic} 
résout des PDM factorisés en utilisant
des représentations symboliques de la fonction valeur et des stratégies
sous la forme d'arbres de décision algébriques (\textit{ADDs}) \cite{Bahar1997ADD}, 
qui représentent de manière compacte
les fonctions réelles de variables booléennes:
les \textit{ADDs} sont des arbres 
dont les noeuds représentent les variables d'état
et les feuilles sont les valeurs de la fonction. 
Au lieu de mettre à jour les valeurs de la fonction pour chaque état
individuellement à chaque itération de l'algorithme,
ils sont aggrégés dans les \textit{ADDs}
et les opérations sont effectuées de manière symbolique
directement entre les \textit{ADDs} sur plusieurs variables en même temps.
Cependant, \textit{SPUDD} est limité par la manipulation potentielle
d'énormes \textit{ADDs} dans le pire des cas: par exemple,
l'espérance implique des additions et des multiplications sur des valeurs réelles
(probabilités et récompenses), 
créant de nouvelles valeurs entres elles,
de manière à ce que le nombre de feuille des \textit{ADDs}
puisse devenir égal à l'espace d'état,
\textit{i.e.} exponentiel en le nombre de variables d'état.

Ainsi,
le travail présenté ici
est motivé par la simple observation
que les \textbf{opérations symboliques avec des PDM possibilistes
devraient nécessairement limiter la taille des \textit{ADDs}}: 
en effet, ce formalisme opère sur une échelle \emph{finie}
$\mathcal{L}$ avec seulement les opérations $\max$ et $\min$,
ce qui implique que les valeurs manipulées restent dans
l'échelle finie $\mathcal{L}$, 
qui est généralement beaucoup plus petite
que le nombre d'états.
\begin{figure} \centering
\begin{tikzpicture}
\begin{axis}[grid=major,xmax=100,
legend entries={ $8$ variables: cadre qualitatif, cadre quantitatif,  
$10$ variables: cadre qualitatif, cadre quantitatif},legend style={at={(2,0.6)}},
xlabel={Taille de l'échelle $\mathcal{L}$},
ylabel={Nombre maximal de noeuds},
title={Nombre maximal de noeud d'un \textit{ADD}: feuilles dans $\mathcal{L}$ vs dans $\mathbb{R}$}]
\addplot table[x=scaleSize, y=MaxNbNodesPoss]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_8VARS.txt};
\addplot table[x=scaleSize, y=MaxNbNodes]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_8VARS.txt};
\addplot table[x=scaleSize,y=MaxNbNodesPoss]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_10VARS.txt};
\addplot table[x=scaleSize,y=MaxNbNodes]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_10VARS.txt};
\end{axis}
\end{tikzpicture}
\caption[Limitations de la taille maximale d'un \textit{ADD} dans le cadre qualitatif]{
La taille maximale (nombre total de noeuds)
d'un \textit{ADD} dont les valeurs sont dans $\mathcal{L}$ est limité:
cette taille maximale est représentée par les courbes avec les ronds bleus et marrons,
en fonction de la taille de $\mathcal{L}$.
Lorsque les feuilles des \textit{ADDs} sont dans $\mathbb{R}$,
le nombre de ses noeuds est potentiellement exponentiel
en le nombre de variables:
la borne supérieure est représentée par les courbes
avec les carrés rouges et noirs
(fonctions constantes de la taille de $\mathcal{L}$).}
\label{ADDsize}
\end{figure}

La figure \ref{ADDsize} montre que
les \textit{ADDs} utilisés dans le cadre possibiliste
a un nombre limité de noeuds
puisque le nombre de feuilles
est au plus égal à la taille de
l'échelle possibiliste qualitative $\mathcal{L}$:
%which is generally far smaller than the number of states.  
%Figure \ref{ADDsize} shows 
la taille maximale (
nombre maximal de noeuds)
d'un \textit{ADD}
dont les feuilles sont dans $\mathcal{L}$, 
est représenté comme une fonction
de $\# \mathcal{L}$,
dans le cas de $8$ et $10$ variables. 

Dans ce chapitre, nous présentons
un algorithme basé sur la programmation dynamique symbolique
pour résoudre les $\pi$-PDMOM factorisés
appelé Possibilistic Planning Using
Decision Diagram (\textit{PPUDD}). 
Cette contribution seule
n'est pas suffisante
puisque les variables de croyance
ont un nombre de valeurs
exponentiel en la taille de l'espace des états cachés.
Donc, notre seconde contribution est un théorème 
visant à factoriser l'état de croyance
en de nombreuses variables de croyance marginales
lorsque certaines hypothèses d'indépendance 
sur les variables d'état et d'observation
d'un $\pi$-PDMOM sont vérifiées:
cela permet de résoudre certains problèmes
dont les calculs sont inabordable. 
Notons que notre idée de factorisation
de l'état de croyance est assez général
pour être valable pour les modèles probabilistes.
Enfin, les performances de \textit{PPUDD} sont comparées à celles de 
\textit{symbolic HSVI} \cite{Sim2008SHS1620163.1620241}
(une version symbolique de l'algorithme pour PDMPO appelé \textit{HSVI} \cite{Smith2004HSV1036843.1036906}) 
et \textit{APPL} \cite{Kurniawati-RSS08,OngShaoHsuWee-IJRR10}
(déjà utilisé dans le chapitre précédent, 
et basé sur \textit{SARSOP})
sous observabilité mixte.
Les résultats obtenus étant prometteurs,
nous avons participé à la compétition internationale de planification probabiliste (\textit{IPPC} 2014)
et les résultats de \textit{PPUDD} durant la session entièrement observable d'\textit{IPPC} 2014
sont présentés et discutés.
Un algorithme dédié à la résolution des $\pi$-PDMOM en utilisant des \textit{ADDs}
est disponible dans le dépôt \url{https://github.com/drougui/ppudd}).

\section{Résoudre des $\pi$-PDMOM par la Programmation Dynamique Symbolique} 
\label{section_PPUDD}
Les PDM factorisés \cite{Hoey99spuddstochastic} 
ont été utilisés pour résoudre plus rapidement les
problèmes structurés de décision séquentielle, 
sous incertitude probabiliste, 
en raisonnant symboliquement sur les fonctions
des états du systèmes, à travers des arbres de décision algébriques. 
Inspiré par ce travail,
cette section présente la résolution symbolique
des $\pi$-PDMOM factorisés:
dans ce modèle, l'espace des états visibles $\mathcal{S}_v$, 
l'espace des états cachés $\mathcal{S}_h$
et l'ensemble des observations $\mathcal{O}_h$
sont tels que l'espace d'état du $\pi$-PDM résultant 
(basé sur l'espace des états de croyances et l'espace des variables visibles)
est sous la forme 
$\mathcal{S}^1_v \times \cdots \times \mathcal{S}^m_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$, 
où chacun de ces espaces sont finis.
Nous verrons dans la section suivante comment $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
peut être factorisé de cette manière
grâce à la factorisation de $\mathcal{S}_h$
et $\mathcal{O}_h$. 
La factorisation de la variable de la croyance probabiliste
dans \cite{BoyenK99,ShaniPBS08} 
est approximative, tandis que celle présentée ici est exacte.
Puisque les espaces finis de taille $K$
peuvent être eux-même factorisés
en $\lceil \log_2 K \rceil$ espaces binaires \cite{Hoey99spuddstochastic}, 
nous pouvons faire l'hypothèse que nous raisonnons 
sur un $\pi$-PDM dont l'espace d'état est noté $\mathcal{X}$
et entièrement décrit par les variables $(X^1,\ldots,X^n)$, 
avec $n \in \mathbb{N}^{\ast}$ et $\forall i$, $X^i \in \set{\top,\bot}$:
$\mathcal{X} = \set{\top,\bot}^n$.

Rappelons que les réseaux bayésiens dynamiques (\textit{DBNs}) \cite{Dean1989DBN}
déjà utilisés en section \ref{section_piPOMDP} 
(dans le diagramme d'influence figure \ref{POMDP})
et dans le chapitre précédent (figure \ref{piMOMDP} 
illustrant la structure d'observabilité mixte)
sont des représentations graphique très utiles pour les processus étudiés. 
Un \textit{DBN} représentant la structure d'un $\pi$-PDM factorisé
est dessiné dans la figure \ref{fig_piMDPFact}:
les variables d'état à une étape de temps donnée $t \geqslant 0$
sont notées $X_t = (X_t^i)_{i=1}^n$ (variables courantes),
et $(X^i_{t+1})_{i=1}^n$ sont les variables d'état à l'étape de temps $t+1$ (variable suivante).
\begin{figure}\centering
\begin{tikzpicture}[scale=1.5,transform shape]
%% vertex shape and color
\tikzstyle{vertex}=[circle,fill=black!30,minimum size=27pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red!60,minimum size=22pt,inner sep=0pt,draw=black,thick]
\tikzstyle{rvertex}=[fill=yellow!60,decision=3,inner sep=-1pt,minimum size=35pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=35pt,inner sep=0pt,draw=black,thick]
%TIME
%\node [font=\huge] (statet) at (4,0.2) {$t$};
%\node [font=\huge] (statetplus1) at (9.2,0.2) {$t+1$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1
\node[vertex] (state1) at (3.3,3) {$X^1_t$};
\node[vertex] (state12) at (3.3,1) {$X^2_t$};
\node (state13) at (3.3,0.2) {\begin{Large} $\vdots$ \end{Large}};
%2
\node[vertex] (state2) at (8,3) {$X^1_{t+1}$};
\node[vertex] (state22) at (8,1) {$X^2_{t+1}$};
\node (state23) at (8,0.2) {\begin{Large} $\vdots$ \end{Large}};

%0
\node (state0) at (0.5,3) {};
\node (state02) at (0.5,1) {};
%3
\node (state3) at (11,3) {};
\node (state32) at (11,1) {};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%action
\node[avertex] (action) at (5.7,-1) {$a_t$};
\node[avertex] (action0) at (1.3,-1) {$a_{t-1}$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS

%SV
% arrows sv->sv
%1->2
\draw[->,>=latex] (state1) -- (state2);
\draw[->,>=latex] (state1) -- (state22);
\draw[->,>=latex] (state12) -- (state2);
\draw[->,>=latex] (state12) -- (state22);
%0->1
\draw[->,>=latex,dashed] (state0) -- (state1);
\draw[->,>=latex,dashed] (state0) -- (state12);
\draw[->,>=latex,dashed] (state02) -- (state1);
\draw[->,>=latex,dashed] (state02) -- (state12);
%2->3
\draw[->,>=latex,dashed] (state2) -- (state3);
\draw[->,>=latex,dashed] (state2) -- (state32);
\draw[->,>=latex,dashed] (state22) -- (state3);
\draw[->,>=latex,dashed] (state22) -- (state32);

%A
% a->s
%1
\draw[->,>=latex] (action) -- (state2);
\draw[->,>=latex] (action) -- (state22);
%0
\draw[->,>=latex,dashed] (action0) -- (state1);
\draw[->,>=latex,dashed] (action0) -- (state12);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRANSITION FUNCTIONS
\node (T1) at (6.4,2.7) {$T^1_a$};
\node (T2) at (6.4,1.3) {$T^2_a$};
\end{tikzpicture}
\caption[Réseau bayésien dynamique d'un ($\pi$-)PDM factorisé]
{Réseau dynamique bayésien pour des ($\pi$-)PDM:
dans le cadre possibiliste (resp. probabiliste) 
$T^i_a$ est la distribution de possibilité (resp. probabilité) de transition
sur la variable d'état $X^i_{t+1}$
conditionnellement à
l'action sélectionnée $a \in \mathcal{A}$
et à ses parents $parents(X^i_{t+1}) \subseteq \set{X^1_t,\ldots,X^n_t}$
(\textit{i.e.} $parents(X^i_{t+1})$ est un sous ensemble de l'ensemble des variables d'état courantes)
où $n\geqslant1$ est le nombre de variables décrivant l'espace d'état.
}
\label{fig_piMDPFact}
\end{figure}
Dans le cadre des \textit{DBNs}, $parents(X^i_{t+1})$ 
est l'ensemble des variables d'état
dont la variable d'état suivante $X^i_{t+1}$ dépend,
\textit{i.e.} une variable $Y$, 
représentée par un noeud dans le \textit{DBN},
est dans $parents(X^i_{t+1})$ 
si et seulement si il y a une flèche de $Y$ à $X^j_{t+1}$.
Nous supposons que $parents(X^i_{t+1}) \subseteq \set{X^1_t,\ldots,X^n_t}$,
\textit{i.e.} les parents de la variable d'état suivante $X^i_{t+1}$
font partie des variables d'état courantes $\set{X^1_t,\ldots,X^n_t}$:
il ne peut y avoir de flèches entre
les variables d'état de la même étape de temps.

Avec les notations du $\pi$-PDMOM, 
les hypothèses du réseau bayésien
de la figure \ref{fig_piMDPFact}
nous permettent de calculer la distribution de possibilité jointe: 
$\pi \paren{s_v',\beta_h' \sachant s_v,\beta_h,a} = \pi \paren{ X' \sachant X,a } = \min_{i=1}^{n} \pi \paren{ X_i' \sachant parents(X_i'),a}$,
où, étant donné l'étape de temps $t$,
les variables primées sont les variables concernant l'étape de temps $t+1$
(variables suivantes), 
et les variables non-primées sont les 
variables courantes
(à l'étape de temps $t$): 
par exemple, 
$X_i'$ est la notation pour $X^i_{t+1}$,
et $X_i$ celle pour $X_t^i$.
Ainsi, un $\pi$-PDMOM factorisé peut être défini
par des fonctions de transition 
$T^i_a= \pi \paren{X_i' \sachant parents(X_i'),a }$ 
pour chaque action $a$
et chaque variable $X_i'$
(si les transitions sont stationnaires).
% such that $T^{a,i}(parents(X_i'),a,X_i') = \pi \paren{\mathbb{X}_i' \sachant parents(\mathbb{X}_i'),a }$.

Chaque fonction de transition
peut être représentée par un arbre de décision algébrique 
(\textit{ADD}) \cite{Bahar1997ADD}. 
Un \textit{ADD}, comme illustré dans la figure \ref{fig_ADDtransition}, 
est un arbre représentant de manière compacte
une fonction réelle de variables binaires, 
dont les sous-graphes identiques sont confondus
et les feuilles valant zéro ne sont pas mémorisées.
Les notations suivantes
sont utilisées pour rendre explicite
le fait que nous travaillons avec des fonctions symboliques
représentées par des \textit{ADDs}:
\begin{itemize}
\item $\ovalbox{$\min$} \set{f,g}$ où $f$ et $g$ sont deux \textit{ADDs};
\item $\ovalbox{$\max$}_{X_i} f$ = $\ovalbox{$\max$} \set{ f^{X_i=0},f^{X_i=1} }$, 
\end{itemize}
qui peut être facilement calculé car les \textit{ADDs}
sont construits sur la base
de l'expension de Shanon: $f = \overline{X_i} \cdot f^{X_i=0} + X_i \cdot f^{X_i=1}$ 
où $f^{X_i=1}$ et $f^{X_i=0}$ 
sont les sous graphes représentant
les cofacteurs de Shanon positifs et négatifs 
(\textit{cf.} figure \ref{fig_ADDtransition}). 

Le schéma de programmation dynamique, 
\emph{i.e.} la ligne \ref{VIupdate_OptAlgo} 
de l'algorithme d'itération sur les valeurs \ref{algorithmIVPIMDP} 
du chapitre précédent,
%equation of Line \ref{alg_piMDP_DP} of Algorithm \ref{algorithmIVPIMDP}
peut être réécrite sous forme symbolique, 
de telle sorte que les états soient
globalement mis à jour en un coup,
plutôt qu'individuellement:
en notant $X=(X_1,\ldots,X_n)$ 
la variable d'état courante
et $X'=(X_1',\ldots,X_n')$ la suivante,
la Q-valeur pour une action $a \in \mathcal{A}$
%in a state $x \in \mathcal{X}$,
est $\overline{q^a} = \overline{q^a}(X)= \ovalbox{$\max$}_{X'} \ovalbox{$\min$} \set{ \pi \paren{ X' \sachant X,a }, \overline{U^*}(X') }$.
Le calcul de cet \textit{ADD} ($\overline{q^a}$) 
peut être décomposé en plusieurs calculs 
en utilisant la proposition suivante:
\begin{Property}[Régression possibiliste de la fonction valeur]
\label{propositionRegression}
Considérons la fonction valeur courante
 $\overline{U^*}: \set{\top,\bot}^n \rightarrow \mathcal{L}$. 
Pour une action donnée $a \in \mathcal{A}$, 
définissons :
\begin{itemize}
\item $\overline{q^a_0} = \overline{U^*}(X'_1,\cdots,X'_n)$, 
\item $\overline{q^a_i} = \max_{X'_i \in \set{\top,\bot}} \min \Big\{ \pi \paren{X_i' \sachant parents(X_i'),a} , \overline{q^a_{i-1}} \Big\}$.
\end{itemize}
Alors, la  Q-valeur possibiliste d'une action $a$ est 
$\overline{q^a} = \overline{q^a_n}$,
qui dépend des variables $X_1,\ldots,X_n$,
et la fonction valeur suivante est $\overline{U^*}(X_1,\ldots,X_n) = \max_{a \in \mathcal{A}} \overline{q^a_n}(X_1,\ldots,X_n)$.
\end{Property}

La Q-valeur de l'action $a$, 
représentée par un \textit{ADD},
peut être alors calculée en plusieurs étapes,
une pour chaque variable d'état suivante $X'_i,
1 \leqslant i \leqslant n$. 
\begin{figure}
\centering
\begin{subfigure}[b]{0.35\linewidth}
\flushleft
\begin{tikzpicture}
\draw (-1.2,5.5) rectangle (2.8,7.3);
\draw (0.75,7) node (key) {KEY};
\draw (0.25,5.5) node (Lt) {};
\draw (1.25,5.5) node (Lf) {};
\draw (key) -- (Lt) node [left,midway] {$\top$ (true)};
\draw [dashed] (key) -- (Lf) node [right,midway] {$\bot$ (false)};

\draw (1,4.5) node (Xp1) {$X_1'$};
\draw (0,0.5) node (L3) {$1$};
\draw (1.5,3.2) node (X11) {$X_1$};
\draw (2,2) node (X2) {$X_2$};
\draw (1,0.5) node (L1) {$\frac{2}{3}$};
\draw (2.6,0.5) node (L2) {$\frac{1}{3}$};
\draw [dashed] (Xp1) -- (X11);
\draw (X11) -- (L1);
\draw [dashed] (X11) -- (X2);
\draw (X2) -- (L1);
\draw [dashed] (X2) -- (L2);
\draw (Xp1) -- (L3);
\end{tikzpicture}
\caption{\textit{ADD} représentant $T^a_1$ de la figure \ref{fig_piMDPFact}\\\\}
\label{fig_ADDtransition}
\end{subfigure}
\qquad
\begin{subfigure}[b]{0.58\linewidth}
\flushright
\begin{tikzpicture}

\draw (-2,3) node {\ovalbox{$\min$} $\Bigg\{$};

\draw (0,4.5) node (Xp1) {$X'_1$};
\draw (1,3) node (Xp2) {$X'_2$};
\draw (-1,1.5) node (Lp1) {$\frac{1}{3}$};
\draw (0.5,1.5) node (Lp2) {$\frac{2}{3}$};
\draw (2,1.5) node (Lp3) {$0$};
\draw (Xp1) -- (Lp1);
\draw [dashed] (Xp1) -- (Xp2);
\draw (Xp2) -- (Lp2);
\draw [dashed] (Xp2) -- (Lp3);

\draw (2.2,3) node {,};

\draw (4,4.5) node (Xp1) {$X_1'$};
\draw (3,1.5) node (L3) {$1$};
\draw  (Xp1) -- (L3);
\draw (4.6,3.5) node (X11) {$X_1$};
\draw (5.2,2.5) node (X2) {$X_2$};
\draw (4,1.5) node (L1) {$\frac{2}{3}$};
\draw (5.7,1.5) node (L2) {$\frac{1}{3}$};
\draw (X11) -- (L1);
\draw [dashed] (X11) -- (X2);
\draw (X2) -- (L1);
\draw [dashed] (X2) -- (L2);
\draw [dashed] (Xp1) -- (X11);


\draw (6.3,3	) node {$\Bigg\}$};



\draw (-2,-1.3) node {=};
%\draw (-0.5,-1.3) node {=};


\draw (-0.7,0.5) node (rXp1) {$X'_1$};
\draw (0.5,-0.2) node (rXp2) {$X'_2$};
\draw (-0.2,-1.2) node (rX1) {$X_1$};
\draw (0.7,-2) node (rX2) {$X_2$};
\draw (-0.5,-3.2) node (rL1) {$\frac{2}{3}$};
\draw (1.3,-3.2) node (rL2) {$\frac{1}{3}$};
\draw (-1.5,-3.2) node (rL3) {$\frac{1}{3}$};
\draw (2,-3.2) node (rL4) {$0$};

\draw [dashed] (rXp1) -- (rXp2);
\draw  (rXp2) -- (rX1);
\draw [dashed] (rXp2) -- (rL4);
%\draw (rXp2) -- (rX1);
%\draw [dashed] (rXp2) -- (rL3);
\draw (rXp1) -- (rL3);
\draw (rX1) -- (rL1);
\draw [dashed] (rX1) -- (rX2);
\draw (rX2) -- (rL1);
\draw [dashed] (rX2) -- (rL2);

%\draw (0.3,-2) node (dL1) {$\frac{1}{3}$};
%\draw (rXp1) -- (rL1);

\draw (2.5,-1.3) node {$\xrightarrow[\text{\ovalbox{$\max$}}_{X'_1}]{}$};

%\draw (4.5,-2) node (rrXp2) {$\mathbb{X}'_2$};
\draw (4.8,0.5) node (rrpX2) {$X_2'$};
\draw (4,-0.5) node (rrX1) {$X_1$};
\draw (4.5,-1.7) node (rrX2) {$X_2$};
\draw (3.5,-3.2) node (rrL1) {$\frac{2}{3}$};
\draw (5.7,-3.2) node (rrL2) {$\frac{1}{3}$};
\draw (rrpX2) -- (rrX1);
\draw (rrX1) -- (rrL1);
%\draw (rrXp2) -- (rrX1);
\draw [dashed] (rrX1) -- (rrX2);
\draw (rrX2) -- (rrL1);
\draw [dashed] (rrX2) -- (rrL2);
\draw [dashed] (rrpX2) -- (rrL2);

\end{tikzpicture}
\caption{Calcul symbolique de la Q-valeur courante sous forme d'un \textit{ADD}: 
elle est combinée à la fonction de transition
représentée par un \textit{ADD} dans la figure \ref{fig_ADDtransition}\\}
\label{fig_ADDregression}
\end{subfigure}
\caption{Exemple d'arbre de décision algébrique comme utilisé dans l'algorithme \textit{PPUDD}}
\label{fig_ADD}
\end{figure}	
La figure \ref{fig_ADDregression}
illustre les calculs possibilistes 
effectués entre les arbres de décision algébriques (\textit{ADDs})
pour calculer la Q-valeur d'une action.

\begin{algorithm} \caption{PPUDD (calcul pour un horizon indéterminé)} \label{PPUDD} 
 $\overline{U^*} \gets 0$ ;
 $\overline{U^c} \gets \Psi$ ;
 $\overline{\delta} \gets \widehat{a}$ \;

\While {$\overline{U^*} \neq \overline{U^c}$ \label{while_PPUDD} }{
 $\overline{U^*} \gets \overline{U^c}$ \;
 \For {$a \in \mathcal{A}$ \label{ppuddBeginQ}}{
	$\overline{q^a}  \gets $ swap each $X_i$ variable in $\overline{U^*}$ with $X_i'$ \label{ppuddSwap} \;
	\For {$1 \leqslant i \leqslant n$}{
	 $\overline{q^a} \gets \ovalbox{$\min$} \ \bigg\{ \overline{q^a} , \pi \Big( X_i' \ \Big\vert parents(X_i'),a \Big) \bigg\}$ \;
	 $\overline{q^a} \gets \ovalbox{$\max$}_{X_i'} \overline{q^a}$ \;
	}
	$\overline{U^c} \gets \ovalbox{$\max$} \set{\overline{q^a},\overline{U^c}  } $ \label{ppuddEndQ} \;
	update $\overline{\delta}$ to $a$ where $\overline{q^a}=\overline{U^c}$ and $\overline{U^c} > \overline{U^*}$ \;
}
}
\Return $\overline{U^*}$, $\overline{\delta^*}$ \;
\end{algorithm}

L'algorithme \ref{PPUDD}
est une version symbolique
de l'algorithme d'itération sur les valeurs
pour $\pi$-PDM dans le chapitre précédent, algorithme \ref{algorithmIVPIMDP}),
qui utilise le schéma de régression défini
dans la proposition \ref{propositionRegression}. 
Inspiré de \textit{SPUDD} \cite{Hoey99spuddstochastic},
\textit{PPUDD} signifie \emph{Possibilistic Planning Using Decision Diagrams}. 
Comme pour \textit{SPUDD},
l'action de transformer les variables non primées
en variables primées est nécessaire à chaque itération
(\textit{cf.} ligne \ref{ppuddSwap} de l'algorithme \ref{PPUDD} 
et figure \ref{fig_ADDregression}). 
Cette opération sert à différentier les états suivants des états courants
lors des opérations entre les ADDs. 
Les lignes \ref{ppuddBeginQ}-\ref{ppuddEndQ} 
appliquent la proposition \ref{propositionRegression} 
et correspondent à la ligne
\ref{VIupdate_OptAlgo} de l'algorithme \ref{algorithmIVPIMDP}.

Nous avons mentionné au début de la section
que l'espace des états de croyance $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
pourrait être décrit par $\lceil \log_2 K \rceil$ 
variables binaires où $K=\#
\mathcal{L}^{\#\mathcal{S}_h} - (\# \mathcal{L}-1)^{\# \mathcal{S}_h}$. 
Cependant, ce $K$ peut être très grand,
ainsi nous proposons
dans la section qui suit,
une méthode pour exploiter la factorisation
de $\mathcal{S}_h$ et $\mathcal{O}_h$
dans le but de factoriser $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
en plusieurs variables, 
ce qui décomposera les \textit{ADDs} de transition
en plus petits \textit{ADDs} facilement manipulables.
Notons que \textit{PPUDD} peut résoudre les $\pi$-PDMOM
même si cette factorisation de la croyance n'est pas
possible, mais les \textit{ADDs} manipulés sont plus gros dans ce cas.

Le chapitre précédent met en évidence qu'un prétraitement
est nécessaire
pour traduire un
$\pi$-PDMOM en un $\pi$-PDM dont l'espace d'état est $\mathcal{X}$.
Nous pouvons alors raisonner sur l'espace d'état
entièrement visible par l'agent $\mathcal{X} = S_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$
et résoudre le $\pi$-PDMOM en tant que $\pi$-PDM.
La section suivante fait le lien entre les propriétés structurelles
d'un $\pi$-PDMOM,
concernant les dépendences des variables originales 
(visibles, cachées et d'observation), 
à la factorisation du problème traité 
\textit{i.e.} du $\pi$-PDM
résultant, défini sur l'espace d'état $S_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$:
la factorisation résultante concerne alors 
les dépendances des variables visibles et des variables de croyance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Factorisation de la Variable de Croyance d'un $\pi$-PDMOM}

La factorisation de la variable de croyance 
requiert trois hypothèse d'indépendance
sur les variables du $\pi$-PDMOM, 
qui sont illustrées à travers le problème classique \textit{Rocksample} 
\cite{Smith2004HSV1036843.1036906}.
\subsection{Exemple de Motivation}
\label{section_RS_motivatingEx}
Un robot se déplaçant sur une grille $g \times g$ 
doit collecter des échantillons scientifiques 
à partir de pierres dites intéressantes parmi $R$ pierres, et enfin d'atteindre la sortie de la grille.
% It is 
%equipped with a noisy long-range sensor that can be used to determine if a rock 
%is ``good'' or not.
Il connait les positions des $R$ pierres $(x_i,y_i)_{i=1}^R$
mais pas lesquelles sont intéressantes. 
Les pierres intéressantes sont
appelées les ``bonnes'' pierres.
Cependant, échantillonner une pierre coûte cher:
le robot est donc équipé d'un capteur 
qu'il peut utiliser pour déterminer 
si une pierre est ``bonne'' ou non (``mauvaise''). 
Lorsqu'une pierre est échantillonnée,
elle devient (ou reste) ``mauvaise'' 
(plus intéressante scientifiquement). 
A la fin de la mission, le robot doit atteindre la position de sortie de la grille.
Décrivons le PDMOM associé:
\begin{itemize} 
\item $\mathcal{S}_{v}$ représente l'ensemble des positions possibles du robot %$(x_r,y_r)$
en plus de la sortie ($\# \mathcal{S}_v = g^2 +1$);
\item $\mathcal{S}_h$ représente l'ensemble des natures possibles de chacune des pierres:
$\mathcal{S}_h = \mathcal{S}^1_h \times \ldots \times \mathcal{S}^R_h$,
avec $\forall 1 \leqslant i \leqslant R$, $\mathcal{S}^i_h=\set{ ``bonne'', ``mauvaise'' }$;
\item $\mathcal{A}$ contient les déplacements (déterministes) dans les $4$ directions ($a_{north},a_{east},a_{south},a_{west}$),  
tester la pierre numéro $i$, ($a_{check_i}$) 
$\forall 1 \leqslant i \leqslant R $, 
et échantillonner la pierre courante, ($a_{sample}$);
\item $\mathcal{O} = \set{ o_{good},o_{bad} }$ sont les différentes réponses
possibles des capteurs pour la pierre testée. 
% $\mathcal{O}_{1} \times \ldots \times \mathcal{O}_{R}$ where $\forall 1 \leqslant i \leqslant R$, $\mathcal{O}_{i}=\set{ o_{good},o_{bad} }$ are observations concerning the $i^{th}$ rock. \\
\end{itemize}

La dynamique des observations est la suivante: 
plus le robot est proche de la pierre testée, 
mieux il observe sa nature.
Dans le problème probabiliste original,
la probabilité d'une observation correcte
est égale à
$\frac{1}{2}\paren{ 1 + e^{-c \sqrt{(x_r-x_i)^2 + (y_r-y_i)^2} }} $ avec $c>0$,
une constante (plus $c$ est petit, plus le capteur est efficace). 
Le robot reçoit la récompense $+10$ (resp. $-10$) 
pour chaque bonne (resp. mauvaise) pierre échantillonnée, % $-10$ for each bad rock sampled,
et $+10$ lorsqu'il atteint la sortie de la grille. 

Dans cadre possibiliste, 
la fonction d'observation
est approximée en utilisant
une distance critique au-delà 
de laquelle tester une pierre n'est pas informatif:
$\pi \paren{ o_i' \sachant s_i',a,s_v } = 1$ $ \forall o_i' \in \mathcal{O}_{i} $.
%If the
%rover is distant from the rock less than $d$, 
Le degré de possibilité d'une observation erronée
devient zéro lorsque le robot se trouve sur
la pierre testée, 
et devient le plus petit degré de possibilité 
non nul sinon. 
Enfin, puisque la sémantique possibiliste ne permet
pas de sommer les récompenses,
une variable additionnelle
$s^2_v \in
\set{ 1, \ldots, R }$
 est introduite:
elle compte le nombre de pierres testées.
La préférence qualitative de l'échantillonage
est définie par $\Psi(s)=\frac{R+2-s_{v}^{2}}{R+2} \in \mathcal{L}$
si la position est la sortie,
et zéro sinon.
Enfin, la position du robot
est notée $s^1_v \in \mathcal{S}^1_v$ 
et donc la variable d'état visible 
est finalement
$s_v=(s^1_v,s^2_v) \in \mathcal{S}^1_v \times \mathcal{S}^2_v = \mathcal{S}_v$. 

Les observations $\set{ o_{good},o_{bad} }$
pour la pierre testée peuvent être modélisées de manière équivalente
comme le produit cartésien des observations 
$\set{ o_{good_1},o_{bad_1} } \times \cdots \times \set{ o_{good_R},o_{bad_R} }$ 
pour chaque pierre.
En utilisant cette modélisation,
les espaces d'états et d'observations
sont respectivement factorisés de la manière suivante,
$\mathcal{S}^1_v \times \ldots \times \mathcal{S}^m_v \times \mathcal{S}^1_h \times \ldots \times \mathcal{S}^l_h $,
et $\mathcal{O} = \mathcal{O}^1 \times \ldots \times \mathcal{O}^l$, 
et nous pouvons maintenant associer une variable d'observation 
$O^j \in \mathcal{O}^j$ à la variable d'état cachée correspondante $S^j_h \in \mathcal{S}^j_h$. 
Cela permet de raisonner
sur le \textit{DBN} de la figure \ref{fig_piMOMDPFact}, 
qui exprime trois hypothèses importantes
qui nous permettrons de factoriser l'état de croyance: 
\begin{enumerate}
\item toutes les variables d'état $S^1_v,S^2_v,\ldots,S^m_v,S^1_h,S^2_h,\ldots,S^l_h$ 
sont indépendantes post-action, 
et une variable d'état suivante ne dépend par des variables d'état cachées courantes.
\item une variable d'état cachée ne dépend pas d'autres variables d'état cachées précédentes: 
par exemple la nature d'une pierre est indépendante de la nature 
des autres pierres.
\item une variable d'observation est disponible pour chaque variable d'état cachée,
et dépend de cet état.
Elle ne dépend pas d'autres
variables d'état cachées,
où des variables visibles courantes,
mais des variables visibles précédentes et de l'action choisie:
\end{enumerate}
Chaque variable d'observation
est en effet seulement associée
à la nature de la pierre correspondante.
La qualité de l'observation dépend de la position du robot 
\emph{i.e.} une variable d'état visible courante, 
ce qui n'est pas autorisé par le \textit{DBN}: 
heureusement, puisque les déplacements sont déterministes,
nous contournons le problème en considérant que cette qualité
dépend de la position précédente et de l'action choisie, ce qui est équivalent ici.


\begin{figure}[b!]\centering
\begin{tikzpicture}
\tikzstyle{vertex}=[circle,fill=black!50,minimum size=32pt,inner sep=0pt,draw=black,thick]
\tikzstyle{vvertex}=[circle,fill=black!30,minimum size=32pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red!60,minimum size=25pt,inner sep=0pt,draw=black,thick]
\tikzstyle{rvertex}=[fill=yellow!60,decision=3,inner sep=-1pt,minimum size=35pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=32pt,inner sep=0pt,draw=black,thick]


%TIME
\node [font=\huge] (statet) at (3,7.3) {$t$};
\node [font=\huge] (statetplus1) at (10,7.3) {$t+1$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%vstates
%1
\node[vvertex] (vstate1) at (3,6) {$S^1_{v,t}$};
\node[vvertex] (vstate12) at (3,4.5) {$S^2_{v,t}$};
\node (vstate13) at (3,3.5) {$\vdots$};
%2
\node[vvertex] (vstate2) at (10,6) {$S^1_{v,t+1}$};
\node[vvertex] (vstate22) at (10,4.5) {$S^2_{v,t+1}$};
\node (vstate23) at (10,3.5) {$\vdots$};

%0
\node (vstate0) at (-1,6) {};
\node (vstate02) at (-1,4.5) {};
%3
\node (vstate3) at (14,6) {};
\node (vstate32) at (14,4.5) {};


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%hstates
%1
\node[vertex] (hstate1) at (3,2) {$S^1_{h,t}$};
\node[vertex] (hstate12) at (3,0.5) {$S^2_{h,t}$};
\node (hstate13) at (3,-0.5) {$\vdots$};
%2
\node[vertex] (hstate2) at (10,2) {$S^1_{h,t+1}$};
\node[vertex] (hstate22) at (10,0.5) {$S^2_{h,t+1}$};
\node (hstate23) at (10,-0.5) {$\vdots$};
%0
\node (hstate0) at (-1,2) {};
\node (hstate02) at (-1,0.5) {};
%3
\node (hstate3) at (14,2) {};
\node (hstate32) at (14,0.5) {};



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%action
\node[avertex] (action) at (6.2,-2.3) {$a_t$};
\node[avertex] (action0) at (0,-2.3) {$a_{t-1}$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% observations
%1
\node[overtex] (hobserv1) at (6,-4) {$O^1_t$};
\node[overtex] (hobserv12) at (4.6,-2.9) {$O^2_t$};
\node (doto1) at (3.9,-2.1) [rotate=36] {$\vdots$};
%\node at (3.7,-3.3) {$\ldots$};
%2
\node[overtex] (hobserv2) at (13,-4) {$O^1_{t+1}$};
\node[overtex] (hobserv22) at (11.6,-2.9) {$O^2_{t+1}$};
\node (doto2) at (10.9,-2.1) [rotate=36] {$\vdots$};
%\node at (7.9,-3.4) {$\ldots$};


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS

%SV
% arrows sv->sv
%1->2
\draw[->,>=latex] (vstate1) -- (vstate2);
\draw[->,>=latex] (vstate1) -- (vstate22);
\draw[->,>=latex] (vstate12) -- (vstate2);
\draw[->,>=latex] (vstate12) -- (vstate22);
%0->1
\draw[->,>=latex,dashed] (vstate0) -- (vstate1);
\draw[->,>=latex,dashed] (vstate0) -- (vstate12);
\draw[->,>=latex,dashed] (vstate02) -- (vstate1);
\draw[->,>=latex,dashed] (vstate02) -- (vstate12);
%2->3
\draw[->,>=latex,dashed] (vstate2) -- (vstate3);
\draw[->,>=latex,dashed] (vstate2) -- (vstate32);
\draw[->,>=latex,dashed] (vstate22) -- (vstate3);
\draw[->,>=latex,dashed] (vstate22) -- (vstate32);


% arrows sv->sh
%1->2
\draw[->,>=latex] (vstate1) -- (hstate2);
\draw[->,>=latex] (vstate1) -- (hstate22);
\draw[->,>=latex] (vstate12) -- (hstate2);
\draw[->,>=latex] (vstate12) -- (hstate22);
%0->1
\draw[->,>=latex,dashed] (vstate0) -- (hstate1);
\draw[->,>=latex,dashed] (vstate0) -- (hstate12);
\draw[->,>=latex,dashed] (vstate02) -- (hstate1);
\draw[->,>=latex,dashed] (vstate02) -- (hstate12);

%2->3
\draw[->,>=latex,dashed] (vstate2) -- (hstate3);
\draw[->,>=latex,dashed] (vstate2) -- (hstate32);
\draw[->,>=latex,dashed] (vstate22) -- (hstate3);
\draw[->,>=latex,dashed] (vstate22) -- (hstate32);


% arrows sv->oh

\draw[->,>=latex] (vstate1) to[bend right] (hobserv2);
\draw[->,>=latex] (vstate12) to[bend right] (hobserv2);
\draw[->,>=latex] (vstate1) to[bend right] (hobserv22);
\draw[->,>=latex] (vstate12) to[bend right] (hobserv22);


\node (fake0) at (-1,-1) {};
\node (fake1) at (-1,0) {};
\node (fake2) at (-1,-0.3) {};
\node (fake3) at (-1,-0.7) {};
\draw[->,>=latex,dashed] (fake0) to (hobserv1);
\draw[->,>=latex,dashed] (fake1) to (hobserv12);
\draw[->,>=latex,dashed] (fake2) to (hobserv1);
\draw[->,>=latex,dashed] (fake3) to (hobserv12);

%SH
% arrows sh->sh
%1->2
\draw[->,>=latex] (hstate1) -- (hstate2);
\draw[->,>=latex] (hstate12) -- (hstate22);
%0->1
\draw[->,>=latex,dashed] (hstate0) -- (hstate1);
\draw[->,>=latex,dashed] (hstate02) -- (hstate12);

%2->3
\draw[->,>=latex,dashed] (hstate2) -- (hstate3);
\draw[->,>=latex,dashed] (hstate22) -- (hstate32);

% arrows sh->oh
%1
\draw[->,>=latex] (hstate1) to (hobserv1);
\draw[->,>=latex] (hstate12) to (hobserv12);
%2
\draw[->,>=latex] (hstate2) to (hobserv2);
\draw[->,>=latex] (hstate22) to (hobserv22);

%A
% a->s
%1
\draw[->,>=latex] (action) -- (vstate2);
\draw[->,>=latex] (action) -- (vstate22);
\draw[->,>=latex] (action) -- (hstate2);
\draw[->,>=latex] (action) -- (hstate22);
%0
\draw[->,>=latex,dashed] (action0) -- (vstate1);
\draw[->,>=latex,dashed] (action0) -- (vstate12);
\draw[->,>=latex,dashed] (action0) -- (hstate1);
\draw[->,>=latex,dashed] (action0) -- (hstate12);

% a->oh
%1
\draw[->,>=latex] (action) to[bend right] (hobserv2);
\draw[->,>=latex] (action) to[bend right] (hobserv22);
%0
\draw[->,>=latex] (action0) to[bend right] (hobserv1);
\draw[->,>=latex] (action0) to[bend right] (hobserv12);

%\node (probao1) at (4,-2.2) [rotate=350] {$ \pi \paren{ o_{h,t} \sachant s_t, a_{t-1}}$};
%\node (probao2) at (11,-2.5) [rotate=350] {$ \pi \paren{ o_{h,t+1} \sachant s_{t+1}, a_{t}}$};
%\node (probas) at (8.9,0.5) [rotate=355] {$ \pi \paren{s_{t+1} \sachant s_t,a_{t}}$};

\end{tikzpicture}
\caption[\textit{DBN} d'un ($\pi$-)PDMOM dont les variables de croyances peuvent être factorisées]{
\textit{DBN} résumant les hypothèses d'indépendance d'un $\pi$-PDMOM
menant à des variables de croyances marginales 
et un $\pi$-PDM avec
une fonction de transition factorisée.
Les parents d'une variable d'état visible sont les variables d'état visibles précédentes.
Les parents d'une variable d'état cachée sont les variables d'état visibles précédentes
et la variable d'état cachée précédente correspondante. 
Enfin, les parents d'une variable d'observation
sont les variables d'état visibles précédentes,
et la variables d'état cachée courante correspondante.
}
\label{fig_piMOMDPFact}
\end{figure}

\subsection{Conséquences des Hypothèses de Factorisation}
\label{section_factoAssumptions}
Dans cette section,
nous présentons le résultat formel
provenant des trois précédentes hypothèses:
ce résultat est la factorisation de $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$ 
en le produit cartésien $\displaystyle \bigtimes_{j=1}^{l} \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$.
En effet, l'état de croyance $\beta_h$
à propos de l'état caché du système $s_h \in \mathcal{S}_h$ 
peut être représenté des croyances marginales $\beta^j_h \in \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
sur les états cachés $s^j \in \mathcal{S}^j_h$, $\forall j \in \set{1,\ldots,l}$.

Le \textit{critère de d-Séparation} \cite{pearl88}
qui permet de montrer graphiquement des résultats d'indépendance
à partir du \textit{DBN}, se cache derrière les résultats fournis. 
Comme expliqué en section \ref{section_PPUDD}, 
un \textit{DBN} peut être construit à partir de relations d'indépendances.
Notons $X \perp\!\!\!\perp Y \ \vert \ Z$
l'assertion ``$X$ est indépendant de $Y$ 
conditionnellement à $Z$'':
rappelons que pour une définition donnée de la relation d'indépendance, 
par exemple l'indépendance probabiliste, ou l'indépendance possibiliste causale,
le \textit{DBN} est construit de telle sorte que pour chaque variable $X$,
$X \perp\!\!\!\perp nondescend(X) \ \vert \ parents(X)$,
où $nondescend(X)$ est l'ensemble des variables
qui ne sont pas des descendants de $X$.
Si l'indépendance utilisée obéit aux axiomes des  \textit{semi-graphoides} 
\cite{Pearl1988PRI52121,abs-1304-2379},
le critère graphique appelé d-Séparation 
peut être utilisé pour identifier certaines indépendances sur le \textit{DBN}.
Ce critère est utilisé, par exemple, dans le cadre des probabilités dans le travail \cite{Witwicki13icaps}.

Tout d'abord, comme
le \textit{DBN} de la figure \ref{fig_piMOMDPFact} 
représente des hypothèses d'indépendance causales,
la distribution de possibilité sur la variable d'état visible numéro $i$
$s^i_{v,t+1} \in \mathcal{S}^i_v$ sachant les variables précédentes peut s'écrire: 
\begin{equation}
\label{EQ_possV}
\pi \paren{ s^i_{v,t+1} \sachant s_{v,t}, a_t } = \Pi \paren{ S^i_{v,t+1} = s^i_{v,t+1} \sachant S_{v,t} = s_{v,t}, a_t   };
\end{equation}

Définissons l'information $i_t$
connue par l'agent à l'étape de temps $t \geqslant 1$
lorsque le modèle est un ($\pi$-)PDMOM:
 % of a $\pi$-MOMDP as: 
$i_0 = \set{ s_{v,0} }$, et
pour chaque étape de temps $t \geqslant 1$, 
$i_t = \set{ o_t,s_{v,t},a_{t-1},i_{t-1} }$.
La variable correspondante est notée $I_t$.
Le théorème suivant assure que l'état de croyance
courant peut être décomposé
en états de croyance marginaux
dépendant de l'information courante.
\begin{theorem}[Indépendance des variables d'état cachées sachant $i_t$]
\label{thmSHind} 
Considérons un $\pi$-PDMOM décrit par le \textit{DBN} de la figure \ref{fig_piMOMDPFact}.
Si les variables d'état cachées initiales $S^1_{h,0}, \ldots,S^l_{h,0}$ 
sont indépendantes, alors à chaque étape de temps $t>0$
l'état de croyance sur les états cachés
peut s'écrire 
\[ \beta_{h,t} = \displaystyle \min_{j=1}^{l} \beta^j_{h,t}\]
avec $\forall s \in \mathcal{S}^j_h$, $\beta^j_{h,t}(s) = \Pi \paren{ S^j_{h,t} = s \sachant I_t = i_t }$ l'état de croyance marginal
concernant les états cachés du système de l'ensemble $\mathcal{S}^j_h$.
\end{theorem}

Grâce au théorème précédent, 
l'espace d'état visible
par l'agent peut se réécrire 
$\mathcal{S}^1_v \times \ldots \times
\mathcal{S}^m_v \times \Pi^{\mathcal{S}^1_h}_{\mathcal{L}} \times \cdots \times \Pi^{\mathcal{S}^l_h}_{\mathcal{L}}$ 
avec $\Pi^{\mathcal{S}^j_h}_{\mathcal{L}}
\subsetneq \mathcal{L}^{\mathcal{S}^j_h}$. 
La taille de $\Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
est $\#\mathcal{L}^{\#\mathcal{S}^j_h} - (\#\mathcal{L}-1)^{\#\mathcal{S}^j_h}$
(\textit{cf.} équation \ref{equation_numberOfPossDistrib}). 
Si toutes les variables d'état sont binaires,
$\# \Pi^{\mathcal{S}^j_h}_{\mathcal{L}} = 2 \#\mathcal{L} - 1$ pour tout $1 \leqslant j \leqslant l$, 
ainsi
$\# \mathcal{S}_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}} = 2^m (2
\#\mathcal{L} - 1)^l$: 
contrairement au cadre probabiliste,
{\bf les états cachés et les états visibles ont un impact
similaire sur la complexité de résolution}, 
\textit{i.e.} tous les deux simplement exponentiels 
en le nombre de variables d'état. 
Dans le cas général, en notant 
$\kappa = \max\{\max_{1 \leqslant i \leqslant m} \# S_{v,i} , \max_{1 \leqslant j \leqslant l} \# S_{h,j}\}$, 
il y a $\mathcal{O}(\kappa^m (\# \mathcal{L})^{(\kappa - 1) l})$ 
états de croyances, ce qui est exponentiel en l'arité des variables d'état. 

La \textbf{fonction de mise à jour de l'état de croyance marginal} est $\nu^j$:
\[ \beta^j_{h,t+1} = \nu^j(s_{v,t},\beta^j_{h,t},a_t,o^j_{t+1}), \]
Cette mise à jour peut se noter 
\[ \beta^j_{h,t+1}(s^j_{h,t+1}) \propto^{\pi} \pi \paren{o^j_{t+1},s^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t} \]
puisqu'elle normalise au sens possibiliste la distribution de possibilité jointe
sur la variable d'état cachée et la variable d'observation qui ont le numéro $j$.

Ainsi, le degré de possibilité
que la variable de croyance marginale  
$B^{\pi,j}_{h,t+1}$ soit égale à $\beta^j_{h,t+1} \in \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
conditionnellement à $B^{\pi,j}_{h,t} = \beta^j_{h,t}$ 
et l'action $a_t \in \mathcal{A}$, 
peut se calculer:
\begin{equation}
\label{trans_marg_belief}
 \Pi \paren{ B^{\pi,j}_{h,t+1} = \beta^j_{h,t+1} \sachant S_{v,t} = s_{v,t}, B^{\pi,j}_{h,t} = \beta^j_{h,t}, a_t } = \max_{\substack{ o^j \in \mathcal{O}^j \mbox{ \tiny s.t. } \\ \nu^j(s_{v,t},\beta^j_{h,t},a_t,o^j) = \beta^j_{h,t+1}}} \pi \paren{ o^j \sachant s_{v,t}, \beta^j_{h,t}, a_t  }
\end{equation}
définissant la distribution de possibilité de transition
des états de croyance marginaux
$\pi \paren{ \beta^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t }$.

Enfin, le théorème \ref{thmVARind} nous assure que
les variables du $\pi$-PDM résultant d'un tel $\pi$-PDMOM sont 
indépendantes post-action conditionnellement à l'état courant du système:
cela permet alors d'écrire
la fonction de transition
de ce $\pi$-PDM sous forme factorisée:
\begin{theorem}[Expression factorisée de la distribution de possibilité de transition]
\label{thmVARind}
Si les hypothèses d'indépendance d'un $\pi$-PDMOM
sont décrites par le \textit{DBN} de la figure \ref{fig_piMOMDPFact},
alors
$\forall \beta_{h,t}=(\beta^1_{h,t},\ldots,\beta^l_{h,t}) \in \Pi^{\mathcal{S}_h}_{\mathcal{L}}, 
\beta_{h,t+1} = (\beta^1_{h,t+1},\ldots,\beta^l_{h,t+1}) \in  \Pi^{\mathcal{S}_h}_{\mathcal{L}}$, 
$\forall (s_{v,t},s_{v,t+1}) \in (\mathcal{S}_v)^2$, 
$\forall a_t \in \mathcal{A}$, \\
$\pi \paren{ s_{v,t+1}, \beta_{h,t+1} \sachant s_{v,t}, \beta_{h,t}, a }$ 
\[  = \displaystyle  \min \set{ \min_{i=1}^{m} \pi \paren{ s^i_{v,t+1} \sachant s_{v,t}, a_t } ,  \min_{j=1}^{l} \pi \paren{ \beta^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t  } }, \]
où la distribution de possibilité de transition
des variables d'état visibles
est donnée par l'équation (\ref{EQ_possV})
et celle des variables de croyance marginales
par l'équation (\ref{trans_marg_belief}).
\end{theorem}
En utilisant ce résultat, 
une telle expression factorisée
de la distribution de possibilité de transition
permet le calcul d'une fonction valeur avec $n=m+l$ étapes,
comme décrit dans la section précédente:
le $\pi$-PDMPO est en effet un $\pi$-PDM factorisé
puisque les variables $(S^1_v,\ldots,S^m_v,B^{\pi,1}_h,\ldots,B^{\pi,l}_h)$,
peuvent jouer le rôle des variables $X_1,\ldots,X_n$
dans l'algorithme \ref{PPUDD}.	

Notons que la relation d'indépendance probabiliste 
satisfait les axiomes des semi-graphoides:
ainsi, les résultats d'indépendance
dus à la d-Séparation sont aussi vrais pour le cadre probabiliste.
Si les hypothèses d'indépendance d'un PDMOM probabiliste \cite{OngShaoHsuWee-IJRR10,AraThoBufCha-ICTAI10} 
sont décrites par le \textit{DBN} de la figure \ref{fig_piMOMDPFact},
alors une factorisation similaire peut être déduite.
Le PDM construit à partir d'un tel PDMOM probabiliste
est donc un PDM factorisé, 
dont l'espace d'état est infini.

Les théorèmes précédents permettent
d'écrire la fonction de transition
d'un ($\pi$-)PDM résultant d'un ($\pi$-)PDMOM
avec des distributions qui concernent
moins de variables.
La mise à jour de la fonction valeur
durant la programmation dynamique
est alors divisée
en $n=m+l$ étapes dans le cas possibiliste,
comme décrit par la boucle \textit{for} de l'algorithme \ref{PPUDD}.
Cette boucle permet de manipuler des \textit{ADDs}
qui ont moins de noeuds
ce qui rend les calculs plus rapides en général.
Ces résultats sont utilisés dans la section suivante
afin de calculer de manière plus efficace
les stratégies optimales d'un $\pi$-PDMOM structuré.

\section{Résultats Expérimentaux}
\label{section_expe_PPUDD}
Les calculs sont beaucoup plus simples dans le cadre possibiliste,
mais il faut évidemment en payer le prix:
les modèles possibilistes peuvent être considérés
comme des approximations
de modèles probabilistes.
Comme montré dans cette section,
une approximation possibiliste en utilisant \textit{PPUDD}
peut mener à de meilleures stratégies,
au sens probabiliste,
que celles calculées par certains 
algorithmes probabilistes
lorsque les dimensions du problème considéré
rendent les calculs insurmontables.

\subsection{Missions Robotiques}

Nous avons comparé \textit{PPUDD} sur le problème \textit{Rocksample} (RS),
décrit en section \ref{section_RS_motivatingEx}, 
contre un récent planificateur probabiliste pour PDMOM, 
\textit{APPL} \cite{OngShaoHsuWee-IJRR10}, 
et un planificateur pour PDMPO utilisant des \textit{ADDs}, 
\textit{symbolic HSVI} \cite{Sim2008SHS1620163.1620241}.
Ces deux algorithmes peuvent être arrêtés à n'importe quel moment, 
et plus les calculs durent, 
plus la stratégie calculée est performante:
ainsi nous avons décidé d'arrêter 
les calculs lorsque l'erreur d'approximation
passe en-dessous de $1$.
La figure \ref{figureRS1},
où l'instance du problème 
augmente avec la complexité du problème,
montre que \textit{APPL} déborde en mémoire à l'instance numéro $8$, 
et \textit{symbolic HSVI} à l'instance numéro $7$,
tandis que \textit{PPUDD}
peut calculer une stratégie pour
des instances beaucoup plus compliquées. 
\begin{figure}\centering
\begin{subfigure}[c]{.48\linewidth}
\includegraphics[width=\linewidth]{RockSampleCompTime.pdf}
\caption{Temps de calcul (secondes)}
\label{figureRS1}
\end{subfigure}
\begin{subfigure}[c]{.48\linewidth}
\includegraphics[width=\linewidth]{courbePerfTime.pdf}
\caption{Récompense totale espérée}
\label{figureRS2}
\end{subfigure}
\vspace{0.5cm}
\caption[Comparaison entre \textit{PPUDD} et les planificateurs probabilistes \textit{APPL} et \textit{symb-HSVI} sur le problème \textit{RockSample}]{Comparaison entre \textit{PPUDD} et les planificateurs probabilistes \textit{APPL} et \textit{symb-HSVI} sur le problème \textit{RockSample}: 
l'axe des abscisses représente l'indice de l'instance du problème qui est
croissant avec la complexité de instance du problème.}
\end{figure}
Nous pouvons aussi fixer une durée de calcul
aux algorithmes probabilistes utilisés:
le temps de calcul d'\textit{APPL} est alors fixé au temps de calcul de \textit{PPUDD},
et les performances de leurs stratégies respectives
sont comparées
en terme d'espérances de la somme des récompenses 
(et en utilisant les récompenses définies par le modèle probabiliste). 
%For PPUDD, we
%simply evaluated with the probabilistic model the policy that was computed
% under
%possibilistic settings.
\'Etonnamment, la figure \ref{figureRS2} 
montre que les récompenses récupérées 
sont en moyenne plus grandes avec \textit{PPUDD}
qu'avec \textit{APPL}.
La raison est que \textit{APPL} est en fait un planificateur 
probabiliste qui cherche à raffiner une approximation durant le temps de calcul, 
ce qui montre qui notre approche consistant à résoudre exactement
un modèle approximé peut mieux fonctionner que de résoudre de manière approchée
un modèle exact.
Enfin, nous pouvons noter que les probabilités du modèle d'observation, 
qui représentent les incertitudes concernant les réponses des capteurs, 
peut être difficile à connaître précisément en pratique,
auquel cas les modèles possibilistes
peuvent plus physiquement rigoureux.

Ces résultats nous ont permis de juger pertinent
la participation de \textit{PPUDD}
à la compétition internationale de planification probabiliste 2014,
même si le calcul de stratégie pour les modèles probabilistes
n'est pas la vocation initiale de ce planificateur.
La section suivante discute des résultats des différents planificateurs.

\subsection{Compétition Internationale de Planification Probabiliste 2014}
La session entièrement observable
de la compétition internationale de planification probabiliste
permet de comparer des planificateurs de PDM
en garantissant les même ressources en terme de puissance de calcul
et de temps de calcul
à chacun des algorithmes participants.
Les planificateurs compétiteurs doivent calculer
des stratégies pour certains problèmes
qui ne sont pas connus à l'avance. 
\'Etant donné un de ces problèmes,
les planificateurs ont un temps limité
pour envoyer des actions à un serveur de la compétition
qui simule l'évolution de l'état du système: 
les états successifs sont 
générés par le serveur de la compétition
en utilisant la fonction de transition probabiliste
du PDM définissant le problème,
et envoyés à un des serveurs de compétiteurs.
Pour chaque état du système reçu,
le planificateur du compétiteur
doit envoyer une action,
qu'il a calculé,
en retour.
Ces échanges de données se produisent sur plusieurs exécutions d'horizon fini
et le score du planificateur pour les problèmes considérés
est la moyenne, sur les exécutions, 
de la somme finie des récompenses
obtenue sur la trajectoire de l'état du système. 

Plus d'informations à propos de cette compétition
sont disponibles sur le site officiel de la compétition
\url{https://cs.uwaterloo.ca/~mgrzes/IPPC_2014/}.
Les problèmes sont regroupés dans des \textit{domaines}, 
qui sont des PDM dont un nombre fini de paramètres ne sont pas définis:
le problème, ou PDM, utilisé en pratique durant la compétition
est une \textit{instance} d'un domaine, \textit{i.e.}
un domaine dont les paramètres ont été définis.
Dans cette compétition,
$8$ domaines ont été proposés,
appelé respectivement
\textit{Academic advising}, \textit{Crossing traffic}, \textit{Elevators}, 
\textit{Skill teaching}, \textit{Tamarisk}, \textit{Traffic}, \textit{Triangle tireworld}
et \textit{Wildfire}.
La compétition consiste à évaluer les planificateurs
sur $10$ instances par domaine avec 
$30$ exécutions de chaque instance et
$18$ minutes allouées par instance: 
elle dure donc $24$ heures au total.

Quatre planificateurs ont été proposés pour cette compétition:
\begin{itemize}
\item \textit{PROST} \cite{KellerE12}, basé sur l'algorithme \textit{Upper Confidence bound applied to Trees} (UCT, \cite{Kocsis2006BBM2091602.2091633});
\item \textit{GOURMAND} \cite{KolobovMW12,Kolobov12reverseiterative}, basé sur l'algorithme \textit{Labeled Real Time Dynamic Programming} (\textit{LRTDP}, \cite{Bonet03labeledrtdp});
\item \textit{symbolic LRTDP} \cite{symbLRTDP};
\item notre algorithme \textit{PPUDD}.
\end{itemize}

Comme le score donné aux planificateurs ne dépend que des $40$ premières
étapes du processus,
la version présentée de \textit{PPUDD}
est l'algorithme \ref{PPUDD}
avec la ``condition du while'' $\overline{U^*} \neq \overline{U^c}$ 
à la ligne \ref{while_PPUDD}
remplacée par la condition ``numéro de l'itération $\leqslant 40$''.
Il augmente de manière incrémentale
l'horizon de planification
en maintenant un masque stocké sous forme
d'un \textit{BDD} (arbre de décision binaire, 
\textit{i.e.} un \textit{ADD} avec des feuilles dans $\set{0,1}$)
représentant les états atteignables
à partir de l'état initial: 
le calcul de la fonction valeur courante
est alors restreinte aux états atteignables seulement. 
Bien que \textit{PPUDD} soit un algorithme de calcul hors-ligne,
nous avons aussi proposé \textit{AnyTime PPUDD} (\textit{ATPPUDD}),
une version qui gère les temps de calcul 
comme décrit dans \cite{KolobovMW12}. 

La bibliothèque utilisée pour faire les calculs entre \textit{ADDs}
s'appelle \textit{CU Decision Diagram Package} 
(\textit{CUDD}, \url{http://vlsi.colorado.edu/~fabio/CUDD/}),
et les versions de \textit{PPUDD} décrites sont disponibles à l'adresse 
\url{https://github.com/drougui/ppudd}.

Les figures qui suivent
sont les résultats d'\textit{IPPC} 2014:
les scores sont donnés en fonction de l'indice de l'instance,
qui augmente généralement avec la complexité du problème associé.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random, ``noop'', ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},
xlabel={Instances des problèmes},
ylabel={Moyenne brute},
title={Problème \textit{Academic advising}},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/academic_advising.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/academic_advising.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/academic_advising.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/academic_advising.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/academic_advising.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/academic_advising.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/academic_advising.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Instances des problèmes},
ylabel={Moyenne brute},
title={Problème \textit{Crossing traffic}},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Résultats d'\textit{IPPC} 2014: problèmes \textit{Academic advising} et \textit{Crossing traffic}]{
Résultats de la compétition internationale de planification probabiliste -- session entièrement observable}
\label{figure_IPPC_ACA_CRO}
\end{figure}

La figure \ref{figure_IPPC_ACA_CRO} présente les scores obtenus
par chacun des planificateurs
pour chacune des $10$ instances du domaine \textit{Academic advising},
\textit{i.e.} la moyenne sur les $30$ exécutions, de la somme des récompenses rencontrées.
Les performances de notre algorithme 
sont proches de celles des meilleurs algorithmes.
Cependant, un bug non expliqué et indésirable
s'est produit avec \textit{ATPPUDD} lors de l'instance numéro $2$,
puisque seules trois exécutions ont été possibles
avec ce planificateur. 
Pour les autres instances, 
\textit{PPUDD} et \textit{ATPPUDD} produisent des stratégies
dont les performances sont semblables à celles de PROST et GOURMAND, 
et meilleures que \textit{Symbolic LRTDP}.
Ceci n'est plus vrai avec le problème \textit{Crossing traffic},
dont les résultats sont décrits par la figure \ref{figure_IPPC_ACA_CRO}.
Ce problème modélise un robot qui doit atteindre un but
qui est de l'autre côté d'une route à plusieurs voies
que de nombreuses voitures empruntent.
Ces voitures arrivent de manière aléatoire
et vont vers la gauche.
Comme le degré de possibilité attribué au fait
qu'aucune voiture arrive a été fixé à $1$
par notre traduction naïve de PDM en $\pi$-PDM,
le critère optimiste mène à la décision de traverser la route
même si une voiture (invisible au moment de la décision) 
peut arriver sur la droite 
(avec une probabilité $<0.5$ mais assez grande pour devoir plutôt être prudent, ou pessimiste,
et traverser la route dans une position avec une visibilité sur l'arrivée des voitures). 
Ceci explique la pauvre qualité des stratégies produites par \textit{PPUDD} pour ce domaine.
Notons cependant que, pour les $6$ dernières intances (\textit{i.e.} les problèmes les plus difficiles)
notre approche mène à de meilleures stratégies
que le planificateur probabiliste \textit{Symbolic LRTDP}.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random, ``noop'', ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm,
xlabel={Instances des problèmes},
ylabel={Moyenne brute},
title={Problème \textit{Elevators}}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/elevators_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/elevators_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/elevators_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/elevators_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/elevators_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/elevators_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/elevators_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Instances des problèmes},
ylabel={Moyenne brute},
title={Problème \textit{Skill teaching}},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Résultats d'\textit{IPPC} 2014: problèmes \textit{Elevators} et \textit{Skill teaching}]{
Résultats de la compétition internationale de planification probabiliste -- session entièrement observable}
\label{figure_IPPC_ELE_SKI}
\end{figure}

Le problème \textit{Elevators} concerne 
des gens qui arrivent de manière aléatoire 
devant un ascenseur
et qui doivent être transportés
à l'étage qu'ils ont choisi d'un immeuble:
comme l'information fréquentiste est perdue
lors de l'utilisation de l'approche possibiliste,
et semble très importante dans ce problème
(les gens ne veulent pas attendre une fois arrivés devant l'ascenseur), 
cela explique pourquoi les scores de notre algorithme sont moins bons que ceux de
PROST et GOURMAND.
Cependant \textit{PPUDD} et \textit{ATPPUDD} sont meilleurs que Symbolic LRTDP, 
comme montré dans la figure \ref{figure_IPPC_ELE_SKI},
et la stratégie qui consiste à ne rien faire (``noop'')
ou la stratégie qui choisit des actions de manière aléatoire (``random'').
\textit{PPUDD} et \textit{ATPPUDD} ont des comportements plutôt bons avec le problème \textit{Skill teaching} 
comme illustré par la même figure.
De plus, \textit{ATPPUDD} mène à de meilleurs résultats
pour les trois dernières instances:
puisque ces instances 
sont celles du domaine \textit{Skill teaching} 
avec l'espace d'état le plus grand, 
la version anytime, 
qui gère les temps de calculs de manière plus recherchée,
produit des stratégies avec de meilleures performances 
que \textit{PPUDD}, 
qui résout classiquement le problème 
du $\pi$-PDM, associé,
mais ne peut pas terminer les calculs
et mène à des stratégies moins bonnes.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Instances des problèmes},
ylabel={Moyenne brute},
title={Problème \textit{Tamarisk}},
compat=newest,
legend entries={GOURMAND, PROST, Symbolic LRTDP, random, ``noop'', ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/tamarisk.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/tamarisk.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/tamarisk.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/tamarisk.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/tamarisk.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/tamarisk.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/tamarisk.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Instances des problèmes},
ylabel={Moyenne brute},
title={Problème \textit{Traffic}},
width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/traffic_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/traffic_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/traffic_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/traffic_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/traffic_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/traffic_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/traffic_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Résultats d'\textit{IPPC} 2014: problèmes \textit{Tamarisk} et \textit{Traffic}]{
Résultats de la compétition internationale de planification probabiliste -- session entièrement observable}
\label{figure_IPPC_TAM_TRA}
\end{figure}

Par rapport aux autres planificateurs,
les algorithmes possibilistes donnent de bon résultats
avec le domaine \textit{Tamarisk}, 
comme le montre la figure \ref{figure_IPPC_TAM_TRA}.
Cependant, certaines instances 
(par exemple les instances numéro $6$, $8$ et $10$) 
ne sont même pas exécutées
puisque l'instanciation des \textit{ADDs} définissant le problème 
prennent trop de temps. 
\textit{Symbolic LRTP} fait face au mêmes problèmes
puisqu'il utilise aussi des \textit{ADDs}.
Le domaine \textit{Traffic} est vraiment 
très dur à résoudre avec \textit{PPUDD} et \textit{ATPPUDD} (\textit{cf.} figure \ref{figure_IPPC_TAM_TRA}). 
En fait, les pires scores sont obtenus avec ce domaine,
et même la stratégie aléatoire ou la stratégie ``noop''
sont meilleures.
Il aurait été avantageux pour nous
d'implémenter un garde-fou
renvoyant des actions aléatoires lorsque la stratégie calculée
est moins performante
que la stratégie aléatoire.
Comme mentionné au-dessus 
avec le problème \textit{Crossing traffic}, 
le critère optimiste peut mener à des actions dangereuses,
comme cela se produit ici.
De plus, puisque ce problème implique
des informations fréquentistes 
(arrivées des voitures aléatoires),
notre approche est sûrement inadaptée pour cet exemple.
Enfin, le domaine \textit{Traffic}
est connu comme étant un des domaines les plus durs à résoudre,
ainsi l'instanciation des \textit{ADDs} en mémoire prennent trop de temps, 
tout comme les calculs,
qui ne sont alors pas assez avancés
pour produire des résultats satisfaisants. 

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random, ``noop'', ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm,
xlabel={Instances des problèmes},
ylabel={Moyenne brute},
title={Problème \textit{Triangle tireworld}}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%PPUDD
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10}, width=11cm,height=11cm,
xlabel={Instances des problèmes},
ylabel={Moyenne brute},
title={Problème \textit{Wildfire}}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/wildfire_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/wildfire_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/wildfire_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/wildfire_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/wildfire_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/wildfire_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/wildfire_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Résultats d'\textit{IPPC} 2014: problèmes \textit{Triangle tireworld} et \textit{Wildfire}]{
Résultats de la compétition internationale de planification probabiliste -- session entièrement observable}
\label{figure_IPPC_TRI_WIL}
\end{figure}

Finalement, les deux derniers domaines,
dont les résultats sont décrits dans la figure \ref{figure_IPPC_TRI_WIL},
sont appelés \textit{Triangle Tireworld} et \textit{Wildfire}.
Tout d'abord, \textit{ATPPUDD} fait face
à un bug inexpliqué pour chaque instance du domaine \textit{Triangle Tireworld}:
aucune exécution n'est réalisée
à partir de l'instance numéro $5$, 
et deux exécutions ont été réalisées
pour les autres instances
(ce qui explique le faible score pour chaque instance).
Comme déjà mentionné pour le domaine \textit{Tamarisk}, 
l'instanciation des \textit{ADDs} prend trop de temps pour les dernières instances,
et aucune exécution n'est réalisée pour les $4$ dernières instances
avec \textit{PPUDD}: \textit{Symbolic LRTDP} rencontre les même problèmes.
Le dernier domaine, appelé \textit{Wildfire}, 
constitue un problème très fréquentiste:
il implique des départs de feux.
C'est pourquoi \textit{PPUDD} et \textit{ATPPUDD} ne sont pas vraiment efficaces, 
mais pas trop distant non plus des résultats de \textit{Symbolic LRTDP}.


\section{Conclusion}
Nous avons présenté \textit{PPUDD},
le premier algorithme, à notre connaissance,
qui résout les PDM(OM) possibilistes qualitatifs
avec des calculs symboliques. 
Nous pensons que les modèles possibilistes 
constituent un bon compromis entre les modèles non-déterministes, 
où l'incertitude n'est pas quantifiée du tout,
menant à un modèle très approximatif, 
et les modèles probabilistes, 
où l'incertitude est complètement spécifiée,
et parfois de manière arbitraire en pratique.
La résolution de problèmes de planification 
en utilisant le cadre du non-déterminisme 
est appelé \textit{planification contingente/conformante}, 
étudiée par exemple dans \cite{Albore_atranslation-based,bonet2014flexible}.

Nos résultats expérimentaux montrent que
l'utilisation d'un algorithme exact (\textit{PPUDD}) 
pour résoudre un modèle approximé ($\pi$-PDMOM)
peut mener à des calculs plus rapides
que de raisonner sur des modèles exacts et complexes, 
tout en générant des stratégies qui peuvent être 
de meilleure qualité que celles retournées 
par des algorithmes procédant à des approximations (\textit{APPL})
sur des modèles exacts (PDMOM). 

Enfin, ce chapitre présente les résultats 
de notre approche possibiliste
durant \textit{IPPC} 2014: 
un des problèmes de cette approche
est la traduction du modèle probabiliste
en modèle possibiliste:
la traduction automatique naïve utilisée est
peut être à l'origine des mauvaises performances
de l'approche pour certain domaines
à la dynamique, ou la fonction de récompense complexe.
Un autre problème semble être l'utilisation des \textit{ADDs}:
l'instanciation de ces objets 
avant même le début des calculs
prend un temps trop important,
où bien ne passe même pas en mémoire.
Cette difficulté est partagée avec le planificateur \textit{Symbolic LRTDP}.
Des problèmes de modélisation ont aussi été mis en évidence:
certains problèmes nécessitent une approche optimiste,
et d'autres une approche pessimiste.

Cependant, bien que PROST et GOURMAND ont des performances deux fois supérieures à
\textit{PPUDD} et \textit{ATPPUDD}, ces derniers proposent des stratégies 
dont les performances sont deux fois supérieures
à son homologue probabiliste \textit{Symbolic LRTDP}.
Notons finalement que \textit{PPUDD} est disponible dans le dépôt \url{https://github.com/drougui/ppudd}.
