\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\section*{Context}
%%% CONTEXT
\malettrine{P}{roviding} the autonomy to a robot 
consists in computing a function which returns 
the name of the actions to be triggered
at a given moment, with respect to the data from its sensors.
The features of interest of the robot 
and its  surroundings form a \textit{system}.
In general, for a given sequence of actions performed by the robot, 
the evolution of this system 
is not fixed for sure, 
but its behavior may be known 
performing tests on the robot
or using information from expert knowledge.
As well, the raw or processed data from the robot sensors
are not generally a deterministic outcome
of the state of the system nor the taken actions:
nevertheless, these data, 
called also \textit{observations} of the system, 
depend on robot's actions and system states. 
Relations between observations, system states and actions 
may be known through tests of the sensors
in various situations, 
or by taking into account the sensors description, 
the data processing, or any related expert information.
For instance, in the case of a robot using Computer Vision (CV),
the output of the picture processing algorithm employed
is considered as an observation of the system
since it is the result of processed sensor data:
here data are pictures from camera.
For a given camera, and a given vision algorithm,
the behavior of the observation
is related to the action and the system state
during the picture taking process. 

Thus, in order to make a robot autonomously fulfill a chosen \textit{mission}, 
we are  looking for a function returning actions 
conditional on the sequence of system observations,
and taking into account uncertainty 
about the system evolution and the observation of it.
Such functions may be called \textit{strategies}.
The research domain associated to this kind of problem,
\textit{i.e.} strategy computation,
is not restricted to robotics and
is called \textit{sequential decision making under uncertainty}:
in the general case,
the entity which has to act is called the \textit{agent}.
In this thesis, 
the problem of strategy computation 
is studied in the context of autonomous robotics,
and the agent is the decisional part of the robot.
Computing a strategy for a given robotic mission needs a proper framework:
the best known model describes the state and observation behaviors
using Probability Theory.

\subsection*{A probabilistic model for strategy computation}

Markov Decision Processes (MDPs) 
define a useful formalism 
to express sequential decision problems 
under probabilistic uncertainty \cite{Bel}.
It is a well suited framework 
if the actual system state is known by the agent
at each point in time.
In the robotic context,
this assumption means that
the considered mission allows to assume 
that the robot has full knowledge 
of the features of interest via its sensors.
In this model,
a system state is denoted by the letter $s$,
and the finite set of all the possible states is $\mathcal{S}$.
The finite set $\mathcal{A}$ consists
of all possible actions $a \in \mathcal{A}$ 
available to the agent. 
The time is discretized into integers $t \in \mathbb{N}$
which represent time steps of the action sequence.

The state dynamics is assumed to be \textit{Markovian}:
at each time step $t$,
the next system state $s_{t+1} \in \mathcal{S}$,  
only depends on the current one $s_t \in \mathcal{S}$
and the chosen action $a_t \in \mathcal{A}$.
This relation is described by a transition function 
$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t}$
which is defined as the probability distribution
on the next system states $s_{t+1}$ conditional for each action: 
if the action $a_t \in \mathcal{A}$ is selected by the agent, 
and the current system state is $s_t \in \mathcal{S}$, 
the next state $s_{t+1} \in \mathcal{S}$
is reached with the probability denoted by 
$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t }$. 

The mission of the agent is described in terms of rewards:
a reward function $r(s,a) \in \mathbb{R}$ 
is defined for each action $a \in \mathcal{A}$
and system state $s \in \mathcal{S}$,
and models the goal of the agent. 
The more rewards are gathered during an execution of the process, the better:
a realization of a sequence of system states
and actions is considered as well fulfilling the desired mission 
if encountered rewards $r(s_t,a_t)$ are high.
Solving an infinite horizon MDP
consists in computing an optimal strategy, 
\textit{i.e.} a function prescribing actions $a \in \mathcal{A}$
to be taken over time,
and maximizing the mean
of the sum of rewards gathered during an execution:
this mean is computed with respect to the probabilistic behavior of the system state
encoded by transition functions $\textbf{p} \paren{s_{t+1} \sachant s_t,a_t}$.
For instance, a preferred strategy 
may be a function $d$ defined on $\mathcal{S}$,
as the current state is available to the agent, 
and with values in $\mathcal{A}$.
A criterion measuring the accuracy of the strategy $d$ 
may then be the (infinite horizon) expected discounted total reward: 
\begin{equation}
\label{criterion}
\mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,d_t) },
\end{equation}
where $d_t=d(s_t) \in \mathcal{A}$ 
and $0<\gamma<1$ is a discount factor 
assuring the convergence of the sum.

%%% ROBOT POMPIER
\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\node (rpomp) at (-1,4.7) {\includegraphics[scale=0.9]{robot_pompier}};
\node (bli) at (0,6.5) {};
\node (pomdp3) at (3.9,6) {\color{orange}{$s \in \mathcal{S}$}: \color{black}{\textbf{system state}}};
\node (t1) at (2,6) {};
\node (r1) at (-2.3,5.5) {};
\node (r11) at (1,4.5) {};
\draw[->,>=latex,color=orange!60,line width=1mm] (t1) to (r1);
\draw[->,>=latex,color=orange!60,line width=1mm] (t1) to (r11);
\node (pomdp4) at (5.8,4.45) {\color{blue!60}{$o \in \mathcal{O}$:} \color{black} \textbf{system observation}};
\node (t2) at (3.3,4.4) {};
\node (r2) at (-2.3,5.05) {};
\draw[->,>=latex,color=blue!40,line width=1mm] (t2) to[bend left] (r2);
\node (pomdp5) at (1.8,3) {\color{red}{$a \in \mathcal{A}$:} \color{black} \textbf{agent's action}};
\node (t3) at (-0.3,3) {};
\node (r3) at (-2,4.2) {};
\draw[->,>=latex,color=red!50,line width=1mm] (t3) to (r3);
\node (pomdp6) at (-5.5,5.5) {\color{ggreen}{$b \in \mathbb{P}^{\mathcal{S}}$:} \color{black} \textbf{belief state} }; %%% ORANGE?
\node (pomdp6) at (-3.5,6) {\color{ggreen}{\Huge \textbf{?}}}; %%% ORANGE?
\end{tikzpicture}
\caption[Use of a POMDP for the firefighter robot mission modeling]{
Use of a POMDP for the firefighter robot mission modeling:
in this toy example, the mission of the robot is the fire prevention.
The \textbf{states of the system} $s \in \mathcal{S}$ encode for instance the robot location, 
the water jet orientation, the amount of water used,
the fire location and its level on a scale between ``minor fire'' and  ``big fire'', etc.
Using vision and heat sensors, 
the robot gets \textbf{observations} $o \in \mathcal{O}$ 
which are the raw or processed values from the sensors:
the output of a classifier
whose input is a picture of the scene
(see Figure \ref{observation_robot} 
and \ref{CV_algoConvNet}), 
and which returns the fire level or location
may be encoded in an observation.
Finally, the \textbf{actions of the robot} $a \in \mathcal{A}$ 
are for instance the rotor activations 
impacting the rotation of the robot's wheels, the water pumping,
the orientation of the water jet or sensors etc.
%Uncertainty dynamics is described by conditional probability distributions:
The \textbf{reward function} $r(s,a)$ decreases with the fire level state,
and is decreased by a cost proportional to the amount of water used:
as an optimal strategy maximizes the mean of the sum of the rewards, 
the goal of the robot is thus to attack fires without wasting water.
This mean can be computed knowing the probabilities describing the uncertainty dynamic of the system.
The robot actions $a \in \mathcal{A}$ have a probabilistic effect on the system,
as described by the \textbf{transition function} $\textbf{p} \paren{s' \sachant s,a}$: 
for instance, the activation of wheel rotors modifies the location of the robot,
and the probability of each possible next locations, given the current system state, 
takes part in the definition of the POMDP.
An other example is the action modifying the water jet orientation, 
which redefines the probability of the next fire level given the current system state.
The robot actions $a \in \mathcal{A}$ and next states $s' \in \mathcal{S}$ 
may also influence the observations from the sensors, 
as defined by the \textbf{observation function} $\textbf{p} \paren{o' \sachant s',a}$: 
for instance, the orientation of the vision sensor may modify 
the probability of fire detection or fire level evaluation, 
which are parts of the observations $o' \in \mathcal{O}$. 
Finally, the \textbf{belief state} is the conditional probability distribution 
of the current system state
conditional on all observations and actions up to the current time step: 
as observations and actions are the only data available to the robot,
the belief state can be seen as the robot's guess.}
\label{robot_pompier}
\end{figure}

The assumption that the agent has a perfect 
knowledge of the system state
is quite strong:
in particular, in the case of robots realizing tasks with conventional captors,
the latter are usually unable to provide 
all the features of interest for the mission 
to the robot.
Thus, a more flexible model has been built, allowing a \textit{partial observability}
of the system state by the agent.
%%% ROBOT POMPIER FIN

%%% POMDP
Indeed a Partially Observable MDP (POMDP) \cite{Smallwood_Sondik} 
makes a step further in the modeling flexibility, 
handling situations in which the agent 
does not know directly
the current state of the system: 
it finely models 
an agent acting under uncertainty in a partially hidden environment.

The set of system states $\mathcal{S}$, 
the set of actions $\mathcal{A}$, 
the transition function $\textbf{p} \paren{ s_{t+1} \sachant s_t,a_t}$ 
and the reward function $r(s,a)$ 
remain the same as for the MDP definition. 
In this model, since the current system state $s \in \mathcal{S}$ 
cannot be used as available information for the agent, 
the agent knowledge about the actual system state 
comes from observations $o \in \mathcal{O}$, 
where $\mathcal{O}$ is a finite set. 
%A full definition of this process 
%includes as well the set of 
%possible observations of the system, 
%$o \in \mathcal{O}$. 
The observation function $\textbf{p} \paren{ o_{t+1} \sachant s_{t+1},a_t }$
gives for each action $a_t \in \mathcal{A}$ and reached system state $s_{t+1} \in \mathcal{S}$, 
the probability over possible observations $o_{t+1} \in \mathcal{O}$. 
Finally, the \textit{initial belief state} $b_0(s)$ 
defines the \textit{prior} probability distribution 
over the system state space $\mathcal{S}$. 
An example of usage of a POMDP is presented in Figure \ref{robot_pompier}.

Solving a POMDP consists in computing 
a strategy 
which returns a proper action at each process step, 
according to all received observations and selected actions  
\textit{i.e.} all of the data available to the agent:
a criterion for the strategy may be also the
expected discounted sum of rewards (\ref{criterion}).

%
% belief
%
Most of the POMDP algorithms reason about the \textit{belief state}, 
defined as the probability of the actual system state knowing
all the system observations and agent actions from the beginning.
This belief is updated at each time step using the Bayes
rule and the new observation. 
At a given time step $t \in \mathbb{N}$, 
the belief state $b_t(s)$ is defined 
as the probability that the $t^{th}$ state is $s \in \mathcal{S}$
conditional on all the past actions and observations, 
and with the prior $b_0$:
it estimates the actual system state using the available data,
as the latter is not directly observable.

It can be easily recursively computed using Bayes rule: 
at time step $t$, 
if the belief state is $b_t$, 
chosen action $a_t \in \mathcal{A}$ and new observation 
$o_{t+1} \in \mathcal{O}$, 
next belief is
\begin{eqnarray}
\label{probBayesRule}
b_{t+1}(s')  \propto \textbf{p} \paren{ o_{t+1} \sachant s', a_t } \cdot \sum_{s \in \mathcal{S}} \textbf{p} \paren{s' \sachant s,a_t} \cdot b_t(s).
\end{eqnarray}
as illustrated by the Bayesian Network in Figure \ref{BayesNetPOMDP}.
%
% BAYESNET
%
\begin{figure}\centering
\begin{tikzpicture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%vertex
\tikzstyle{vertex}=[circle,fill=black!40,minimum size=30pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red! 60,minimum size=25pt,inner sep=0pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=30pt,inner sep=0pt,draw=black,thick]
%nodes
\node[vertex] (state1) at (0,1.8) {$S_t$};
\node[vertex] (state2) at (7,1.8) {$S_{t+1}$};
\node[overtex] (obs) at (13,0) {$O_{t+1}$};
\node[avertex] (action) at (2.5,0) {$a_t$};
%%bels
\node (bel1) at (1,2.5) {$b_t$};
\node (bel2) at (8.5,2.5) {$b_{t+1}$};
%probas
\node (trans) at (4.5,1.5) {$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t }$};
\node (observ) at (9.6,0.3) {$\textbf{p} \paren{ o_{t+1} \sachant s_{t+1}, a_t }$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS
\draw[->,>=latex, thick] (state1) -- (state2);
\draw[->,>=latex, thick] (state2) -- (obs);
\draw[->,>=latex, thick] (action) -- (state2);
\draw[->,>=latex, thick] (action) -- (obs);
\end{tikzpicture}
\caption[Bayesian Network illustrating the belief update.]{Bayesian Network illustrating the belief update: the states are the gray circular nodes, 
the action is the red square node, and the observation is the blue circular node.
The random variable $S_t$ representing the next state $s_{t+1}$ 
depends on the current one $s_t$ and the current action $a_t$.
The random variable $O_{t+1}$ representing the next observation $o_{t+1}$ 
depends on the next state $s_{t+1}$ 
and the current action $a_t$ too.
The belief state $b_{t}$ (resp. $b_{t+1}$)
is the probabilistic estimation of the current (resp. next) system state $s_t$ (resp. $s_{t+1}$).}
\label{BayesNetPOMDP}
\end{figure}%

As successive beliefs are computed 
with the observations perceived by the agent,
they are considered as visible for the agent. 
%Moreover, it can be easily shown that
%the expected total reward can be rewritten 
%\begin{equation}
%\label{probCriterion}
%\mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,d_t) } = \mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(b_t,d_t) }, 
%\end{equation}
%defining $r(b_t,a) = \sum_s r(s,a) \cdot b_t(s)$ as the reward of belief $b_t$.
Let us denote by $\mathbb{P}^{\mathcal{S}}$ 
the infinite set of probability distributions 
over $\mathcal{S}$.
An optimal strategy can be looked for 
as a function $d$ defined on $\mathbb{P}^{\mathcal{S}}$ 
such that successive $d_t = d(b_t) \in \mathcal{A}$ 
maximize the expected reward \ref{criterion}:
the agent decisions are then based on the belief state.

%
% POMDP ROBOTICS
%
The POMDP framework is a flexible model for autonomous robotics,
as illustrated by the firefighter example, see Figure \ref{robot_pompier}:
it allows to describe all the robotic and surrounding system,
as well as the robot mission,
and it is commonly used in robotics 
\cite{conf/isrr/PineauG05,OngShaoHsuWee-IJRR10,DBLP:conf/rss/Marthi12,DBLP:conf/ecai/ChanelTL12,DBLP:conf/aaai/ChanelTL13}.
It takes into account that the robot receives data
from its sensors only,
and thus has to figure out the actual system state 
using these data, called observations,
in order to fulfill the mission.
However the POMDP model raises some issues,
in particular in the robotic context.

%
% HIGH COMPLEXITY
%
\section*{Practical issues of the POMDP framework}
\subsection*{Complexity}
Solving a POMDP \textit{i.e.} computing an optimal strategy, 
is PSPACE-hard in finite horizon \cite{Papadimitriou:1987} 
and even
undecidable in infinite horizon \cite{Madani:1999:UPP:315149.315395}.
Moreover a space exponential in the problem description may be required 
for an explicit specification of such a strategy
(see \cite{Mundhenk:2000:CPP:867838} 
for a more detailed complexity analysis of POMDPs).

This high complexity is well-known by POMDP users:
optimality can be reached 
for tiny problems, 
or highly structured ones.
Classical approaches try to solve this problem
using Dynamic Programming \cite{Cassandra97incrementalpruning}.
Otherwise, only approximate solutions can be computed,
and thus the strategy has no optimality guaranty.
For instance, popular approaches such as point-based methods 
\cite{Pineau_2003_4826,Kurniawati-RSS08,Smith:2004:HSV:1036843.1036906}, 
grid-based ones \cite{Geffner98solvinglarge,Brafman97aheuristic,Bonet_newgrid-based}
or Monte Carlo approach \cite{NIPS2010_4031},
use approximate computations.
The next POMDP's practical issues that will be highlighted, 
concern modeling flaws appearing when using this framework:
they are easily illustrated via robotic situations.

%
% VISION IN ROBOTICS
%
\begin{figure} \centering
\includegraphics[scale=0.75]{fig2}
\caption[Example of an observation method in a robotic context]{
Example of an observation method in a robotic context:
the robot, here a drone, is equipped with a camera
and uses a classifier computed from a picture dataset
(as NORB, see Figure \ref{NORB}): 
such a classifier is described by Figure \ref{CV_algoConvNet}.
The classifier is computed before the mission (off-line)
with a picture dataset (see the right part of the illustration),
and the classifier output is used during the mission (online) 
as an observation, for the agent (see the left part).
Here, observations are thus generated by computer vision.}
\label{observation_robot}
\end{figure}
%
% VISION IN ROBOTICS END
%


%
% COMPUTER VISION TODO TODO TODO
%
\subsection*{Parameter imprecision and computer vision}
Consider now robots using visual perception, 
and whose observations 
come from computer vision algorithms based on statistical learning
(see Figure \ref{observation_robot}).
In this situation, the robot uses a \textit{classifier}
to recognize objects in pictures: 
the classifier is supposed to return the name of 
the object actually in the picture, 
and makes some mistakes 
with a low probability 
(see confusion matrix of Figure \ref{confusion_matrix}).

The classifier is computed using a \textit{training dataset} of pictures
(as NORB, see Figure \ref{NORB}, authors made it available at \url{http://www.cs.nyu.edu/~ylclab/data/norb-v1.0/}).
A powerful gradient-based learning algorithm meant to compute classifiers using picture datasets
is described in Figure \ref{CV_algoConvNet}: the associated framework is called Convolutional Nextwork \cite{Lecun98gradient-basedlearning}.
The figures (\ref{NORB}), (\ref{CV_algoConvNet}) and (\ref{confusion_matrix})
illustrate the example of a classifier computed for a drone mission where features of interests
(system states of the problem) 
are related with the presence (or absence) of animals, cars, humans, planes or trucks:
the statistical problem of computing a classifier recognizing such objects in pictures 
is called \textit{multiclass classification}.

%%%
%%% NORB DATA SET
%%%
\newcounter{moncompteur} %define counter
\begin{figure} \centering
\begin{tikzpicture}
\node (notes) at (1cm,7cm) { 
NORB dataset: \color{blue} $(\mbox{picture}_i,\mbox{label}_i)_{i=1}^N$
};

\def\names{{"bla","human","car","truck","truck","nothing","nothing","nothing","truck"}}%
\def\namess{{"bla","airplane","car","human","animal","car","human","animal","animal"}}%

\setcounter{moncompteur}{1}
	\coordinate (norb11) at (6.2cm,3.05cm); 
	\coordinate (norb12) at (14cm,3.05cm); 
	\coordinate (norb11lab) at (6.2cm,6.2cm); 
	\coordinate (norb12lab) at (14cm,6.2cm); 
	\coordinate (norb11lab2) at (6.2cm,1.9cm); 
	\coordinate (norb12lab2) at (14cm,1.9cm); 
	\foreach \y in {0.25,0.5,...,2} {
		\coordinate (weight) at (barycentric cs:norb11= \y,norb12=1-\y);
		\def\reptemp{photos/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics[scale=0.5]{\reptemp}};
		
		\coordinate (weight2) at (barycentric cs:norb11lab= \y,norb12lab=1-\y);
		\node[scale=0.7] (leslabels) at (weight2) {\pgfmathparse{\names[\themoncompteur]}\pgfmathresult};
		\coordinate (weight3) at (barycentric cs:norb11lab2= \y,norb12lab2=1-\y);
		\node[scale=0.7] (leslabels) at (weight3) {\pgfmathparse{\namess[\themoncompteur]}\pgfmathresult};

		\addtocounter{moncompteur}{1}
	}

	\coordinate (norb21) at (6.2cm,5cm); 
	\coordinate (norb22) at (14cm,5cm); 
	\foreach \y in {0.25,0.5,...,2} {
		\coordinate (weight) at (barycentric cs:norb21= \y,norb22=1-\y);
		\def\reptemp{photos/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics[scale=0.5]{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

\end{tikzpicture}
\caption[Example of picture dataset for computer vision.]{Example of picture dataset for computer vision: 
the labeled picture dataset NORB \cite{LeCun.2004}. The size of NORB is higher than $3.10^5$,
and pictures from this dataset represents objects among the $5$ classes: 
``animal'', ``car'', ``human'', ``nothing'', ``plane'' and ``truck''.
Each element of a labeled picture dataset is composed of a picture (e.g. a picture showing a car)
and a label corresponding to the class of the object represented by the picture 
(in the previous example, the label is ``car'').
This dataset can be used for supervised learning to compute a classifier (see Figure \ref{CV_algoConvNet}).
In order to be able to discern locations of targets, 
a picture is labeled with the name of centered object 
(``nothing'' if there is nothing in the picture center).}
\label{NORB}
\end{figure}
%%%%%
%%%%% NORB DATASET END
%%%%%

%
% IMPRECISION OBSERVATION
%
As the classifier is learned based on a picture dataset 
(see weights learned in Figure \ref{CV_algoConvNet}),
its behavior, and thus its performances 
(\textit{i.e.} how well it predicts objects in images) 
are inevitably dependent on the dataset.
It is a problem if the picture variability in the dataset
is too low:
in this case, the probabilistic behavior of the classifier 
will be dependent on these particular pictures, 
and the robotic system will have poor observation
capabilities when the considered mission involves 
pictures too different from the ones from the dataset. 

Some large picture datasets with a high image variability exists 
(e.g. NORB, Figure \ref{NORB}, although variability could be ideally higher):
note however that with such datasets, the vision performances are reduced,
or good performances are, at least, harder to reach.

A confusion matrix can be computed (see Figure \ref{confusion_matrix})
using such a labeled picture dataset,
not used for the training, and called \textit{testing dataset}: 
observation frequencies can be deduced from this matrix,
normalizing rows into probabilities.
A row corresponds to an object in the scene,
and probabilities on this row are observation probabilities,
\textit{i.e.} each probability is the frequency with which 
the classifier returns the name of the object of the corresponding column.
These probabilities can be used to define the observation function 
$\textbf{p} \paren{o' \sachant s',a}$
introduced above. 
This approach raises the issue of knowing if the testing dataset 
is quite representative of the mission reality.
If not, the observation probabilities may be not reliable,
and the POMDP badly defined: 
however, as shown by the equation (\ref{probBayesRule})
the belief update needs a perfect knowledge of the
observation probability distributions.

More generally, observations of the agent 
may be outputs of image processing algorithms 
whose semantics (image correlation, object matching, class inference, 
preprocessing followed by classifiers such as the one computed 
in Figure \ref{CV_algoConvNet} etc.) 
are so complex that probabilities of 
occurrence are hard to rigorously extract.

Finally, if the considered datasets are labeled more precisely 
(as NORB, which includes information such as the lighting condition, or the object scale),
we can imagine that the computed observation probabilities (from the confusion matrix) 
were more reliable, or the vision performances upgraded 
(since separation when learning the classifier is easier).
However, as more observation or states are involved in this case,
the POMDP is harder to solve.
Moreover, as the number of pictures per class is reduced 
(since there are more complex and numerous classes), 
a poorer confusion matrix is obtained when testing (in terms of confidence).

As a conclusion, the POMDP model supposes 
the knowledge of all the involved 
probability distributions:
unfortunately the actual frequencies 
used to compute them
are not precisely known in practice.
The imprecision about these probabilities, 
for instance the imprecision related to 
the vision algorithm behavior
with real world pictures,
has to be taken into account 
to make the robot autonomous 
under any circumstances.
In general, the computation 
of the probability distributions
of a POMDP needs enough tests 
for each possible system state and action,
which is hard to perform:
there are limited data for learning the model
in practice.

%%
%% ConvNets
%%
\begin{figure} \centering
\begin{tikzpicture} 
\label{bipyr}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TIKZ
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\node (car) at (-1,-1) {\includegraphics[scale=0.52]{NORB_car.png}};

	% input image 
	\coordinate (A1) at (0cm,0cm); 
	\coordinate (A2) at (0cm,-2cm); 
	\coordinate (A3) at (-2cm,0cm);
	\coordinate (A4) at (-2cm,-2cm); 
	\draw[thick] (A1) -- (A3);
	\draw[thick] (A3) -- (A4);
	\draw[thick] (A4) -- (A2);
	\draw[thick] (A2) -- (A4);

	\node at (barycentric cs:A1=1,A3=1,A2=-0.15) {\tiny $m$};
	\node at (barycentric cs:A3=1,A4=1,A1=-0.2) {\tiny $m$};

	% output feature
	\coordinate (A6) at (7cm,-4cm);
	\coordinate (A7) at (9cm,-0.7cm);
	\coordinate (A8) at (9.1cm,-0.7cm);
	\coordinate (A9) at (7.1cm,-4cm);
	\draw[thick] (A6) -- (A9);
	\draw[thick] (A7) -- (A8);
	\draw[thick] (A8) -- (A9);

	\node at (barycentric cs:A7=1,A8=1,A6=-0.1) {\tiny $1$};
	\node at (barycentric cs:A8=1,A9=1,A1=-0.05) {\tiny $n$};

	%% draw faces
	%\fill[gray!50,opacity=0.7] (A1) -- (A6) -- (A7) -- cycle;
	%\fill[gray!50,opacity=0] (A1) -- (A2) -- (A7) -- cycle; 
	%\fill[gray!50,opacity=0.7] (A2) -- (A1) -- (A6) -- cycle;
	\draw[thick,dashed] (A1) -- (A7);
	\draw[thick,dashed] (A2) -- (A7);

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% INTERMEDIATE FEATURE MAPS DRAWING %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE1

	% base points (of right and left images)
	\coordinate (im111) at (2.77cm,-0.08cm); 
	\coordinate (im112) at (2.77cm,-1.85cm); 
	\coordinate (im113) at (1cm,-0.08cm);
	\coordinate (im114) at (1cm,-1.85cm); 

	\coordinate (im131) at (2.45cm,-0.4cm); 
	\coordinate (im132) at (2.45cm,-2.17cm); 
	\coordinate (im133) at (0.68cm,-0.4cm);
	\coordinate (im134) at (0.68cm,-2.17cm); 


	% images plans
	% beginning
	\draw[thick] (im111) -- (im113);
	\draw[thick] (im113) -- (im114);
	\draw[thick] (im114) -- (im112);
	\draw[thick] (im112) -- (im111);
	\fill[gray!00,opacity=0.7] (im111) -- (im112) -- (im114) -- (im113) -- cycle;


	% intermediate plans
	\coordinate (im121) at (2.62cm,-0.24cm); 
	\coordinate (im122) at (2.62cm,-2.01cm); 
	\coordinate (im123) at (0.85cm,-0.24cm);
	\coordinate (im124) at (0.85cm,-2.01cm); 
	\draw[thick] (im121) -- (im123);
	\draw[thick] (im123) -- (im124);
	\draw[thick] (im124) -- (im122);
	\draw[thick] (im122) -- (im121);
	\fill[gray!00,opacity=0.7] (im121) -- (im122) -- (im124) -- (im123) -- cycle;

	%% end
	\draw[thick] (im131) -- (im133);
	\draw[thick] (im133) -- (im134);
	\draw[thick] (im134) -- (im132);
	\draw[thick] (im132) -- (im131);
	\fill[gray!00,opacity=0.7] (im131) -- (im132) -- (im134) -- (im133) -- cycle;


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE2
	
	% base points (of right and left images)
	\coordinate (im211) at (4.5cm,-2cm); 
	\coordinate (im212) at (4.5cm,-3cm); 
	\coordinate (im213) at (3.5cm,-2cm);
	\coordinate (im214) at (3.5cm,-3cm); 

	\coordinate (im291) at (5.5cm,-0.35cm); 
	\coordinate (im292) at (5.5cm,-1.35cm); 
	\coordinate (im293) at (4.5cm,-0.35cm);
	\coordinate (im294) at (4.5cm,-1.35cm); 

	% images plans
	% beginning
	\draw[thick] (im291) -- (im293);
	\draw[thick] (im293) -- (im294);
	\draw[thick] (im294) -- (im292);
	\draw[thick] (im292) -- (im291);
	\fill[gray!00,opacity=0.7] (im291) -- (im292) -- (im294) -- (im293) -- cycle;

	% intermediate plans
	\foreach \y in {0.1,0.2,...,1} {
		\coordinate (ii1) at (barycentric cs:im211= \y,im291=1-\y);
		\coordinate (ii2) at (barycentric cs:im212= \y,im292=1-\y);
		\coordinate (ii3) at (barycentric cs:im213= \y,im293=1-\y);
		\coordinate (ii4) at (barycentric cs:im214= \y,im294=1-\y);

		\draw[thick] (ii1) -- (ii3);
		\draw[thick] (ii3) -- (ii4);
		\draw[thick] (ii4) -- (ii2);
		\draw[thick] (ii2) -- (ii1);
		\fill[gray!00,opacity=0.7] (ii1) -- (ii2) -- (ii4) -- (ii3) -- cycle;
	}

	% end
	\draw[thick] (im211) -- (im213);
	\draw[thick] (im213) -- (im214);
	\draw[thick] (im214) -- (im212);
	\draw[thick] (im212) -- (im211);
	\fill[gray!00,opacity=0.7] (im211) -- (im212) -- (im214) -- (im213) -- cycle;

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE3

	% base points (of right and left images)
	\coordinate (im311) at (5.75cm,-3cm); 
	\coordinate (im312) at (5.75cm,-3.5cm); 
	\coordinate (im313) at (5.25cm,-3cm);
	\coordinate (im314) at (5.25cm,-3.5cm); 

	\coordinate (im391) at (7.2cm,-0.53cm); 
	\coordinate (im392) at (7.2cm,-1.03cm); 
	\coordinate (im393) at (6.7cm,-0.53cm);
	\coordinate (im394) at (6.7cm,-1.03cm); 

	% images plans
	% beginning
	\draw[thick] (im391) -- (im393);
	\draw[thick] (im393) -- (im394);
	\draw[thick] (im394) -- (im392);
	\draw[thick] (im392) -- (im391);
	\fill[gray!00,opacity=0.7] (im391) -- (im392) -- (im394) -- (im393) -- cycle;

	% intermediate plans
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (ii1) at (barycentric cs:im311= \y,im391=1-\y);
		\coordinate (ii2) at (barycentric cs:im312= \y,im392=1-\y);
		\coordinate (ii3) at (barycentric cs:im313= \y,im393=1-\y);
		\coordinate (ii4) at (barycentric cs:im314= \y,im394=1-\y);

		\draw[thick] (ii1) -- (ii3);
		\draw[thick] (ii3) -- (ii4);
		\draw[thick] (ii4) -- (ii2);
		\draw[thick] (ii2) -- (ii1);
		\fill[gray!00,opacity=0.7] (ii1) -- (ii2) -- (ii4) -- (ii3) -- cycle;
	}

	% end
	\draw[thick] (im311) -- (im313);
	\draw[thick] (im313) -- (im314);
	\draw[thick] (im314) -- (im312);
	\draw[thick] (im312) -- (im311);
	\fill[gray!00,opacity=0.7] (im311) -- (im312) -- (im314) -- (im313) -- cycle;

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE4
	
	% base points (of right and left images)
	\coordinate (im411) at (6.35cm,-3.5cm); 
	\coordinate (im412) at (6.35cm,-3.75cm); 
	\coordinate (im413) at (6.1cm,-3.5cm);
	\coordinate (im414) at (6.1cm,-3.75cm); 

	\coordinate (im491) at (8.12cm,-0.61cm); 
	\coordinate (im492) at (8.12cm,-0.86cm); 
	\coordinate (im493) at (7.87cm,-0.61cm);
	\coordinate (im494) at (7.87cm,-0.86cm); 

	% images plans
	% beginning
	\draw[thick] (im491) -- (im493);
	\draw[thick] (im493) -- (im494);
	\draw[thick] (im494) -- (im492);
	\draw[thick] (im492) -- (im491);
	\fill[gray!00,opacity=0.8] (im491) -- (im492) -- (im494) -- (im493) -- cycle;

	% intermediate plans
	\foreach \y in {0.04,0.08,...,1} {
		\coordinate (ii1) at (barycentric cs:im411= \y,im491=1-\y);
		\coordinate (ii2) at (barycentric cs:im412= \y,im492=1-\y);
		\coordinate (ii3) at (barycentric cs:im413= \y,im493=1-\y);
		\coordinate (ii4) at (barycentric cs:im414= \y,im494=1-\y);

		\draw[thick] (ii1) -- (ii3);
		\draw[thick] (ii3) -- (ii4);
		\draw[thick] (ii4) -- (ii2);
		\draw[thick] (ii2) -- (ii1);
		\fill[gray!00,opacity=0.7] (ii1) -- (ii2) -- (ii4) -- (ii3) -- cycle;
	}

	% end
	\draw[thick] (im411) -- (im413);
	\draw[thick] (im413) -- (im414);
	\draw[thick] (im414) -- (im412);
	\draw[thick] (im412) -- (im411);
	\fill[gray!00,opacity=0.8] (im411) -- (im412) -- (im414) -- (im413) -- cycle;


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE5

	% base points (of right and left images)
	\coordinate (im511) at (6.75cm,-3.8cm); 
	\coordinate (im512) at (6.75cm,-3.9cm); 
	\coordinate (im513) at (6.65cm,-3.8cm);
	\coordinate (im514) at (6.65cm,-3.9cm); 

	\coordinate (im591) at (8.65cm,-0.67cm); 
	\coordinate (im592) at (8.65cm,-0.77cm); 
	\coordinate (im593) at (8.55cm,-0.67cm);
	\coordinate (im594) at (8.55cm,-0.77cm); 

	% images plans
	% beginning
	\draw[thick] (im591) -- (im593);
	\draw[thick] (im593) -- (im594);
	\draw[thick] (im594) -- (im592);
	\draw[thick] (im592) -- (im591);
	\fill[gray!00,opacity=0.8] (im591) -- (im592) -- (im594) -- (im593) -- cycle;

	% intermediate plans
	\foreach \y in {0.025,0.05,...,1} {
		\coordinate (ii1) at (barycentric cs:im511= \y,im591=1-\y);
		\coordinate (ii2) at (barycentric cs:im512= \y,im592=1-\y);
		\coordinate (ii3) at (barycentric cs:im513= \y,im593=1-\y);
		\coordinate (ii4) at (barycentric cs:im514= \y,im594=1-\y);

		\draw[thick] (ii1) -- (ii3);
		\draw[thick] (ii3) -- (ii4);
		\draw[thick] (ii4) -- (ii2);
		\draw[thick] (ii2) -- (ii1);
		\fill[gray!00,opacity=0.7] (ii1) -- (ii2) -- (ii4) -- (ii3) -- cycle;
	}

	% end
	\draw[thick] (im511) -- (im513);
	\draw[thick] (im513) -- (im514);
	\draw[thick] (im514) -- (im512);
	\draw[thick] (im512) -- (im511);
	\fill[gray!00,opacity=0.8] (im511) -- (im512) -- (im514) -- (im513) -- cycle;


	%%%%%%%%%%%%%%%%%%%%%%	
	%% END FEATURE MAPS %%
	%%%%%%%%%%%%%%%%%%%%%%

	\draw[thick,dashed] (A2) -- (A6);
	\draw[thick] (A7) -- (A6);
	\draw[thick,dashed] (A1) -- (A6);
	\draw[thick] (A1) -- (A2);
	
	% draw points
	\foreach \i in {1,2,3,4,6,7,8,9}
	{
	  \draw[fill=black] (A\i) circle (0.01em);
	}

	% draw arrows
	\coordinate (F1) at (-0.5,-2.2cm);
	\coordinate (F2) at (0.7,-2.5cm);
	\coordinate (F3) at (1.7,-2.8cm);
	\coordinate (F4) at (3.2,-3.1cm);
	\coordinate (F5) at (4,-3.4cm);
	\coordinate (F6) at (5,-3.7cm);
	\node (Fdots) at (5.6,-3.9cm) [rotate=345] {$\ldots$};
	\coordinate (F7) at (6.6,-4.05cm);
	\coordinate (F8) at (7,-4.1cm);
	\coordinate (F9) at (7.3,-4.1cm);
	\coordinate (F10) at (8.7,-3.2cm);

	\draw[->,>=latex,thick, color=blue!60] (F1) to[bend right] (F2);
	\draw[->,>=latex,thick, color=blue!60] (F3) to[bend right] (F4);
	\draw[->,>=latex,thick, color=blue!60] (F5) to[bend right] (F6);
	\draw[->,>=latex,thick, color=blue!60] (F7) to[bend right] (F8);
	\draw[->,>=latex,thick, color=blue!60] (F9) to[bend right] (F10);

	\coordinate (A10) at (8.7cm,-3cm);
	\coordinate (A11) at (9.55cm,-1.7cm);
	\coordinate (A12) at (9.65cm,-1.7cm);
	\coordinate (A13) at (8.8cm,-3cm);
	\draw[thick] (A10) -- (A11);
	\draw[thick] (A11) -- (A12);
	\draw[thick] (A12) -- (A13);
	\draw[thick] (A13) -- (A10);
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%****************************************************************%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\node at (-0.3,0.8) {Given $i \in \set{1,\ldots,N}$,};
	\node at (barycentric cs:A2=1,A4=1,A1=-0.4) {\color{blue} $\mbox{picture}_i$};


	\draw[thick,dashed] (A9) -- (A10);
	\draw[thick,dashed] (A8) -- (A11);
	\node (etiq2) at (barycentric cs:A12=1,A13=1,A1=-0.2) {$\mbox{vector}_{\color{black}i}^{\color{red}W}$};

	\node (w0) at (0.3cm, -3cm) {{\color{red} $w_0$ }};
	\node (w1) at (2.2cm, -3.5cm) {{\color{red} $w_1$ }};
	\node (w2) at (4cm, -3.9cm) {{\color{red} $w_2$ }};
	\node (wm) at (9cm, -4cm) {{\color{red} $w_m$ }};

	\node  at (12cm,0cm) {\color{blue} $\mbox{label}_i$};
	\node (etiq) at (12cm,-0.7cm) {e.g. ``car''};


\tikzstyle{grisEncadre}=[thick, fill=gray!20];
\draw [grisEncadre] (6.8,-4.65) rectangle (13.2,-5.9);

	\node (perte) at (11.3cm, -4.7cm) {};	
	\node (perte1) at (10cm, -4.9cm) { \textit{gradient-based} \textit{ minimization} of};
	\node (perte2) at (10cm, -5.5cm) {  $loss\Big( (\mbox{vector}_{\color{black}i}^{\color{red}W})_{i=1}^N, ({\color{blue} \mbox{label}_i})_{i=1}^N \Big)$ in {\color{red} W}};
	

	\draw[->,>=latex,thick, color=blue!60,line width=1mm] (etiq) to (perte);
	\draw[->,>=latex,thick, color=blue!60,line width=1mm] (etiq2) to (perte);
	

	\node (Wpictures) at (-0.1cm,-4cm) {{\color{red} $W$:}};

	\node (Wpictures2) at (2.55cm,-4.5cm) {};
	\node (classifieur) at (1.1cm,-4.5cm) {};

	\node (perte3) at (6.92cm, -5.2cm) {};

	\node (lesweights) at (5cm, -5.4cm) { { \color{red} $\left . \begin{array}{ccccc}
	\\
	\\
	\\
	\\
	\\
	\end{array} \right \}$}};
	\node (w) at (5.4cm, -5.2cm) {};
	\draw[->,>=latex,thick, color=red!60, line width=1mm] (perte3) to (w);

% to rename files in a directory in a correct way:
% compteur=0
% for file in *.png; do compteur=$((compteur+1)); echo $compteur; mv $file $compteur.png; done
%%%% WARNING!!! IT REMOVE SOME FILES!


	\setcounter{moncompteur}{171}

	\coordinate (ligne01) at (0cm,-4.4cm); 
	\coordinate (ligne02) at (5cm,-4.4cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne01= \y,ligne02=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}	


	\coordinate (ligne11) at (0cm,-4.65cm); 
	\coordinate (ligne12) at (5cm,-4.65cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne11= \y,ligne12=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	
	\coordinate (ligne21) at (0cm,-4.9cm); 
	\coordinate (ligne22) at (5cm,-4.9cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne21= \y,ligne22=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne31) at (0cm,-5.15cm); 
	\coordinate (ligne32) at (5cm,-5.15cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne31= \y,ligne32=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne41) at (0cm,-5.4cm); 
	\coordinate (ligne42) at (5cm,-5.4cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne41= \y,ligne42=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne51) at (0cm,-5.65cm); 
	\coordinate (ligne52) at (5cm,-5.65cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne51= \y,ligne52=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne61) at (0cm,-5.9cm); 
	\coordinate (ligne62) at (5cm,-5.9cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne61= \y,ligne62=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne71) at (0cm,-6.15cm); 
	\coordinate (ligne72) at (5cm,-6.15cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne71= \y,ligne72=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne81) at (0cm,-6.4cm); 
	\coordinate (ligne82) at (5cm,-6.4cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne81= \y,ligne82=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}
\end{tikzpicture}
\caption[Example of classifier training for computer vision]{Example of classifier training for computer vision:
the labeled picture dataset NORB, see Figure \ref{NORB},
is used to train and test a classifier.
The learning algorithm is based on Convolutional Network \cite{Lecun98gradient-basedlearning,DBLP:journals/jfr/SermanetHSGBECML09}
using gradient methods \cite{bottou-91c,lecun-98x,bottou-tricks-2012}.
The weights $W=(w_0,\ldots,w_m)$ are the parameters of a particular transformation 
(see the ``bi-pyramid'' of successive transformation stages) 
from the picture to a vector representing a label among $\set{animal,car,human,nothing,plane,truck}$. 
These weights are learned in order to minimize a given loss function
\textit{i.e.} a proper criterion representing the error of the classifier over the dataset.
A classical loss function is the Mean Squared Error (MSE). 
The environment Torch7 (based on lua and C languages, \cite{Collobert_NIPSWORKSHOP_2011})
has been used to compute the displayed weights.
 }
\label{CV_algoConvNet}
\end{figure}
%%
%% CONVNETS END
%%

\begin{figure}[b!] \centering
\begin{tabular}{c|c|c|c|c|c!{\vrule width 2pt}c!{\vrule width 2pt}c}%!{\vrule width 2pt}}
animal & human & plane & truck & car & nothing \\ \specialrule{.2em}{.0em}{.0em}  
$3688$ & $575$ & $256$ & $48$ & $144$ & $149$ &   animal & $75.885\%$ \\ \specialrule{.05em}{.0em}{.0em}  
$97$ & $4180$ & $81$ & $20$ & $225$ & $257$ & human & $86.008\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$292$ & $136$ & $3906$ & $237$ & $202$ & $87$ & plane & $80.370\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$95$ & $1$ & $44$ & $4073$ & $514$ & $133$ & truck & $83.807\%$  \\  \specialrule{.05em}{.0em}{.0em}  
$129$ & $3$ & $130$ & $1283$ & $3283$ & $32$ &  car & $67.551\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$154$ & $283$ & $36$ & $63$ & $61$ & $4263$ & nothing & $87.716\%$  \\ %\specialrule{.2em}{.0em}{.0em}  
\end{tabular}
\caption[Example of confusion Matrix for multiclass classification]{
Example of confusion matrix for multiclass classification:
this matrix is computed with a testing dataset of pictures, different from the training dataset.
Each row only considers the pictures of a given object,
and numbers represents the answers of the classifier: 
for instance, $3688$ pictures of animals are well recognized, 
but $575$ are confused with a human.
Average row correct is here $80.223\%$.
Torch7 environment \cite{Collobert_NIPSWORKSHOP_2011} has also been used 
to compute this matrix from the classifier and the testing dataset.}
\label{confusion_matrix}
\end{figure}


%
% IMPRECISION POMDP WORKS TODO TODO TODO
%
Some variations of the POMDP framework 
have been built in order to take into account 
the imprecision about the probability distributions of the model, 
also called \textit{parameter imprecision}.
\subsubsection{Works handling parameter imprecision}
Here, transition and observation functions,
namely $\textbf{p} \paren{ s' \sachant s,a }$ 
and $\textbf{p} \paren{ o' \sachant s',a }$,
$\forall (s,s',o',a) \in \mathcal{S}^2 \times \mathcal{O} \times \mathcal{A}$,
are called \textit{POMDP parameters}, or yet \textit{model parameters}. 
To the best of our knowledge,
the first model meant to handle parameter imprecision
has been called POMDPIP, 
for \textit{POMDP with Imprecise Parameters} (POMDPIP) \cite{Itoh2007453}. 
This work considers sets of possible parameters
instead of only one. Figure \ref{uncertainty_theories}
introduces the Imprecise Probability Theory 
which has inspired this idea:
in this theory, uncertainty and knowledge about it 
are represented by set of probability distributions
(called \textit{credal sets})
to express the fact that the model is not fully known.
Imprecision of the POMDP model is well expressed in this way:
for instance, if confidence bounds on the computed probabilities
are available, 
they can be taken into account when modeling,
considering the set of all the likely 
probability distributions. 

Starting from this modeling, 
a \textit{second order belief}
is introduced in this work:
it is defined as a probability distribution 
over the possible model parameters.
This approach confuses thus 
parameter imprecision and event frequency.
Indeed, strategies which are looked for
are the optimal strategies
of a particular ``averaged POMDP'': 
this POMDP results from the computation of
the average of the possible parameters 
with respect to the given second order belief.
However, the parameters of this new POMDP 
are potentially not even part of the set 
of the possible parameters.
Although POMDPIPs are well designed, 
imprecision is not really handled when 
the problem is solved
because of the use of a second order belief.
Moreover, it is claimed that the actual second order belief 
is not known, and then is allowed to vary in order to make 
the computation of an approximated strategy easier:
analysis of performances only details 
the extent to which the approximated strategy 
is far from the ``averaged POMDP''.	

An other work, called \textit{Bounded Parameters POMDPs} (BPPOMDP) \cite{NiYaLiaZhi} 
does not introduce any second order belief. 
However, solving BPPOMDPs is similar as \cite{Itoh2007453} in spirit
since the set of possible one are choosing to make the computations as easy as possible.
In this framework, the criterion defining optimal strategies 
is not even made explicit.

The major issue with these frameworks 
(namely POMDPIP and BPPOMDP)
is that no appropriate criterion choice is made
to manage parameter imprecision:
for instance, a suitable approach is 
to compute a cautious strategy 
regarding the imprecision \textit{i.e.} 
taking into account the worst case.
Yet, the opposite approach is to look 
for an optimistic strategy, 
considering the best case.

A more recent approach deals 
with the cautious point of view,
and is thus called 
\textit{robust POMDP} \cite{DBLP:conf/icml/Osogami15}.
Inspired by the corresponding work 
in the fully observable case 
(called \textit{uncertain MDP} \cite{NE:05}),
this work uses the well-known \textit{maximin},
or \textit{worst case} criterion,
which comes from \textit{Game Theory}: 
in this framework, an optimal strategy 
maximizes the lowest criterion
among all the parameters considered as possible.
If the parameter imprecision is not stationary,
\textit{i.e.} can change at each time step,
the corresponding optimal strategy (in the maximin sense)
can be easily computed using classical computations
(called \textit{Dynamic Programming}, see Section \ref{subsectionDP} of Chapter \ref{chap_SOTA}).
However, when the parameter imprecision is stationary,
things become harder:
the proposed computations lead to an approximately optimal strategy,
as the handled criterion is a lower bound
of the desired maximin criterion. 
Yet, the stationary assumption for the parameter imprecision seems more suitable 
according to the proposed POMDP definition:
indeed, transition and observation functions are here defined independent from the time step,
mostly to make the use of infinite horizon criteria such as (\ref{criterion}) easier.

Although the use of sets of probability distributions 
(credal sets) makes the model more consistent 
with the reality of the problem (parameters are imprecise in practice),
taking them (really) into account (e.g. with a maximin criterion) 
increases eventually the complexity 
of the optimal strategy computation.
This intuition is quite well illustrated by Figure \ref{uncertainty_theories}:
Imprecise Probability Theory has the greatest expressiveness 
compared to the other theories,
that is why computations with this modeling 
can be expected to be harder to perform.
As explained above, POMDP solving is already a really hard task,
thus using a framework leading to more complex computations
does not seem to be a good approach. 
Moreover, with this framework, 
some restrictive assumptions are commonly needed
such like the convexity of credal sets,
and optimization may require linear programs \cite{NE:05,DBLP:conf/icml/Osogami15}.
%As discussed during this thesis 
%(for instance see Section \ref{section_expe_PPUDD} of Chapter \ref{chap_symb}),
Modeling a problem in an expressive manner may lead
to the use of important approximations without control of the latter,
as done with the POMDPIP and BPPOMDP frameworks.
It may be a better approach to start with a simpler model
which can be solved easily in practice. 

Finally, while there are fascinating questions of algorithms in this area,
this thesis does not deal with Reinforcement Learning (RL, \cite{NIPS2009_3780,Ross:2011:BAL:1953048.2021055}), 
as we are interested in robotic applications 
which have to well behave during very first executions.
The basic reason is that robotic systems may be expensive,
and executions are not allowed to break the robot.
The problem of interest here 
is called \textit{offline POMDP solving}.
However, they are complementary studies 
since statistical learning could be performed
during each of the executions performed 
with the offline solution (long-term learning).

\begin{figure} \centering
\begin{tikzpicture}
\node (IP) at (2,10) {\textbf{Imprecise Probability Theory} \cite{Walley2000125}};
\node (BF) at (2,8) {\textbf{Belief Functions Theory} \cite{dempster1967,shafer1976mathematical}};
\node (PROB) at (-1,6) {\textbf{Probability Theory}};
\node (POSS) at (5,6) {\textbf{Possibility Theory} \cite{Du2006.7,Dubois1997359}};
\draw[->,>=latex,thick, color=blue!60, line width=2mm, dashed] (IP) to (BF);
\draw[->,>=latex,thick, color=blue!60, line width=2mm, dashed] (BF) to (PROB);
\draw[->,>=latex,thick, color=blue!60, line width=2mm, dashed] (BF) to (POSS);

\node (topr) at (8.5,10) {};
\node (botr) at (8.5,6) {};
\node (middr) at (8,7.6) [rotate=90] {expressiveness};
\draw[->,>=latex,thick, color=red!60, line width=2mm] (botr) to (topr);
\draw (-6,7) rectangle (-3,10);
\node (toplegend) at (-3.75,10) {};
\node (botlegend) at (-3.75,7) {};
\draw[->,>=latex,thick, color=blue!60, line width=2mm, dashed] (toplegend) to (botlegend);
\node (bluelegend) at (-5,9) {particular};
\node (bluelegend) at (-5,8.5) {case of};
\end{tikzpicture}
\caption[Most known uncertainty theories and their relations]{Most known uncertainty theories and their relations. 
Imprecise Probability Theory (IPT) 
considers sets of probability measures
defined on the universe $\Omega$ 
in order to represent uncertainty of events 
and knowledge about it: 
this theory is the most general one. 
The Belief Functions Theory 
(BFT, or Dempster-Shafer Theory, or yet Theory of Evidence),
which is a particular case of IPT, 
considers a mass function $m$ 
defined on all the subsets of the universe,
and which sums to $1$. 
From this mass function,
upper and lower bounds
on possible probability measures
can be defined. 
The upper bound is called
plausibility measure: $\forall A \subset \Omega$, 
$Pl(A) = \sum_{B \cap A \neq \emptyset} m(B)$.
The lower bound is called the belief function:
$\forall A \subset \Omega$, 
$bel(A) = \sum_{B \subset A} m(B)$.
The set of probability distributions
represented by $m$ are all measures $\mathbb{P}$
such that $\forall A \subset \Omega$,
$bel(A) \leqslant \mathbb{P}(A) \leqslant Pl(A)$.
For instance, 
if the function $m$ returns $1$ for the universe,
and $0$ for all other subsets, 
it expresses the total ignorance 
about the actual probability distribution:
it corresponds to the set of all the possible 
probability distributions in IPT.
If the mass function is equal to zero 
for each non-singleton subsets,
it corresponds simply to a probability distribution
since $bel=Pl$.
If the mass function is positive only on a
sequence of nested subsets, 
it corresponds to a possibility distribution:
$bel$ is then called the \textbf{necessity measure},
and $Pl$ the \textbf{possibility measure} (see Section \ref{posspres} of Chapter \ref{chap_SOTA}).
Probability and Possibility Theory are thus
particular cases of BFT and a fortiori of IPT.}
\label{uncertainty_theories}
\end{figure}

Another practical issue of the POMDP model
may be also pointed out: 
it is about the definition of
the belief state concerning the very first
state of the system,
and more generally about
the way to represent agent knowledge.
%%
%%  FULL IGNORANCE/ KNOWLEDGE OF THE AGENT
%%
\subsection*{Agent ignorance modeling}
The initial belief state $b_0$, 
or \textit{prior} probability distribution 
over the system states, 
takes part in the definition of the POMDP problem.
Given a system state $s \in \mathcal{S}$, 
$b_0(s)$ is the frequency 
of the event ``the initial state is $s$''. 
This quantity may be hard to properly compute,
especially when the amount of available past experiments 
is limited: this reason has been already invoked above,
leading to the imprecision of the transition and observation 
functions.

As an example, consider a robot that is for the first time in a room
with an unknown exit location (initial belief state) 
and has to find the exit and 
reach it. 
In practice, no experience can be repeated 
in order to extract a frequency of the location of
the exit. 
In this kind of situation,
uncertainty is not due to a random fact, 
but to a lack of knowledge: 
no frequentist initial belief state can be used to define the model.

In other cases, 
the agent may strongly believe 
that the exit is located 
in a wall as in the vast majority of rooms, 
but it still grants a very small probability $p_{\epsilon}$ 
to the fact that the exit may be a staircase 
in the middle of the room. 
Even if this is very unlikely to be the case,
this second option must be taken into account in the belief state, 
otherwise Bayes rule (see Equation \ref{probBayesRule}) 
cannot correctly update it 
if the exit is actually in the middle of the room. 
Eliciting $p_{\epsilon}$ without past experience 
is not obvious at all 
and does not rely on any rational reasons, 
yet it dramatically impacts the agent's strategy. 

The initial system state 
may be deliberately stated as unknown by the agent 
with absolutely no probabilistic information:
consider robotic missions 
for which a part of the system state,
describing something that the robot
is supposed to infer by itself, 
is initially fully unknown.
In a robotic exploration context, 
the location or the nature of a target, 
or yet the initial location of the robot
may be defined as absent from the knowledge of the agent.
Classical approaches initialize the belief state as 
a uniform probability distribution
(\textit{e.g.} over all robot/target possible locations, 
or over possible target natures), 
but it is a subjectivist answer \cite{de1974theory,Dubois96representingpartial}.
Indeed, all probabilities are the same
because no event is more plausible than another:
it corresponds to equal betting rates.
However following belief updates (see Equation \ref{probBayesRule}) 
will eventually mix up frequentist probability distributions 
(transition and observation functions) 
with this initial belief which is a subjective probability distribution:
it does not always make sense, 
and it is questionable in any cases. 
Thus, the use of POMDPs in these contexts, 
faces the difficulty to encode agent ignorance.
 
Since the knowledge of the agent 
about given features of the system 
may be initially partial (or even absent),
it makes sense to pay attention to the evolution of 
this knowledge during the execution of the considered process.
Some works inspired by the field of \textit{Active Perception}, 
have been concerned about the information gathered 
by the agent in the POMDP framework
\cite{conf/stairs/ChanelFTI10,oatao11449}.
In these works, 
the \textit{entropy} of the belief state
is taken into account in the criterion, 
making the computed strategy 
ensure that reached belief states have a lower entropy
(\textit{i.e.} are more close to a deterministic probability distribution)
while always satisfying the requirement of an high expected total reward.
This approach leads to really interesting results
in practice.

Nevertheless, a tradeoff parameter is introduced
making the entropy part 
of the criterion more or less important: 
tuning the latter may
add additional computations.
Moreover this approach
does not distinguish
between the actual frequentist behavior of the system state, 
and the lack of knowledge about the actual belief state
(as a probability distribution)
due to the imprecision about the initial belief state,
or yet about the transition and observation function:
it just tends to make the belief as deterministic as
possible, even if it is not possible due to the actual
probability distributions of the model 
(e.g. if the transition probability distribution 
$\textbf{p} \paren{s_{t+1} \sachant s_t,a_t}$ has an high entropy for each actions $a_t \in \mathcal{A}$),
and even if it is not needed 
(e.g. if each system states $s \in \mathcal{S}$ such that the belief state $b_t$
is not zero, $b_t(s)>0$, lead to an high reward).
%In other words, the POMDP framework 
%defines the successive belief states
%as probability distributions
%instead of sets of possible ones:
%measure of knowledge about the actual system state 
%(in a frequentist point of view)
%is mixed up with the measure of knowledge about the model.

However, the proposed criterion (including the entropy of the  belief state) 
has been shown to make the agent estimate faster its real state,
which is really useful in some practical problems.
Other models have been introduced, making the reward function dependent
on the belief state 
\cite{conf/nips/Araya-LopezBTC10,oatao11437}:
this allows to make the agent behavior vary with respect to
the probabilistic representation of its knowledge.

%it may make the  actually the real  the POMDP is in practice, a belief state may have an high entropy
%not because of the imprecision of the parameters, 
%may be not related to the agent's lack of knowledge in the probabilistic about the ,
%but rather to the 
%(fully known) 
%variance of the system state:
%hence the agent does not know 
%which is the actual system state 
%since its  is far from being deterministic,
%but it may perfectly know how the state behaves,
%and then nothing can be improved concerning its knowledge.

\textbf{CEST LA FIN MON AMOUR!! (1)}

%and knowledge of the agent CARO \\

%
% WHAT WE WOULD LIKE TO DO
%
\section*{General problem}
Previous sections presented some issues encountered in practice 
when using the POMDP framework to compute strategies, 
especially in the robotic context. 
The really high complexity of the problem of computing an optimal strategy 
is a first point: robotic missions often result in high dimensional problems
preventing algorithms from the computation of sufficiently good strategies.
Second we presented the difficulty of defining the probability distributions
of a POMDP encoding a given problem 
(e.g. defining the observation function when using computer vision algorithms).
Finally the problem of managing the knowledge of the agent has been discussed:
defining the initial agent's lack of knowledge about the actual world,
as well as handling and taking into account the current knowledge,
is an hard task: 
the difficulty comes from the POMDP definition
which only allows the use of 
frequentist probability distributions,
while more expressive mathematical tools are needed.

These problems are the starting points of our work.
Indeed, the latter consists in contributing to the problem
of computing strategies for partially observable domains.
The computed strategies have to make the robot fulfill the mission
as well as possible, from the very first execution,
\textit{i.e.} computations are performed before any execution of the mission:
for instance, RL algorithms 
may lead to improvements of the computed strategy
at each execution of the robotic mission,
but are not considered in this work.
%to this purpose, a criterion
%(e.g the discounted expected total reward of Equation \ref{criterion})
%is maximized.
The general challenge of this work 
is to proceed to these computations 
using only data and knowledge about the problem 
which are really available in practice,
and paying particular attention to the issues pointed out above:
namely, the complexity of the strategy computation,
the imprecision of the model, 
and the management of the agent's knowledge.

Although the use of a more expressive framework 
may makes strategy computations harder, 
the issues about the parameter imprecision 
and about agent knowledge managing 
lead us to consider alternative uncertainty theories 
as those described in Figure \ref{uncertainty_theories}. 
As presented above, the Imprecise Probability Theory (IPT) 
has been already explored to improve the POMDP framework
\cite{Itoh2007453,NiYaLiaZhi,DBLP:conf/icml/Osogami15}.  
However, a tradeoff
between the proper use of the credal sets 
and the computation time
has to be reached. 
A less expressive uncertainty theory 
is called the Belief Functions Theory 
(BFT, see Figure \ref{uncertainty_theories}): 
this theory encodes the uncertainty of a given situation
with a mass $m$ defined on each disjunctive subsets of the finite universe $\Omega$:
$m : 2^{\Omega} \rightarrow [0,1]$, with $\sum_{A \subseteq \Omega} m(A) = 1$.
Recall that the frequentist probability of an elementary event $\omega \in \Omega$ 
to occur can be computed in practice using the law of large numbers
\textit{i.e.} this probability is the number of times this event occurred, 
over the size of the used dataset of events 
(called sample space, and which as to be sufficiently big). %%% TODO
Suppose that we wanted to compute the frequentist probability distribution 
describing a given situation, but some  
but events were imprecisely observed during this computation,

however allows
The quantity $m(A)$ is defined as the frequency of observing 
one of the elements of this set, 
without a better precision. The total ignorance about the actual probability distribution defining the model is encoded with a mass returning 1 for $\Omega$, and 0 for each other subset: in IPT, it corresponds to the credal set containing all the existing probability distributions. This ability to encode imprecision and lack of knowledge is a good point. Not also that this framework allows to encode the full knowledge about the probability distribution describing the model. Already studied to improve the fully observable MDP framework) (ref),



Because of the impre
The sentence: why have we to consider the alternative 
Figure \ref{uncertainty_theories} attention  particular  goal make us to
taking into account or benefiting from the lack of knowledge

with a model built from imprecise data and
using reasonable computation time.
+ MODELIZATION and interested by the actual knowledge of the agent (CARO)


%
% UNCERTAINTY THEORIES
%
The study of other uncertainty theories may bring useful properties
to deal with this problem.
DEMPSTER: no,belief plausibility evidence: a probability value for each $2^{\#\Omega}$
(to complex too)
 poss quant no, poss qual!!

plausibilité $Pl(A)=\sum_{\set{B \sachant B \cap A \neq \emptyset  } }m(B)$ et la croyance $Bel(A)=\sum_{\set{B \subset A}} m(B)$.
Dempster-Shafer Theory \cite{shafer1976mathematical} less complex than IP but more than proba alone,
fully observable case \cite{trevizan2007planning} \\ 

%
% POSS QUAL
%
\subsection*{A qualitative possibilistic model}
\textbf{Qualitative Possibility Theory:}
$\rightarrow$ simplification, ignorance and imprecision modeling.
The $\pi$-POMDP model is a possibilistic and qualitative counterpart of the
probabilistic POMDP model \cite{Sa1999.5}: it allows a formal modeling of
total ignorance using a possibility distribution equal to $1$ on all
the states. This distribution means that all states are equally possible
independently of how likely they are to happen (no necessary state). 

Finding qualitative estimates of their recognition performance is easier: the
$\pi$-POMDP model only require qualitative data, thus it allows to
construct the model without using more information than really available. 
Constat que les possibilitees qualitatives sont qualitatives, et peux modeliser la connaissance
The Qualitative Possibility Theory allows to handle imprecise data
and model the lack of knowledge (pas forcement le modele)
tropical algebra. (SIMPLIFY or ENCODE AVAILABLE DATA)

The use of the Qualitative Possibility Theory \cite{DBLP:journals/eor/DuboisPS01}
is studied here,
as it appears capable
to both simplify the POMDPs, 
and model imprecision and ignorance 
related to robotic missions.

In our context, distributions defined in the Possibility Theory framework  
are valued in a totally ordered scale $\mathcal{L}=\set{ 1=l_1,l_2,\ldots,0 }$ with
$l_1>l_2>\ldots>0$. A possibility measure $\Pi$ defined on $\mathcal{S}$ is a 
fuzzy measure valued in $\mathcal{L}$, such that $\forall A,B \subset \mathcal{S}$, 
$\Pi(A \cup B) = \max \set{ \Pi(A), \Pi(B) }$, $\Pi(\emptyset)=0$ and $\Pi(\mathcal{S})=1$.
It follows that this measure is entirely defined by the associated possibility distribution,
\textit{i.e.} the measure of the singletons: 
$\forall s \in \mathcal{S}$, $\pi(s) = \Pi(\set{s})$. Properties of this measure lead to
the possibilistic normalization: 
\begin{equation} 
\label{possNormDef}
\max_{s \in \mathcal{S}} \pi(s) = \Pi(\mathcal{S}) = 1.
\end{equation}
If $\overline{s},\underline{s} \in \mathcal{S}$ 
are such that $\pi(\overline{s})<\pi(\underline{s})$, it means
that $\overline{s}$ is less plausible than $\underline{s}$. States with possibility
degree $0$, \textit{i.e.} states $s \in \mathcal{S}$ such that $\pi(s)=0$, are impossible
(same meaning as $\textbf{p}(s)=0$), and those such that $\pi(s)=1$ are entirely possible
(but not necessary the most probable one).

note the similarities between Possibility and Probability Theory, 
replacing $\max$ by $+$ and $\min$ by $\times$. Moreover, Possibility Theory has its
own counterpart of the Bayes rule \cite{Dubois199023}: 



the indicator (chararacteristic) function of this set.
Unlike classical sets, values of a fuzzy set indicator function $\pi$
are not only in $\set{0,1}$. Recall that the indicator function of
a classical set $A \subseteq \mathcal{S}$ is $\mathds{1}_{A}(s) = 1$ 
if $s \in A$ and $0$ otherwise.
Values of a fuzzy set indicator function are chosen in a totally ordered scale 
$\mathcal{L} = \set{ 1=l_1, l_2, \ldots, 0}$ with $l_1>l_2>\ldots>0$: 
$\pi: \mathcal{S} \rightarrow \mathcal{L}$. If $s \in \mathcal{S}$ is such that
$\pi(s)=l_i$, $s$ is in the fuzzy set described by $\pi$, with degree $l_i$.  
Possibilistic beliefs used in this work will represent fuzzy sets of possible 
states. If the current possibilistic belief coincide with the distribution
$\pi(s)=1$ $\forall s \in \mathcal{S}$, all system states are totally possible,
and it models therefore a total ignorance about the current system state:
qualitative possibilistic beliefs can model agent initial ignorance.
The full knowledge of the current state, say $\tilde{s} \in \mathcal{S}$, 
is encoded by a possibility distribution equal to the classical indicator
function of the singleton $\pi(s) = \mathds{1}_{\set{ s=\tilde{s} }}(s)$.
Between these two extrema, current knowledge of the system is described by
a set of entirely possible states, $\set{ s \in \mathcal{S} \mbox{ s.t. } \pi(s)=1  }$,
and successive sets of less plausible ones $\set{ s \in \mathcal{S} \mbox{ s.t. } \pi(s)=l_i}$
down to the set of impossible states $\set{ s \in \mathcal{S} \mbox{ s.t. } \pi(s)=0  }$.

Les $\pi$-POMDPs permettent une mod\'elisation formelle de 
la m\'econnaissance de l'agent, bien que ce mod\`ele n'ait pas \'et\'e \'etudi\'e de 
mani\`ere approfondie. \\
%
% piPOMDP
%

PSR \ref{Littman01predictiverepresentations}

Enfin, un homologue des POMDPs appel\'e $\pi$-POMDP a \'et\'e d\'evelopp\'e dans 
le cadre de la Th\'eorie des Possibilit\'es \cite{Sabbadin:1999:pipomdp}, 
r\'eduisant la complexit\'e du probl\`eme. De plus, ce cadre possibiliste 
d\'ecrit de mani\`ere qualitative l'incertitude de
l'agent, ce qui permet de rester prudent dans la d\'efinition des param\`etres du probl\`eme. 
Cependant, ce mod\`ele n'a \'et\'e
que tr\`es peu \'etudi\'e, et les recherches le concernant peu abouties.\\


Qualitative possibilistic POMDPs, 
$\pi$-POMDPs \cite{Sabbadin:1999:pipomdp}, 
are alternative processes 
defined using a qualitative evaluation 
of events plausibility
instead of probabilities: 
it allows to formally represent 
agent ignorance, 
and imprecision on observations hazard. 
As number of belief finite, and MDP P complexity \cite{DBLP:journals/corr/abs-1202-3718},
at most exponential the process description.



\section*{Description of our Study}
% Le sujet de la thèse
The goal of this thesis is to show what the Qualitative Possibility Theory can bring 
in Planning under Uncertainty and Sequential uncertainty management in practice, 
in particular through Graphical models,
in terms of simplification and modeling. 
\dots\\*
This paper presents recent contributions 
in the use of the Qualitative Possibility Theory
for planning under uncertainty, 
studied to answer to these concerns.




% ANNONCE DU PLAN DE LA THÈSE
Our contributions consists in : \dots \\*

	
JUSTIFIER LES IDEES FORTES, ET VERIFIER QUIL Y A DEUX IDEEES FORTES par sous section de chaque chapitre
\\


%%%% CHAP1
\subsection*{State of the art}
The \emph{first chapter} presents in parallel \\*

piPOMDP detailed, as never detailed much
to the best of our knowledge

The work developed in this paper remains in the classical MDP and POMDP frameworks, 
which are recalled in this chap: 
possibilistic material necessary to build the promised translation are then presented. 

Ces modèles probabilistes sont conçus pour représenter de manière simple une situation où une entité (appelée \textit{agent}) 
et le monde qui l'entoure peuvent être dans différents états $s \in \mathcal{S}$ au cours du temps 
(ce dernier, modélisé par l'ensemble des entiers naturels $\mathbb{N}$). 
L'agent choisit une action parmi celles dont il dispose à chaque étape de temps $t \in \mathbb{N}$, 
et le système comprenant l'agent et son environnement évolue de manière Markovienne dans $\mathcal{S}$ (\cite{bellman},\cite{ber01}).

Le modèle POMDP, autour duquel ce travail est développé, est utilisé lorsque le système peut se modéliser à l'aide de probabilités. 
Il permet aussi bien de modéliser les incertitudes sur l'observation du système, que sur sa dynamique. 
Nous commencerons donc par présenter un modèle plus simple (MDP, ou l'observation est parfaite) afin de construire plus facilement le modèle POMDP.\\

Dans le modèle POMDP, les distributions de probabilité sont supposées parfaitement connues a priori. En pratique, ce n'est pas toujours le cas, notamment lorsque les observations de l'environnement sont filtrées par des algorithmes de traitement du signal. De plus, l'agent est supposé avoir une croyance initiale de l'état du système: sa croyance initiale n'est pas précise en pratique (extraire des fréquences d'évènements initiaux n'est pas forcément possible), et une probabilité uniforme ne représente pas réellement cette méconnaissance. Il existe aussi des situations dans lequelles, donner de l'importance à l'information de la croyance (au sens entropique) améliore la politique en pratique \cite{conf/stairs/ChanelFTI10}. \\

Ces problèmes pratiques peuvent trouver des solutions grâce à d'autres modélisations de l'incertitude que la théorie des probabilités. C'est pourquoi ce rapport s'intéresse ensuite à des modèles utilisant la théorie des possibilités ($\pi$-MDP et $\pi$-POMDP). Ces deux modèles introduits par R.Sabbadin sont présentés, puis une preuve est donnée à l'homologue de l'algorithme d'itération sur les valeurs. Enfin, l'astuce de O.Buffet \textit{et al.} est adaptée pour le modèle $\pi$-POMDP. 



%%% CHAP2
\subsection*{Natural updates of the possibilistic model}
The \emph{second chapter} proposes \\* 

It begins with an update 
of the work of Sabbadin 
about a possibilistic counterpart 
of POMDPs called $\pi$-POMDP. 
a possibilistic version of 
Mixed-Observable MDPs \cite{OngShaoHsuWee-IJRR10}, 
called $\pi$-MOMDP \cite{Drougard13}, 
is first presented to reduce dramatically 
the complexity of solving $\pi$-POMDPs, 
some state variables of which are fully observable. 
An algorithm
for missions with unbounded durations
is next proposed:
returned strategies can outperform 
probabilistic POMDP ones 
for a target recognition problem 
where the agent's observations dynamics 
is not properly defined.
\cite{Drougard13}

La d\'emarche a consist\'e dans un premier temps \`a adapter les processus d\'ecisionnels 
possibilistes qualitatifs aux missions robotiques car ils modelisent formellement la meconnaissance initiale. 
Pour cela, le probl\`eme $\pi$-POMDP
est d\'efini pour un horizon infini, afin que les missions puissent 
\^etre d\'efinies sans d\'ecider a priori de la dur\'ee de la mission. Sachant l'\'etat 
du syst\`eme n'est pas enti\`erement cach\'e en pratique lors des missions \cite{OngShaoHsuWee-IJRR10}, 
les $\pi$-MOMDPs, pour (Mixed-Observable MDPs) 
sont introduits afin de profiter de cette observation dans la r\'eduction du temps de calcul de la
strat\'egie.
Par exemple, le niveau de batterie d'un drone est accessible directement par le syst\`eme robotique. 

 L'algorithme associ\'e \`a ces contributions ainsi que sa 
preuve de convergence est alors n\'ecessaire dans le but de calculer une strat\'egie \cite{Drougard13}. 
Enfin, les performances de ce nouveau 
mod\`ele sont illustr\'ees \`a l'aide de simulations de probl\`emes particuliers pour diff\'erents crit\`eres.
De plus, ces simulations illustrent aussi la dynamique de la croyance de l'agent: sa mise \`a jour
poss\`ede la caract\'eristique int\'eressante d'accro\^itre la connaissance associ\'ee.
Cependant, la r\'esolution propos\'ee reste limit\'ee \`a des probl\`emes de faible taille.\\




%%% CHAP3
\subsection*{Factorization work on the models}
The \emph{third chapter} is \\* 

\cite{DBLP:conf/aaai/DrougardTFD14}
Then strategies computation time is decreased 
with the use of Agebraic Decision Diagrams (ADDs)
and benefiting from the problem structure.
We compare performances 
of our solvers with those of 
its probabilistic counterparts, 
in terms of computation time, 
and with criteria measuring 
the mission achievement. 
While this possibilistic framework 
provides good results,
some highlighted issues
are finally discussed: 
Then factorized $\pi$-MOMDPs
are defined making possible the processing
of large structured planning problems. 
Building upon the probabilistic 
SPUDD algorithm \cite{Hoey99spudd:stochastic},
we conceived an algorithm named PPUDD \cite{DBLP:conf/aaai/DrougardTFD14} 
for solving factorized $\pi$-MOMDPs
using \textit{Algebraic Decision Diagrams} (ADD).
Our experiments and the  results 
of the International Probabilistic Planning Competition (IPPC 2014)
show that this possibilistic approach
can involve lower computation time
and produce better policies
than its probabilistic counterparts:
it highlights also some issues 
of these qualitative models.
Finally, points raised are added to 
the symbolic computations limits,
and lead to the description
of a future work.
We participated in the competition in order to test the performance of our algorithm
in the case of well known probabilistic models.


LETAPE SUIVANTE A ETE DADAPTER LALGO AFIN QUE...
Une fois ce mod\`ele adapt\'e aux besoins robotiques pratiques, nous avons souhait\'e pouvoir
r\'esoudre des probl\`emes de d\'ecision s\'equentielle dans l'incertain de tailles
plus g\'en\'erales. Profitant \`a la fois du caract\`ere qualitatif de ces processus,
ainsi que de la structure des probl\`emes en pratique, un algorithme
symbolique de r\'esolution \cite{DBLP:conf/aaai/DrougardTFD14} est propos\'e. Pour cela, nous 
introduisons les $\pi$-MOMDPs factoris\'es, et d\'ecrivons sur les hypoth\`eses d'ind\'ependance
possibles pour de tels processus.
AFIN DE..
\\
ON a voulu que cet algorithme utilise les ADD pour pouvoir synthetiser les calculs.

L'algorithme r\'esultant, utilisant des arbres de d\'ecision
alg\'ebriques (ADDs) afin de synth\'etiser les calculs, est nomm\'e PPUDD pour \textit{Possibilistic
Planning using Decision Diagrams}
\\
POUR CELA, on a simulté des problemes de plannif variés.
 et a \'et\'e test\'e lors de la comp\'etition internationale 
de planification probabiliste (IPPC14). 
\\
EN TERMES DE REWARD EXPECTED

Ces tests sur des probl\`emes de planification vari\'es m\`enent \`a l'observation que 
ces m\'ethodes \`a ADDs, dites \textit{symboliques}, ne font pas le poids face aux
approches proc\'edant \`a une recherche heuristique dans l'espace d'\'etat \cite{DBLP:conf/aips/KellerE12} (en termes de rewards... expliquer!). 
Nous avons pu aussi prendre note des inconv\'enients du formalisme possibiliste qualitatif pour la mod\'elisation:
le crit\`ere utilis\'e \'etant global, son choix restreint la g\'en\'eralit\'e du mod\`ele.\\


%%% CHAP4
\subsection*{Qualitative possibilistic framework process for human-machine interaction modeling}
The \emph{fourth chapter} is \\* 

Les processus possibilistes qualitatifs ont toutefois leur place lorsque toutes les distributions
de probabilit\'e ne peuvent clairement pas \^etre d\'efinies dans le contexte: dans les syst\`emes
mod\'elisant le comportement human, seules des donn\'ees expertes peuvent \^etre utilis\'ees pour
la mod\'elisation. Pour cela, nous appliquons ces processus... \textbf{Application des processus possibilistes en diagnostique dans l'interaction homme-machine} \\

Processus $\pi$-HMP hidden markov processes, outils de diagnostic pour l'Intéraction Homme-Machine (avec Sergio Pizziol)
\begin{itemize}
\item \textbf{occurrences:} états de la machine et actions humaines;
\item \textbf{évaluation humaine} (de l'état de la machine);
\item \textbf{effets:} transitions, classées par degrés de possibilité.
\end{itemize}
\begin{itemize}
\item \textbf{estimation} de l'état selon l'opérateur humain;
\item \textbf{détection} des erreurs humaines d'évaluation de l'état; 
\item causes plausibles de ces erreurs (\textbf{diagnostique}).
\end{itemize}




%%% CHAP5
\subsection*{Probabilistic-possibilistic approach: an hybrid perspective}
Finally, the \emph{fith chapter} is \\* 

argues for a hydrid POMDP 
model with both probabilistic 
and possibilistic settings. 



