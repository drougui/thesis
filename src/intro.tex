\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\section*{Context}
%%% CONTEXT
\malettrine{P}{roviding} the autonomy to a robot 
consists in computing a function which returns 
the name of the actions to be triggered
at a given moment, with respect to the data from its sensors.
The features of interest of the robot 
and its  surroundings form a \textit{system}.
In general, for a given sequence of actions performed by the robot, 
the evolution of this system 
is not fixed for sure, 
but its behavior may be known 
performing tests on the robot
or using information from expert knowledge.
As well, the raw or processed data from the robot sensors
are not generally a deterministic outcome
of the state of the system nor the taken actions:
nevertheless, these data, 
called also \textit{observations} of the system, 
depend on robot's actions and system states. 
Relations between observations, system states and actions 
may be known through tests of the sensors
in various situations, 
or by taking into account the sensors description, 
the data processing, or any related expert information.
For instance, in the case of a robot using Computer Vision (CV),
the output of the picture processing algorithm employed
is considered as an observation of the system
since it is the result of processed sensor data:
here data are pictures from camera.
For a given camera, and a given vision algorithm,
the behavior of the observation
is related to the action and the system state
during the picture taking process. 

Thus, in order to make a robot autonomously fulfill a chosen \textit{mission}, 
we are  looking for a function returning actions 
conditional on the sequence of system observations,
and taking into account the uncertainty 
about the system evolution and the observation of it.
Such functions may be called \textit{strategies}.
The research domain associated to this kind of problem,
\textit{i.e.} strategy computation,
is not restricted to robotics and
is called \textit{sequential decision making under uncertainty}:
in the general case,
the entity which has to act is called the \textit{agent}.
In this thesis, 
the problem of strategy computation 
is studied in the context of autonomous robotics,
and the agent is the decisional part of the robot.
Computing a strategy for a given robotic mission needs a proper framework:
the best known model describes the state and observation behaviors
using Probability Theory.

\subsection*{A probabilistic model for strategy computation}

Markov Decision Processes (MDPs) 
define a useful formalism 
to express sequential decision problems 
under probabilistic uncertainty \cite{Bel}.
It is a well suited framework 
if the actual system state is known by the agent
at each point in time.
In the robotic context,
this assumption means that
the considered mission allows to assume 
that the robot has full knowledge 
of the features of interest via its sensors.
In this model,
a system state is denoted by the letter $s$,
and the finite set of all the possible states is $\mathcal{S}$.
The finite set $\mathcal{A}$ consists
of all possible actions $a \in \mathcal{A}$ 
available to the agent. 
The time is discretized into integers $t \in \mathbb{N}$
which represent time steps of the action sequence.

The state dynamics is assumed to be \textit{Markovian}:
at each time step $t$,
the next system state $s_{t+1} \in \mathcal{S}$,  
only depends on the current one $s_t \in \mathcal{S}$
and the chosen action $a_t \in \mathcal{A}$.
This relation is described by a transition function 
$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t}$
which is defined as the probability distribution
on the next system states $s_{t+1}$ conditional for each action: 
if the action $a_t \in \mathcal{A}$ is selected by the agent, 
and the current system state is $s_t \in \mathcal{S}$, 
the next state $s_{t+1} \in \mathcal{S}$
is reached with the probability denoted by 
$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t }$. 

The mission of the agent is described in terms of rewards:
a reward function $r(s,a) \in \mathbb{R}$ 
is defined for each action $a \in \mathcal{A}$
and system state $s \in \mathcal{S}$,
and models the goal of the agent. 
The more rewards are gathered during an execution of the process, the better:
a realization of a sequence of system states
and actions is considered as well fulfilling the desired mission 
if encountered rewards $r(s_t,a_t)$ are high.
Solving an infinite horizon MDP
consists in computing an optimal strategy, 
\textit{i.e.} a function prescribing actions $a \in \mathcal{A}$
to be taken over time,
and maximizing the mean
of the sum of rewards gathered during an execution:
this mean is computed with respect to the probabilistic behavior of the system state
encoded by transition functions $\textbf{p} \paren{s_{t+1} \sachant s_t,a_t}$.
For instance, a preferred strategy 
may be a function $d$ defined on $\mathcal{S}$,
as the current state is available to the agent, 
and with values in $\mathcal{A}$.
A criterion measuring the accuracy of the strategy $d$ 
may then be the expected discounted total reward: 
\begin{equation}
\label{criterion}
\mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,d_t) },
\end{equation}
where $d_t=d(s_t) \in \mathcal{A}$ 
and $0<\gamma<1$ is a discount factor 
assuring the convergence of the sum.

%%% ROBOT POMPIER
\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\node (rpomp) at (-1,4.7) {\includegraphics[scale=0.9]{robot_pompier}};
\node (bli) at (0,6.5) {};
\node (pomdp3) at (3.9,6) {\color{orange}{$s \in \mathcal{S}$}: \color{black}{\textbf{system state}}};
\node (t1) at (2,6) {};
\node (r1) at (-2.3,5.5) {};
\node (r11) at (1,4.5) {};
\draw[->,>=latex,color=orange!60,line width=1mm] (t1) to (r1);
\draw[->,>=latex,color=orange!60,line width=1mm] (t1) to (r11);
\node (pomdp4) at (5.8,4.45) {\color{blue!60}{$o \in \mathcal{O}$:} \color{black} \textbf{system observation}};
\node (t2) at (3.3,4.4) {};
\node (r2) at (-2.3,5.05) {};
\draw[->,>=latex,color=blue!40,line width=1mm] (t2) to[bend left] (r2);
\node (pomdp5) at (1.8,3) {\color{red}{$a \in \mathcal{A}$:} \color{black} \textbf{agent's action}};
\node (t3) at (-0.3,3) {};
\node (r3) at (-2,4.2) {};
\draw[->,>=latex,color=red!50,line width=1mm] (t3) to (r3);
\node (pomdp6) at (-5.5,5.5) {\color{ggreen}{$b \in \mathbb{P}^{\mathcal{S}}$:} \color{black} \textbf{belief state} }; %%% ORANGE?
\node (pomdp6) at (-3.5,6) {\color{ggreen}{\Huge \textbf{?}}}; %%% ORANGE?
\end{tikzpicture}
\caption[Use of a POMDP for the firefighter robot mission modeling]{
Use of a POMDP for the firefighter robot mission modeling:
in this toy example, the mission of the robot is the fire prevention.
The \textbf{states of the system} $s \in \mathcal{S}$ encode for instance the robot location, 
the water jet orientation, the amount of water used,
the fire location and its level on a scale between ``minor fire'' and  ``big fire'', etc.
Using vision and heat sensors, 
the robot gets \textbf{observations} $o \in \mathcal{O}$ 
which are the raw or processed values from the sensors:
the output of a classifier
whose input is a picture of the scene
(see Figure \ref{observation_robot} 
and \ref{CV_algoConvNet}), 
and which returns the fire level or location
may be encoded in an observation.
Finally, the \textbf{actions of the robot} $a \in \mathcal{A}$ 
are for instance the rotor activations 
impacting the rotation of the robot's wheels, the water pumping,
the orientation of the water jet or sensors etc.
%Uncertainty dynamics is described by conditional probability distributions:
The \textbf{reward function} $r(s,a)$ decreases with the fire level state,
and is decreased by a cost proportional to the amount of water used:
as an optimal strategy maximizes the mean of the sum of the rewards, 
the goal of the robot is thus to attack fires without wasting water.
This mean can be computed knowing the probabilities describing the uncertainty dynamic of the system.
The robot actions $a \in \mathcal{A}$ have a probabilistic effect on the system,
as described by the \textbf{transition function} $\textbf{p} \paren{s' \sachant s,a}$: 
for instance, the activation of wheel rotors modifies the location of the robot,
and the probability of each possible next locations, given the current system state, 
takes part in the definition of the POMDP.
An other example is the action modifying the water jet orientation, 
which redefines the probability of the next fire level given the current system state.
The robot actions $a \in \mathcal{A}$ and next states $s' \in \mathcal{S}$ 
may also influence the observations from the sensors, 
as defined by the \textbf{observation function} $\textbf{p} \paren{o' \sachant s',a}$: 
for instance, the orientation of the vision sensor may modify 
the probability of fire detection or fire level evaluation, 
which are parts of the observations $o' \in \mathcal{O}$. 
Finally, the \textbf{belief state} is the conditional probability distribution 
of the current system state
conditional on all observations and actions up to the current time step: 
as observations and actions are the only data available to the robot,
the belief state can be seen as the robot's guess.}
\label{robot_pompier}
\end{figure}

The assumption that the agent has a perfect 
knowledge of the system state
is quite strong:
in particular, in the case of robots realizing tasks with conventional captors,
the latter are usually unable to provide 
all the features of interest for the mission 
to the robot.
Thus, a more flexible model has been built, allowing a \textit{partial observability}
of the system state by the agent.
%%% ROBOT POMPIER FIN

%%% POMDP
Indeed a Partially Observable MDP (POMDP) \cite{Smallwood_Sondik} 
makes a step further in the modeling flexibility, 
handling situations in which the agent 
does not know directly
the current state of the system: 
it finely models 
an agent acting under uncertainty in a partially hidden environment.

The set of system states $\mathcal{S}$, 
the set of actions $\mathcal{A}$, 
the transition function $\textbf{p} \paren{ s_{t+1} \sachant s_t,a_t}$ 
and the reward function $r(s,a)$ 
remain the same as for the MDP definition. 
In this model, since the current system state $s \in \mathcal{S}$ 
cannot be used as available information for the agent, 
the agent knowledge about the actual system state 
comes from observations $o \in \mathcal{O}$, 
where $\mathcal{O}$ is a finite set. 
%A full definition of this process 
%includes as well the set of 
%possible observations of the system, 
%$o \in \mathcal{O}$. 
The observation function $\textbf{p} \paren{ o_{t+1} \sachant s_{t+1},a_t }$
gives for each action $a_t \in \mathcal{A}$ and reached system state $s_{t+1} \in \mathcal{S}$, 
the probability over possible observations $o_{t+1} \in \mathcal{O}$. 
Finally, the \textit{initial belief state} $b_0(s)$ 
defines the \textit{prior} probability distribution 
over the system state space $\mathcal{S}$. 
An example of usage of a POMDP is presented in Figure \ref{robot_pompier}.

Solving a POMDP consists in computing 
a strategy 
which returns a proper action at each process step, 
according to all received observations and selected actions  
\textit{i.e.} all of the data available to the agent:
a criterion for the strategy may be also the
expected discounted sum of rewards (\ref{criterion}).

%
% belief
%
Most of the POMDP algorithms reason about the \textit{belief state}, 
defined as the probability of the actual system state knowing
all the system observations and agent actions from the beginning.
This belief is updated at each time step using the Bayes
rule and the new observation. 
At a given time step $t \in \mathbb{N}$, 
the belief state $b_t(s)$ is defined 
as the probability that the $t^{th}$ state is $s \in \mathcal{S}$
conditional on all the past actions and observations, 
and with the prior $b_0$:
it estimates the actual system state using the available data,
as the latter is not directly observable.

It can be easily recursively computed using Bayes rule: 
at time step $t$, 
if the belief state is $b_t$, 
chosen action $a_t \in \mathcal{A}$ and new observation 
$o_{t+1} \in \mathcal{O}$, 
next belief is
\begin{eqnarray}
\label{probBayesRule}
b_{t+1}(s')  \propto \textbf{p} \paren{ o_{t+1} \sachant s', a_t } \cdot \sum_{s \in \mathcal{S}} \textbf{p} \paren{s' \sachant s,a_t} \cdot b_t(s).
\end{eqnarray}
as illustrated by the Bayesian Network in Figure \ref{BayesNetPOMDP}.
%
% BAYESNET
%
\begin{figure}\centering
\begin{tikzpicture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%vertex
\tikzstyle{vertex}=[circle,fill=black!40,minimum size=30pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red! 60,minimum size=25pt,inner sep=0pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=30pt,inner sep=0pt,draw=black,thick]
%nodes
\node[vertex] (state1) at (0,1.8) {$S_t$};
\node[vertex] (state2) at (7,1.8) {$S_{t+1}$};
\node[overtex] (obs) at (13,0) {$O_{t+1}$};
\node[avertex] (action) at (2.5,0) {$a_t$};
%%bels
\node (bel1) at (1,2.5) {$b_t$};
\node (bel2) at (8.5,2.5) {$b_{t+1}$};
%probas
\node (trans) at (4.5,1.5) {$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t }$};
\node (observ) at (9.6,0.3) {$\textbf{p} \paren{ o_{t+1} \sachant s_{t+1}, a_t }$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS
\draw[->,>=latex, thick] (state1) -- (state2);
\draw[->,>=latex, thick] (state2) -- (obs);
\draw[->,>=latex, thick] (action) -- (state2);
\draw[->,>=latex, thick] (action) -- (obs);
\end{tikzpicture}
\caption[Bayesian Network illustrating the belief update.]{Bayesian Network illustrating the belief update: the states are the gray circular nodes, 
the action is the red square node, and the observation is the blue circular node.
The random variable $S_t$ representing the next state $s_{t+1}$ 
depends on the current one $s_t$ and the current action $a_t$.
The random variable $O_{t+1}$ representing the next observation $o_{t+1}$ 
depends on the next state $s_{t+1}$ 
and the current action $a_t$ too.
The belief state $b_{t}$ (resp. $b_{t+1}$)
is the probabilistic estimation of the current (resp. next) system state $s_t$ (resp. $s_{t+1}$).}
\label{BayesNetPOMDP}
\end{figure}%

As successive beliefs are computed 
with the observations perceived by the agent,
they are considered as visible for the agent. 
%Moreover, it can be easily shown that
%the expected total reward can be rewritten 
%\begin{equation}
%\label{probCriterion}
%\mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,d_t) } = \mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(b_t,d_t) }, 
%\end{equation}
%defining $r(b_t,a) = \sum_s r(s,a) \cdot b_t(s)$ as the reward of belief $b_t$.
Let us denote by $\mathbb{P}^{\mathcal{S}}$ 
the infinite set of probability distributions 
over $\mathcal{S}$.
An optimal strategy can be looked for 
as a function $d$ defined on $\mathbb{P}^{\mathcal{S}}$ 
such that successive $d_t = d(b_t) \in \mathcal{A}$ 
maximize the expected reward \ref{criterion}:
the agent decisions are then based on the belief state.

%
% POMDP ROBOTICS
%
The POMDP framework is a flexible model for autonomous robotics,
as illustrated by the firefighter example, see Figure \ref{robot_pompier}:
it allows to describe all the robotic and surrounding system,
as well as the robot mission,
and it is commonly used in robotics 
\cite{conf/isrr/PineauG05,OngShaoHsuWee-IJRR10,DBLP:conf/rss/Marthi12,DBLP:conf/ecai/ChanelTL12,DBLP:conf/aaai/ChanelTL13}.
It takes into account that the robot receives data
from its sensors only,
and thus has to figure out the actual system state 
using these data, called observations,
in order to fulfill the mission.
However the POMDP model raises some issues,
in particular in the robotic context.

%
% HIGH COMPLEXITY
%
\section*{Practical issues of the POMDP framework}
\subsection*{Complexity}
Solving a POMDP \textit{i.e.} computing an optimal strategy, 
is PSPACE-hard in finite horizon \cite{Papadimitriou:1987} 
and even
undecidable in infinite horizon \cite{Madani:1999:UPP:315149.315395}.
Moreover a space exponential in the problem description may be required 
for an explicit specification of such a strategy
(see \cite{Mundhenk:2000:CPP:867838} 
for a more detailed complexity analysis of POMDPs).

This high complexity is well-known by POMDP users:
optimality can be reached 
for tiny problems, 
or highly structured ones.
Classical approaches try to solve this problem
using Dynamic Programming \cite{Cassandra97incrementalpruning}.
Otherwise, only approximate solutions can be computed,
and thus the strategy has no optimality guaranty.
For instance, popular approaches such as point-based methods 
\cite{Pineau_2003_4826,Kurniawati-RSS08,Smith:2004:HSV:1036843.1036906}, 
grid-based ones \cite{Geffner98solvinglarge,Brafman97aheuristic,Bonet_newgrid-based}
or Monte Carlo approach \cite{NIPS2010_4031},
use approximate computations.
The next POMDP's practical issues that will be highlighted, 
concern modeling flaws of this model, 
illustrated by robotic situations.

%
% VISION IN ROBOTICS
%
\begin{figure} \centering
\includegraphics[scale=0.75]{fig2}
\caption[Example of an observation method in a robotic context]{
Example of an observation method in a robotic context:
the robot, here a drone, is equipped with a camera
and uses a classifier computed from a picture dataset
(as NORB, see Figure \ref{NORB}): 
such a classifier is described by Figure \ref{CV_algoConvNet}.
The classifier is computed before the mission (off-line)
with a picture dataset (see the right part of the illustration),
and the classifier output is used during the mission (online) 
as an observation, for the agent (see the left part).
Here, observations are thus generated by computer vision.}
\label{observation_robot}
\end{figure}
%
% VISION IN ROBOTICS END
%


%
% COMPUTER VISION TODO TODO TODO
%
\subsection*{Parameter imprecision and computer vision}
Consider now robots using visual perception, 
and whose observations 
come from computer vision algorithms based on statistical learning
(see Figure \ref{observation_robot}).
In this situation, the robot uses a \textit{classifier}
to recognize objects in pictures: 
the classifier is supposed to return the name of 
the object actually in the picture, 
and to make some mistakes 
with a low probability 
(see confusion matrix of Figure \ref{confusion_matrix}).

The classifier is computed using a \textit{training dataset of pictures}
(as NORB, see Figure \ref{NORB}, authors made it available at \url{http://www.cs.nyu.edu/~ylclab/data/norb-v1.0/}).
A powerful gradient-based learning algorithm meant to compute classifiers using picture datasets
and called Convolutional Nextwork is described in Figure \ref{CV_algoConvNet}.
The figures (\ref{NORB}), (\ref{CV_algoConvNet}) and (\ref{confusion_matrix})
illustrate the example of a classifier computed for a drone mission where features of interests,
or system states of the problem, are related with the presence (or absence) of animals, cars,  humans, planes or trucks:
the statistical problem of computing a classifier recognizing such objects in pictures 
is called \textit{multiclass classification}.

%%%
%%% NORB DATA SET
%%%
\newcounter{moncompteur} %define counter
\begin{figure} \centering
\begin{tikzpicture}
\node (notes) at (1cm,7cm) { 
NORB dataset: \color{blue} $(\mbox{picture}_i,\mbox{label}_i)_{i=1}^N$
};

\def\names{{"bla","human","car","truck","truck","nothing","nothing","nothing","truck"}}%
\def\namess{{"bla","airplane","car","human","animal","car","human","animal","animal"}}%

\setcounter{moncompteur}{1}
	\coordinate (norb11) at (6.2cm,3.05cm); 
	\coordinate (norb12) at (14cm,3.05cm); 
	\coordinate (norb11lab) at (6.2cm,6.2cm); 
	\coordinate (norb12lab) at (14cm,6.2cm); 
	\coordinate (norb11lab2) at (6.2cm,1.9cm); 
	\coordinate (norb12lab2) at (14cm,1.9cm); 
	\foreach \y in {0.25,0.5,...,2} {
		\coordinate (weight) at (barycentric cs:norb11= \y,norb12=1-\y);
		\def\reptemp{photos/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics[scale=0.5]{\reptemp}};
		
		\coordinate (weight2) at (barycentric cs:norb11lab= \y,norb12lab=1-\y);
		\node[scale=0.7] (leslabels) at (weight2) {\pgfmathparse{\names[\themoncompteur]}\pgfmathresult};
		\coordinate (weight3) at (barycentric cs:norb11lab2= \y,norb12lab2=1-\y);
		\node[scale=0.7] (leslabels) at (weight3) {\pgfmathparse{\namess[\themoncompteur]}\pgfmathresult};

		\addtocounter{moncompteur}{1}
	}

	\coordinate (norb21) at (6.2cm,5cm); 
	\coordinate (norb22) at (14cm,5cm); 
	\foreach \y in {0.25,0.5,...,2} {
		\coordinate (weight) at (barycentric cs:norb21= \y,norb22=1-\y);
		\def\reptemp{photos/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics[scale=0.5]{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

\end{tikzpicture}
\caption[Example of picture dataset for computer vision.]{Example of picture dataset for computer vision: 
the labeled picture dataset NORB \cite{LeCun.2004}. The size of NORB is higher than $3.10^5$,
and pictures from this dataset represents objects among the $5$ classes: 
``animal'', ``car'', ``human'', ``nothing'', ``plane'' and ``truck''.
Each element of a labeled picture dataset is composed of a picture (e.g. a picture showing a car)
and a label corresponding to the class of the object represented by the picture 
(in the previous example, the label is ``car'').
This dataset can be used for supervised learning to compute a classifier (see Figure \ref{CV_algoConvNet}).
In order to be able to discern locations of targets, 
a picture is labeled with the name of centered object 
(``nothing'' if there is nothing in the picture center).}
\label{NORB}
\end{figure}
%%%%%
%%%%% NORB DATASET END
%%%%%

%
% IMPRECISION OBSERVATION
%
As the classifier is learned based on a picture dataset 
(see weights learned in Figure \ref{CV_algoConvNet}),
its behavior, and thus its performances (\textit{i.e.} how well it predicts objects in images) 
are inevitably dependent on the dataset.
It is a problem if the picture variability in the dataset
is too low:
in this case, the probabilistic behavior of the classifier 
will be dependent on these particular pictures, 
and the robotic system will have poor observation
capabilities when the considered mission involves 
pictures too different from the ones from the dataset. 

Some large picture datasets with a high variability exists 
(e.g. NORB, Figure \ref{NORB}, although variability could be ideally higher):
note however that with such datasets, the vision performances are reduced,
or good performances are, at least, harder to reach.

A confusion matrix can be computed (see Figure \ref{confusion_matrix})
using such a labeled picture dataset,
not used for the training, and called testing dataset: 
observation frequencies can be deduced from this matrix,
normalizing rows into probabilities.
A row corresponds to an object in the scene,
and probabilities on this row are observation probabilities,
\textit{i.e.} each probability is the frequency with which 
the classifier returns the name of the object of the corresponding column.
These probabilities can be used to define the observation function $\textbf{p} \paren{o' \sachant s',a}$
introduced above. 
This approach raises the issue of knowing if the testing dataset 
is quite representative of the mission reality.
If not, the observation probabilities may be not reliable,
and the POMDP badly defined: 
however, as shown by the equation (\ref{probBayesRule})
the belief update needs a perfect knowledge of the
observation 
probability distributions.

More generally, observations of the robot agent 
are outputs of image processing algorithms 
whose semantics (image correlation, object matching, class inference, 
preprocessing followed by classifiers such as the one computed 
in Figure \ref{CV_algoConvNet} etc.) 
are so complex that probabilities of occurrence are hard to rigorously extract.

Finally, if the considered datasets are labeled more precisely 
(as NORB, which includes information such as the lighting condition or the object scale),
we can imagine that the computed observation probabilities (from the confusion matrix) 
were more reliable, or the vision performances upgraded (since separation when learning is easier).
However, more observation or states are involved,
and the POMDP is harder to solve.
Moreover, as the number of pictures per class is reduced 
(since there are more complex and numerous classes), 
a poorer confusion matrix is obtained when testing (in terms of confidence).

As a conclusion, the POMDP model supposes 
the knowledge of all the involved probabilities:
unfortunately these frequencies are imprecisely known in practice.
The imprecision about these probabilities, 
for instance those related to the actual vision algorithm behavior
with real world pictures,
has to be taken into account 
to make the robot autonomous 
under any circumstances.
In general, all the probabilities of this model have to be computed
needing enough tests for each possible system state and action,
which seems hard to perform in practice.


%TODO TODO TODO
%%
%% ConvNets
%%
\begin{figure} \centering
\begin{tikzpicture} 
\label{bipyr}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TIKZ
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\node (car) at (-1,-1) {\includegraphics[scale=0.52]{NORB_car.png}};

	% input image 
	\coordinate (A1) at (0cm,0cm); 
	\coordinate (A2) at (0cm,-2cm); 
	\coordinate (A3) at (-2cm,0cm);
	\coordinate (A4) at (-2cm,-2cm); 
	\draw[thick] (A1) -- (A3);
	\draw[thick] (A3) -- (A4);
	\draw[thick] (A4) -- (A2);
	\draw[thick] (A2) -- (A4);

	\node at (barycentric cs:A1=1,A3=1,A2=-0.15) {\tiny $m$};
	\node at (barycentric cs:A3=1,A4=1,A1=-0.2) {\tiny $m$};

	% output feature
	\coordinate (A6) at (7cm,-4cm);
	\coordinate (A7) at (9cm,-0.7cm);
	\coordinate (A8) at (9.1cm,-0.7cm);
	\coordinate (A9) at (7.1cm,-4cm);
	\draw[thick] (A6) -- (A9);
	\draw[thick] (A7) -- (A8);
	\draw[thick] (A8) -- (A9);

	\node at (barycentric cs:A7=1,A8=1,A6=-0.1) {\tiny $1$};
	\node at (barycentric cs:A8=1,A9=1,A1=-0.05) {\tiny $n$};

	%% draw faces
	%\fill[gray!50,opacity=0.7] (A1) -- (A6) -- (A7) -- cycle;
	%\fill[gray!50,opacity=0] (A1) -- (A2) -- (A7) -- cycle; 
	%\fill[gray!50,opacity=0.7] (A2) -- (A1) -- (A6) -- cycle;
	\draw[thick,dashed] (A1) -- (A7);
	\draw[thick,dashed] (A2) -- (A7);

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% INTERMEDIATE FEATURE MAPS DRAWING %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE1

	% base points (of right and left images)
	\coordinate (im111) at (2.77cm,-0.08cm); 
	\coordinate (im112) at (2.77cm,-1.85cm); 
	\coordinate (im113) at (1cm,-0.08cm);
	\coordinate (im114) at (1cm,-1.85cm); 

	\coordinate (im131) at (2.45cm,-0.4cm); 
	\coordinate (im132) at (2.45cm,-2.17cm); 
	\coordinate (im133) at (0.68cm,-0.4cm);
	\coordinate (im134) at (0.68cm,-2.17cm); 


	% images plans
	% beginning
	\draw[thick] (im111) -- (im113);
	\draw[thick] (im113) -- (im114);
	\draw[thick] (im114) -- (im112);
	\draw[thick] (im112) -- (im111);
	\fill[gray!00,opacity=0.7] (im111) -- (im112) -- (im114) -- (im113) -- cycle;


	% intermediate plans
	\coordinate (im121) at (2.62cm,-0.24cm); 
	\coordinate (im122) at (2.62cm,-2.01cm); 
	\coordinate (im123) at (0.85cm,-0.24cm);
	\coordinate (im124) at (0.85cm,-2.01cm); 
	\draw[thick] (im121) -- (im123);
	\draw[thick] (im123) -- (im124);
	\draw[thick] (im124) -- (im122);
	\draw[thick] (im122) -- (im121);
	\fill[gray!00,opacity=0.7] (im121) -- (im122) -- (im124) -- (im123) -- cycle;

	%% end
	\draw[thick] (im131) -- (im133);
	\draw[thick] (im133) -- (im134);
	\draw[thick] (im134) -- (im132);
	\draw[thick] (im132) -- (im131);
	\fill[gray!00,opacity=0.7] (im131) -- (im132) -- (im134) -- (im133) -- cycle;


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE2
	
	% base points (of right and left images)
	\coordinate (im211) at (4.5cm,-2cm); 
	\coordinate (im212) at (4.5cm,-3cm); 
	\coordinate (im213) at (3.5cm,-2cm);
	\coordinate (im214) at (3.5cm,-3cm); 

	\coordinate (im291) at (5.5cm,-0.35cm); 
	\coordinate (im292) at (5.5cm,-1.35cm); 
	\coordinate (im293) at (4.5cm,-0.35cm);
	\coordinate (im294) at (4.5cm,-1.35cm); 

	% images plans
	% beginning
	\draw[thick] (im291) -- (im293);
	\draw[thick] (im293) -- (im294);
	\draw[thick] (im294) -- (im292);
	\draw[thick] (im292) -- (im291);
	\fill[gray!00,opacity=0.7] (im291) -- (im292) -- (im294) -- (im293) -- cycle;

	% intermediate plans
	\foreach \y in {0.1,0.2,...,1} {
		\coordinate (ii1) at (barycentric cs:im211= \y,im291=1-\y);
		\coordinate (ii2) at (barycentric cs:im212= \y,im292=1-\y);
		\coordinate (ii3) at (barycentric cs:im213= \y,im293=1-\y);
		\coordinate (ii4) at (barycentric cs:im214= \y,im294=1-\y);

		\draw[thick] (ii1) -- (ii3);
		\draw[thick] (ii3) -- (ii4);
		\draw[thick] (ii4) -- (ii2);
		\draw[thick] (ii2) -- (ii1);
		\fill[gray!00,opacity=0.7] (ii1) -- (ii2) -- (ii4) -- (ii3) -- cycle;
	}

	% end
	\draw[thick] (im211) -- (im213);
	\draw[thick] (im213) -- (im214);
	\draw[thick] (im214) -- (im212);
	\draw[thick] (im212) -- (im211);
	\fill[gray!00,opacity=0.7] (im211) -- (im212) -- (im214) -- (im213) -- cycle;

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE3

	% base points (of right and left images)
	\coordinate (im311) at (5.75cm,-3cm); 
	\coordinate (im312) at (5.75cm,-3.5cm); 
	\coordinate (im313) at (5.25cm,-3cm);
	\coordinate (im314) at (5.25cm,-3.5cm); 

	\coordinate (im391) at (7.2cm,-0.53cm); 
	\coordinate (im392) at (7.2cm,-1.03cm); 
	\coordinate (im393) at (6.7cm,-0.53cm);
	\coordinate (im394) at (6.7cm,-1.03cm); 

	% images plans
	% beginning
	\draw[thick] (im391) -- (im393);
	\draw[thick] (im393) -- (im394);
	\draw[thick] (im394) -- (im392);
	\draw[thick] (im392) -- (im391);
	\fill[gray!00,opacity=0.7] (im391) -- (im392) -- (im394) -- (im393) -- cycle;

	% intermediate plans
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (ii1) at (barycentric cs:im311= \y,im391=1-\y);
		\coordinate (ii2) at (barycentric cs:im312= \y,im392=1-\y);
		\coordinate (ii3) at (barycentric cs:im313= \y,im393=1-\y);
		\coordinate (ii4) at (barycentric cs:im314= \y,im394=1-\y);

		\draw[thick] (ii1) -- (ii3);
		\draw[thick] (ii3) -- (ii4);
		\draw[thick] (ii4) -- (ii2);
		\draw[thick] (ii2) -- (ii1);
		\fill[gray!00,opacity=0.7] (ii1) -- (ii2) -- (ii4) -- (ii3) -- cycle;
	}

	% end
	\draw[thick] (im311) -- (im313);
	\draw[thick] (im313) -- (im314);
	\draw[thick] (im314) -- (im312);
	\draw[thick] (im312) -- (im311);
	\fill[gray!00,opacity=0.7] (im311) -- (im312) -- (im314) -- (im313) -- cycle;

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE4
	
	% base points (of right and left images)
	\coordinate (im411) at (6.35cm,-3.5cm); 
	\coordinate (im412) at (6.35cm,-3.75cm); 
	\coordinate (im413) at (6.1cm,-3.5cm);
	\coordinate (im414) at (6.1cm,-3.75cm); 

	\coordinate (im491) at (8.12cm,-0.61cm); 
	\coordinate (im492) at (8.12cm,-0.86cm); 
	\coordinate (im493) at (7.87cm,-0.61cm);
	\coordinate (im494) at (7.87cm,-0.86cm); 

	% images plans
	% beginning
	\draw[thick] (im491) -- (im493);
	\draw[thick] (im493) -- (im494);
	\draw[thick] (im494) -- (im492);
	\draw[thick] (im492) -- (im491);
	\fill[gray!00,opacity=0.8] (im491) -- (im492) -- (im494) -- (im493) -- cycle;

	% intermediate plans
	\foreach \y in {0.04,0.08,...,1} {
		\coordinate (ii1) at (barycentric cs:im411= \y,im491=1-\y);
		\coordinate (ii2) at (barycentric cs:im412= \y,im492=1-\y);
		\coordinate (ii3) at (barycentric cs:im413= \y,im493=1-\y);
		\coordinate (ii4) at (barycentric cs:im414= \y,im494=1-\y);

		\draw[thick] (ii1) -- (ii3);
		\draw[thick] (ii3) -- (ii4);
		\draw[thick] (ii4) -- (ii2);
		\draw[thick] (ii2) -- (ii1);
		\fill[gray!00,opacity=0.7] (ii1) -- (ii2) -- (ii4) -- (ii3) -- cycle;
	}

	% end
	\draw[thick] (im411) -- (im413);
	\draw[thick] (im413) -- (im414);
	\draw[thick] (im414) -- (im412);
	\draw[thick] (im412) -- (im411);
	\fill[gray!00,opacity=0.8] (im411) -- (im412) -- (im414) -- (im413) -- cycle;


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LINE5

	% base points (of right and left images)
	\coordinate (im511) at (6.75cm,-3.8cm); 
	\coordinate (im512) at (6.75cm,-3.9cm); 
	\coordinate (im513) at (6.65cm,-3.8cm);
	\coordinate (im514) at (6.65cm,-3.9cm); 

	\coordinate (im591) at (8.65cm,-0.67cm); 
	\coordinate (im592) at (8.65cm,-0.77cm); 
	\coordinate (im593) at (8.55cm,-0.67cm);
	\coordinate (im594) at (8.55cm,-0.77cm); 

	% images plans
	% beginning
	\draw[thick] (im591) -- (im593);
	\draw[thick] (im593) -- (im594);
	\draw[thick] (im594) -- (im592);
	\draw[thick] (im592) -- (im591);
	\fill[gray!00,opacity=0.8] (im591) -- (im592) -- (im594) -- (im593) -- cycle;

	% intermediate plans
	\foreach \y in {0.025,0.05,...,1} {
		\coordinate (ii1) at (barycentric cs:im511= \y,im591=1-\y);
		\coordinate (ii2) at (barycentric cs:im512= \y,im592=1-\y);
		\coordinate (ii3) at (barycentric cs:im513= \y,im593=1-\y);
		\coordinate (ii4) at (barycentric cs:im514= \y,im594=1-\y);

		\draw[thick] (ii1) -- (ii3);
		\draw[thick] (ii3) -- (ii4);
		\draw[thick] (ii4) -- (ii2);
		\draw[thick] (ii2) -- (ii1);
		\fill[gray!00,opacity=0.7] (ii1) -- (ii2) -- (ii4) -- (ii3) -- cycle;
	}

	% end
	\draw[thick] (im511) -- (im513);
	\draw[thick] (im513) -- (im514);
	\draw[thick] (im514) -- (im512);
	\draw[thick] (im512) -- (im511);
	\fill[gray!00,opacity=0.8] (im511) -- (im512) -- (im514) -- (im513) -- cycle;


	%%%%%%%%%%%%%%%%%%%%%%	
	%% END FEATURE MAPS %%
	%%%%%%%%%%%%%%%%%%%%%%

	\draw[thick,dashed] (A2) -- (A6);
	\draw[thick] (A7) -- (A6);
	\draw[thick,dashed] (A1) -- (A6);
	\draw[thick] (A1) -- (A2);
	
	% draw points
	\foreach \i in {1,2,3,4,6,7,8,9}
	{
	  \draw[fill=black] (A\i) circle (0.01em);
	}

	% draw arrows
	\coordinate (F1) at (-0.5,-2.2cm);
	\coordinate (F2) at (0.7,-2.5cm);
	\coordinate (F3) at (1.7,-2.8cm);
	\coordinate (F4) at (3.2,-3.1cm);
	\coordinate (F5) at (4,-3.4cm);
	\coordinate (F6) at (5,-3.7cm);
	\node (Fdots) at (5.6,-3.9cm) [rotate=345] {$\ldots$};
	\coordinate (F7) at (6.6,-4.05cm);
	\coordinate (F8) at (7,-4.1cm);
	\coordinate (F9) at (7.3,-4.1cm);
	\coordinate (F10) at (8.7,-3.2cm);

	\draw[->,>=latex,thick, color=blue!60] (F1) to[bend right] (F2);
	\draw[->,>=latex,thick, color=blue!60] (F3) to[bend right] (F4);
	\draw[->,>=latex,thick, color=blue!60] (F5) to[bend right] (F6);
	\draw[->,>=latex,thick, color=blue!60] (F7) to[bend right] (F8);
	\draw[->,>=latex,thick, color=blue!60] (F9) to[bend right] (F10);

	\coordinate (A10) at (8.7cm,-3cm);
	\coordinate (A11) at (9.55cm,-1.7cm);
	\coordinate (A12) at (9.65cm,-1.7cm);
	\coordinate (A13) at (8.8cm,-3cm);
	\draw[thick] (A10) -- (A11);
	\draw[thick] (A11) -- (A12);
	\draw[thick] (A12) -- (A13);
	\draw[thick] (A13) -- (A10);
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%****************************************************************%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\node at (-0.3,0.8) {Given $i \in \set{1,\ldots,N}$,};
	\node at (barycentric cs:A2=1,A4=1,A1=-0.4) {\color{blue} $\mbox{picture}_i$};


	\draw[thick,dashed] (A9) -- (A10);
	\draw[thick,dashed] (A8) -- (A11);
	\node (etiq2) at (barycentric cs:A12=1,A13=1,A1=-0.2) {$\mbox{vector}_{\color{black}i}^{\color{red}W}$};

	\node (w0) at (0.3cm, -3cm) {{\color{red} $w_0$ }};
	\node (w1) at (2.2cm, -3.5cm) {{\color{red} $w_1$ }};
	\node (w2) at (4cm, -3.9cm) {{\color{red} $w_2$ }};
	\node (wm) at (9cm, -4cm) {{\color{red} $w_m$ }};

	\node  at (12cm,0cm) {\color{blue} $\mbox{label}_i$};
	\node (etiq) at (12cm,-0.7cm) {e.g. ``car''};


\tikzstyle{grisEncadre}=[thick, fill=gray!20];
\draw [grisEncadre] (6.8,-4.65) rectangle (13.2,-5.9);

	\node (perte) at (11.3cm, -4.7cm) {};	
	\node (perte1) at (10cm, -4.9cm) { \textit{gradient-based} \textit{ minimization} of};
	\node (perte2) at (10cm, -5.5cm) {  $loss\Big( (\mbox{vector}_{\color{black}i}^{\color{red}W})_{i=1}^N, ({\color{blue} \mbox{label}_i})_{i=1}^N \Big)$ in {\color{red} W}};
	

	\draw[->,>=latex,thick, color=blue!60,line width=1mm] (etiq) to (perte);
	\draw[->,>=latex,thick, color=blue!60,line width=1mm] (etiq2) to (perte);
	

	\node (Wpictures) at (-0.1cm,-4cm) {{\color{red} $W$:}};

	\node (Wpictures2) at (2.55cm,-4.5cm) {};
	\node (classifieur) at (1.1cm,-4.5cm) {};

	\node (perte3) at (6.92cm, -5.2cm) {};

	\node (lesweights) at (5cm, -5.4cm) { { \color{red} $\left . \begin{array}{ccccc}
	\\
	\\
	\\
	\\
	\\
	\end{array} \right \}$}};
	\node (w) at (5.4cm, -5.2cm) {};
	\draw[->,>=latex,thick, color=red!60, line width=1mm] (perte3) to (w);

% to rename files in a directory in a correct way:
% compteur=0
% for file in *.png; do compteur=$((compteur+1)); echo $compteur; mv $file $compteur.png; done
%%%% WARNING!!! IT REMOVE SOME FILES!


	\setcounter{moncompteur}{171}

	\coordinate (ligne01) at (0cm,-4.4cm); 
	\coordinate (ligne02) at (5cm,-4.4cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne01= \y,ligne02=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}	


	\coordinate (ligne11) at (0cm,-4.65cm); 
	\coordinate (ligne12) at (5cm,-4.65cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne11= \y,ligne12=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	
	\coordinate (ligne21) at (0cm,-4.9cm); 
	\coordinate (ligne22) at (5cm,-4.9cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne21= \y,ligne22=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne31) at (0cm,-5.15cm); 
	\coordinate (ligne32) at (5cm,-5.15cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne31= \y,ligne32=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne41) at (0cm,-5.4cm); 
	\coordinate (ligne42) at (5cm,-5.4cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne41= \y,ligne42=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne51) at (0cm,-5.65cm); 
	\coordinate (ligne52) at (5cm,-5.65cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne51= \y,ligne52=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne61) at (0cm,-5.9cm); 
	\coordinate (ligne62) at (5cm,-5.9cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne61= \y,ligne62=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne71) at (0cm,-6.15cm); 
	\coordinate (ligne72) at (5cm,-6.15cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne71= \y,ligne72=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

	\coordinate (ligne81) at (0cm,-6.4cm); 
	\coordinate (ligne82) at (5cm,-6.4cm); 
	\foreach \y in {0.05,0.1,...,1} {
		\coordinate (weight) at (barycentric cs:ligne81= \y,ligne82=1-\y);
		\def\reptemp{weights/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics{\reptemp}};
		\addtocounter{moncompteur}{1}
	}
\end{tikzpicture}
\caption[Example of classifier training for computer vision]{Example of classifier training for computer vision:
the labeled picture dataset NORB, see Figure \ref{NORB},
is used to train and test a classifier.
The learning algorithm is based on Convolutional Network \cite{Lecun98gradient-basedlearning,DBLP:journals/jfr/SermanetHSGBECML09}
using gradient methods \cite{bottou-91c,lecun-98x,bottou-tricks-2012}.
The weights $W=(w_0,\ldots,w_m)$ are the parameters of a particular transformation (see the ``bi-pyramid'' of successive transformation stages) 
from the picture to a vector representing a label among $\set{animal,car,human,nothing,plane,truck}$. 
These weights are learned in order to minimize a given loss function
\textit{i.e.} a proper criterion representing the error of the classifier over the dataset.
A classical loss function is the Mean Squared Error (MSE). 
The environment Torch7 (based on lua and C languages, \cite{Collobert_NIPSWORKSHOP_2011})
has been used to compute the displayed weights.
 }
\label{CV_algoConvNet}
\end{figure}
%%
%% CONVNETS END
%%


%
% IMPRECISION POMDP WORKS
%
Imprecision Models taking into acount imprecision
Les probl\`emes de prise en compte de la
m\'econnaissance sur les param\`etres peuvent \^etre contourn\'es en utilisant
les \'evolutions de ce mod\`ele: BPOMDP et POMDPIP. Toutefois, il est
tr\`es difficile, en termes de calculs, d'extraire une strat\'egie de ces mod\`eles. 

Les POMDPs \`a param\`etres born\'es (BPOMDP) \cite{NiYaLiaZhi} et les POMDPs avec probabilit\'e impr\'ecises (POMDPIP) \cite{Itoh2007453} 
ont \'et\'e mis en place afin de mod\'eliser la m\'econnaissance sur les distributions de probabilit\'e: l'incertitude issue de l'algorithme
de traitement d'image peut donc \^etre encod\'ee dans le cadre de ces mod\`eles. Cependant, ils 
sont plus complexes que les POMDPs, eux-m\^eme \'etant d\'ej\`a au moins PSPACE-hard


BPPOMDP \cite{NiYaLiaZhi} Bounded Parameters POMDPs + NOTES\\ 
POMDPIP POMDP with Imprecise Probabilities \cite{Itoh2007453} + NOTES


POMDPIP: 
manage imprecision of the model, POMDPIP - BPPOMDP .. NOTES, autant travailler sur un already discretized model \\  
robust \cite{DBLP:conf/icml/Osogami15} it answers to the robust question (approx), but at least the POMDP complexity,
optim require linear program, and lack of knowlegde is represented by (convex) credal sets, PO version of the robust model for 
non stationnary uncertain MDP models, or of the robust approximation for uncertain MDPs \cite{NE:05} 
(lowerbound of the maximin criterion)\\ 

While there are fascinating questions of algorithms
no learning because we are interested in robotic applications which have to quite well behave in first tests,
(but can learn in each use e.g. long term learning)
for instance in order to begin Reinforcement Learning (RL, \cite{NIPS2009_3780,Ross:2011:BAL:1953048.2021055}) 
problem of solving the problem offline:


\begin{figure} \centering
\begin{tabular}{c|c|c|c|c|c!{\vrule width 2pt}c!{\vrule width 2pt}c}%!{\vrule width 2pt}}
animal & human & plane & truck & car & nothing \\ \specialrule{.2em}{.0em}{.0em}  
$3688$ & $575$ & $256$ & $48$ & $144$ & $149$ &   animal & $75.885\%$ \\ \specialrule{.05em}{.0em}{.0em}  
$97$ & $4180$ & $81$ & $20$ & $225$ & $257$ & human & $86.008\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$292$ & $136$ & $3906$ & $237$ & $202$ & $87$ & plane & $80.370\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$95$ & $1$ & $44$ & $4073$ & $514$ & $133$ & truck & $83.807\%$  \\  \specialrule{.05em}{.0em}{.0em}  
$129$ & $3$ & $130$ & $1283$ & $3283$ & $32$ &  car & $67.551\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$154$ & $283$ & $36$ & $63$ & $61$ & $4263$ & nothing & $87.716\%$  \\ %\specialrule{.2em}{.0em}{.0em}  
\end{tabular}
\caption[Example of confusion Matrix for multiclass classification]{
Example of confusion matrix for multiclass classification:
this matrix is computed with a testing dataset of pictures, different from the training dataset.
Each row only considers the pictures of a given object,
and numbers represents the answers of the classifier: 
for instance, $3688$ pictures of animals are well recognized, 
but $575$ are confused with a human.
Average row correct is here $80.223\%$.
Torch7 environment \cite{Collobert_NIPSWORKSHOP_2011} has also been used 
to compute this matrix from the classifier and the testing dataset.}
\label{confusion_matrix}
\end{figure}


%%
%%  FULL IGNORANCE/ KNOWLEDGE OF THE AGENT
%%
\subsection*{Agent ignorance modeling}
The initial belief, or \textit{a priori}
probability distribution over the system states, takes part in
the definition of the POMDP problem. However in practice, the initial system
state can be unknown with absolutely no probabilistic information: 
for instance, in a robotic exploration context, 
the initial location of the agent, or the presence of an entity in the scene. 
Defining the process with a uniform probability distribution as initial belief
(\textit{e.g.} over all locations or over entity presence) is a subjectivist
answer \cite{de1974theory}, \textit{i.e.} all probabilities are the same
because no event is more plausible than another:
it corresponds to equal betting rates. However following belief
updates will eventually mix frequentist probability distributions 
defining the POMDP problem with this initial belief which is a subjective probability, 
and it does not always make sense. 

La connaissance de $b_0$ pose aussi un problème: 
il existe des missions où le robot ne connait initialement rien de l'état du système, 
ce que ne modélise pas tout à fait une probabilité uniforme. 
Lorsqu'aucune expérience passée n'est disponible, 
il n'est pas évident d'attribuer une fréquence à un évènement initial...

and knowledge of the agent CARO \\
Moreover, consider missions 
where a part of the system state,
describing something that the robot
is supposed to infer by itself, 
is initially fully unknown:
for instance the location 
or the nature of a target. 
Classical approaches initialize 
the belief state as 
a subjective uniform probability distribution. 
In this case the belief update 
mixes subjective probabilities and frequencies,
which is questionable.
\begin{itemize}
\item \textbf{Initial belief} $b_0$ (\textit{prior} information on the system state). 
\end{itemize}
uniform $=$ subjectif, mix up with frequencies!\\

The use of the POMDPs 
raises however some practical issues,
such as the difficulty to encode 
robot ignorance, 
Caroline \cite{conf/stairs/ChanelFTI10,oatao11449} \\

De plus, d\`es le d\'ebut du processus, l'agent n'observe que 
partiellement l'\'etat du syst\`eme: une croyance initiale est n\'ecessaire 
pour d\'efinir enti\`erement le POMDP. Dans le cas d'une mission de rep\'erage de cible, 
il faut en effet attribuer une distribution de 
probabilit\'e \`a la position initiale de cette derni\`ere. Si cette position est totalement m\'econnue, 
une distribution de 
probabilit\'e uniforme sur toutes
les positions possibles peut-\^etre une solution, ce qui en fait une distribution de probabilit\'e
subjective (elle est choisie uniforme par prudence compte tenu de la m\'econnaissance). 
Cependant, les autres probabilit\'es d\'efinissant le mod\`ele \'etant des fr\'equences, cette
approche m\`ene \`a un m\'elange incongru de probabilit\'es subjectives et de fr\'equences. 
Les $\pi$-POMDPs permettent une mod\'elisation formelle de 
la m\'econnaissance de l'agent, bien que ce mod\`ele n'ait pas \'et\'e \'etudi\'e de 
mani\`ere approfondie. \\


Consider situations where the agent totally ignores the system's initial 
state, for instance a robot that is for the first time in a room
with an unknown exit location (initial belief) and has to find the exit and 
reach it. In practice, no experience can be repeated in order to extract a frequency of 
the exit's location. In this kind of situation,
uncertainty is not due to a random fact, but to a lack of knowledge: no frequentist 
initial belief can be used to define the model. A uniform probability
 distribution is often chosen in order to assign
the same mass to each state. This choice can be justified based on
 the subjective probability theory \cite{Dubois96representingpartial} 
(the probability distribution represents then an exchangeable bet) but subjective probabilities and observation frequencies are combined 
during the belief update.

In other cases, the agent may strongly believe that
the exit is located in a wall as in the vast majority of rooms, but it still
grants a very small probability $p_{\epsilon}$ to the fact that the exit may be a
staircase in the middle of the room. Even if this is very unlikely to be the case,
this second option must be taken into account in the belief, otherwise Bayes'
rule cannot correctly update it if the exit is actually in the middle of
the room. Eliciting $p_{\epsilon}$ without past experience is not obvious at
all and does not rely on any rational reasons, yet it dramatically impacts the
agent's policy. On the contrary, possibilistic uncertainty models allow the agent to
elicit beliefs with imprecise unbiased knowledge.




%
% WHAT WE WOULD LIKE TO DO
%
\section*{General problem}
problem of solving the problem offline:
fulfill the mission at the first execution,
with a model built from imprecise data and
using reasonable computation time.
+ MODELIZATION and interested by the actual knowledge of the agent (CARO)
C'est ce qui justifie cette \'etude. Elle consiste \`a d\'evelopper un algorithme
de calcul de strat\'egie pour une mission robotique autonome, adapt\'e \`a l'incertitude
li\'ee \`a la vision artificielle, et permettant la mod\'elisation formelle de 
la m\'econnaissance initiale de l'agent. \\


%
% UNCERTAINTY THEORIES
%
The study of other uncertainty theories may bring useful properties
to deal with this problem.
DEMPSTER: no,belief plausibility evidence: a probability value for each $2^{\#\Omega}$
(to complex too)
 poss quant no, poss qual!!

plausibilité $Pl(A)=\sum_{\set{B \sachant B \cap A \neq \emptyset  } }m(B)$ et la croyance $Bel(A)=\sum_{\set{B \subset A}} m(B)$.
Dempster-Shafer Theory \cite{shafer1976mathematical} less complex than IP but more than proba alone\\ 

%
% POSS QUAL
%
\subsection*{A qualitative possibilistic model}
\textbf{Qualitative Possibility Theory:}
$\rightarrow$ simplification, ignorance and imprecision modeling.
The $\pi$-POMDP model is a possibilistic and qualitative counterpart of the
probabilistic POMDP model \cite{Sa1999.5}: it allows a formal modeling of
total ignorance using a possibility distribution equal to $1$ on all
the states. This distribution means that all states are equally possible
independently of how likely they are to happen (no necessary state). 

Finding qualitative estimates of their recognition performance is easier: the
$\pi$-POMDP model only require qualitative data, thus it allows to
construct the model without using more information than really available. 
Constat que les possibilitees qualitatives sont qualitatives, et peux modeliser la connaissance
The Qualitative Possibility Theory allows to handle imprecise data
and model the lack of knowledge (pas forcement le modele)
tropical algebra. (SIMPLIFY or ENCODE AVAILABLE DATA)

The use of the Qualitative Possibility Theory \cite{DBLP:journals/eor/DuboisPS01}
is studied here,
as it appears capable
to both simplify the POMDPs, 
and model imprecision and ignorance 
related to robotic missions.

In our context, distributions defined in the Possibility Theory framework  
are valued in a totally ordered scale $\mathcal{L}=\set{ 1=l_1,l_2,\ldots,0 }$ with
$l_1>l_2>\ldots>0$. A possibility measure $\Pi$ defined on $\mathcal{S}$ is a 
fuzzy measure valued in $\mathcal{L}$, such that $\forall A,B \subset \mathcal{S}$, 
$\Pi(A \cup B) = \max \set{ \Pi(A), \Pi(B) }$, $\Pi(\emptyset)=0$ and $\Pi(\mathcal{S})=1$.
It follows that this measure is entirely defined by the associated possibility distribution,
\textit{i.e.} the measure of the singletons: 
$\forall s \in \mathcal{S}$, $\pi(s) = \Pi(\set{s})$. Properties of this measure lead to
the possibilistic normalization: 
\begin{equation} 
\label{possNormDef}
\max_{s \in \mathcal{S}} \pi(s) = \Pi(\mathcal{S}) = 1.
\end{equation}
If $\overline{s},\underline{s} \in \mathcal{S}$ 
are such that $\pi(\overline{s})<\pi(\underline{s})$, it means
that $\overline{s}$ is less plausible than $\underline{s}$. States with possibility
degree $0$, \textit{i.e.} states $s \in \mathcal{S}$ such that $\pi(s)=0$, are impossible
(same meaning as $\textbf{p}(s)=0$), and those such that $\pi(s)=1$ are entirely possible
(but not necessary the most probable one).

note the similarities between Possibility and Probability Theory, 
replacing $\max$ by $+$ and $\min$ by $\times$. Moreover, Possibility Theory has its
own counterpart of the Bayes rule \cite{Dubois199023}: 



the indicator (chararacteristic) function of this set.
Unlike classical sets, values of a fuzzy set indicator function $\pi$
are not only in $\set{0,1}$. Recall that the indicator function of
a classical set $A \subseteq \mathcal{S}$ is $\mathds{1}_{A}(s) = 1$ 
if $s \in A$ and $0$ otherwise.
Values of a fuzzy set indicator function are chosen in a totally ordered scale 
$\mathcal{L} = \set{ 1=l_1, l_2, \ldots, 0}$ with $l_1>l_2>\ldots>0$: 
$\pi: \mathcal{S} \rightarrow \mathcal{L}$. If $s \in \mathcal{S}$ is such that
$\pi(s)=l_i$, $s$ is in the fuzzy set described by $\pi$, with degree $l_i$.  
Possibilistic beliefs used in this work will represent fuzzy sets of possible 
states. If the current possibilistic belief coincide with the distribution
$\pi(s)=1$ $\forall s \in \mathcal{S}$, all system states are totally possible,
and it models therefore a total ignorance about the current system state:
qualitative possibilistic beliefs can model agent initial ignorance.
The full knowledge of the current state, say $\tilde{s} \in \mathcal{S}$, 
is encoded by a possibility distribution equal to the classical indicator
function of the singleton $\pi(s) = \mathds{1}_{\set{ s=\tilde{s} }}(s)$.
Between these two extrema, current knowledge of the system is described by
a set of entirely possible states, $\set{ s \in \mathcal{S} \mbox{ s.t. } \pi(s)=1  }$,
and successive sets of less plausible ones $\set{ s \in \mathcal{S} \mbox{ s.t. } \pi(s)=l_i}$
down to the set of impossible states $\set{ s \in \mathcal{S} \mbox{ s.t. } \pi(s)=0  }$.


%
% piPOMDP
%



Enfin, un homologue des POMDPs appel\'e $\pi$-POMDP a \'et\'e d\'evelopp\'e dans 
le cadre de la Th\'eorie des Possibilit\'es \cite{Sabbadin:1999:pipomdp}, 
r\'eduisant la complexit\'e du probl\`eme. De plus, ce cadre possibiliste 
d\'ecrit de mani\`ere qualitative l'incertitude de
l'agent, ce qui permet de rester prudent dans la d\'efinition des param\`etres du probl\`eme. 
Cependant, ce mod\`ele n'a \'et\'e
que tr\`es peu \'etudi\'e, et les recherches le concernant peu abouties.\\


Qualitative possibilistic POMDPs, 
$\pi$-POMDPs \cite{Sabbadin:1999:pipomdp}, 
are alternative processes 
defined using a qualitative evaluation 
of events plausibility
instead of probabilities: 
it allows to formally represent 
agent ignorance, 
and imprecision on observations hazard. 
As number of belief finite, and MDP P complexity \cite{DBLP:journals/corr/abs-1202-3718},
at most exponential the process description.



\section*{Description of our Study}
% Le sujet de la thèse
The goal of this thesis is to show what the Qualitative Possibility Theory can bring 
in Planning under Uncertainty and Sequential uncertainty management in practice, 
in particular through Graphical models,
in terms of simplification and modeling. 
\dots\\*
This paper presents recent contributions 
in the use of the Qualitative Possibility Theory
for planning under uncertainty, 
studied to answer to these concerns.




% ANNONCE DU PLAN DE LA THÈSE
Our contributions consists in : \dots \\*

	
JUSTIFIER LES IDEES FORTES, ET VERIFIER QUIL Y A DEUX IDEEES FORTES par sous section de chaque chapitre
\\


%%%% CHAP1
\subsection*{State of the art}
The \emph{first chapter} presents in parallel \\*

piPOMDP detailed, as never detailed much
to the best of our knowledge

The work developed in this paper remains in the classical MDP and POMDP frameworks, 
which are recalled in this chap: 
possibilistic material necessary to build the promised translation are then presented. 

Ces modèles probabilistes sont conçus pour représenter de manière simple une situation où une entité (appelée \textit{agent}) 
et le monde qui l'entoure peuvent être dans différents états $s \in \mathcal{S}$ au cours du temps 
(ce dernier, modélisé par l'ensemble des entiers naturels $\mathbb{N}$). 
L'agent choisit une action parmi celles dont il dispose à chaque étape de temps $t \in \mathbb{N}$, 
et le système comprenant l'agent et son environnement évolue de manière Markovienne dans $\mathcal{S}$ (\cite{bellman},\cite{ber01}).

Le modèle POMDP, autour duquel ce travail est développé, est utilisé lorsque le système peut se modéliser à l'aide de probabilités. 
Il permet aussi bien de modéliser les incertitudes sur l'observation du système, que sur sa dynamique. 
Nous commencerons donc par présenter un modèle plus simple (MDP, ou l'observation est parfaite) afin de construire plus facilement le modèle POMDP.\\

Dans le modèle POMDP, les distributions de probabilité sont supposées parfaitement connues a priori. En pratique, ce n'est pas toujours le cas, notamment lorsque les observations de l'environnement sont filtrées par des algorithmes de traitement du signal. De plus, l'agent est supposé avoir une croyance initiale de l'état du système: sa croyance initiale n'est pas précise en pratique (extraire des fréquences d'évènements initiaux n'est pas forcément possible), et une probabilité uniforme ne représente pas réellement cette méconnaissance. Il existe aussi des situations dans lequelles, donner de l'importance à l'information de la croyance (au sens entropique) améliore la politique en pratique \cite{conf/stairs/ChanelFTI10}. \\

Ces problèmes pratiques peuvent trouver des solutions grâce à d'autres modélisations de l'incertitude que la théorie des probabilités. C'est pourquoi ce rapport s'intéresse ensuite à des modèles utilisant la théorie des possibilités ($\pi$-MDP et $\pi$-POMDP). Ces deux modèles introduits par R.Sabbadin sont présentés, puis une preuve est donnée à l'homologue de l'algorithme d'itération sur les valeurs. Enfin, l'astuce de O.Buffet \textit{et al.} est adaptée pour le modèle $\pi$-POMDP. 



%%% CHAP2
\subsection*{Natural update of the possibilistic model}
The \emph{second chapter} proposes \\* 

It begins with an update 
of the work of Sabbadin 
about a possibilistic counterpart 
of POMDPs called $\pi$-POMDP. 
a possibilistic version of 
Mixed-Observable MDPs \cite{OngShaoHsuWee-IJRR10}, 
called $\pi$-MOMDP \cite{Drougard13}, 
is first presented to reduce dramatically 
the complexity of solving $\pi$-POMDPs, 
some state variables of which are fully observable. 
An algorithm
for missions with unbounded durations
is next proposed:
returned strategies can outperform 
probabilistic POMDP ones 
for a target recognition problem 
where the agent's observations dynamics 
is not properly defined.
\cite{Drougard13}

La d\'emarche a consist\'e dans un premier temps \`a adapter les processus d\'ecisionnels 
possibilistes qualitatifs aux missions robotiques car ils modelisent formellement la meconnaissance initiale. 
Pour cela, le probl\`eme $\pi$-POMDP
est d\'efini pour un horizon infini, afin que les missions puissent 
\^etre d\'efinies sans d\'ecider a priori de la dur\'ee de la mission. Sachant l'\'etat 
du syst\`eme n'est pas enti\`erement cach\'e en pratique lors des missions \cite{OngShaoHsuWee-IJRR10}, 
les $\pi$-MOMDPs, pour (Mixed-Observable MDPs) 
sont introduits afin de profiter de cette observation dans la r\'eduction du temps de calcul de la
strat\'egie.
Par exemple, le niveau de batterie d'un drone est accessible directement par le syst\`eme robotique. 

 L'algorithme associ\'e \`a ces contributions ainsi que sa 
preuve de convergence est alors n\'ecessaire dans le but de calculer une strat\'egie \cite{Drougard13}. 
Enfin, les performances de ce nouveau 
mod\`ele sont illustr\'ees \`a l'aide de simulations de probl\`emes particuliers pour diff\'erents crit\`eres.
De plus, ces simulations illustrent aussi la dynamique de la croyance de l'agent: sa mise \`a jour
poss\`ede la caract\'eristique int\'eressante d'accro\^itre la connaissance associ\'ee.
Cependant, la r\'esolution propos\'ee reste limit\'ee \`a des probl\`emes de faible taille.\\




%%% CHAP3
\subsection*{Factorization work on the models}
The \emph{third chapter} is \\* 

\cite{DBLP:conf/aaai/DrougardTFD14}
Then strategies computation time is decreased 
with the use of Agebraic Decision Diagrams (ADDs)
and benefiting from the problem structure.
We compare performances 
of our solvers with those of 
its probabilistic counterparts, 
in terms of computation time, 
and with criteria measuring 
the mission achievement. 
While this possibilistic framework 
provides good results,
some highlighted issues
are finally discussed: 
Then factorized $\pi$-MOMDPs
are defined making possible the processing
of large structured planning problems. 
Building upon the probabilistic 
SPUDD algorithm \cite{Hoey99spudd:stochastic},
we conceived an algorithm named PPUDD \cite{DBLP:conf/aaai/DrougardTFD14} 
for solving factorized $\pi$-MOMDPs
using \textit{Algebraic Decision Diagrams} (ADD).
Our experiments and the  results 
of the International Probabilistic Planning Competition (IPPC 2014)
show that this possibilistic approach
can involve lower computation time
and produce better policies
than its probabilistic counterparts:
it highlights also some issues 
of these qualitative models.
Finally, points raised are added to 
the symbolic computations limits,
and lead to the description
of a future work.
We participated in the competition in order to test the performance of our algorithm
in the case of well known probabilistic models.


LETAPE SUIVANTE A ETE DADAPTER LALGO AFIN QUE...
Une fois ce mod\`ele adapt\'e aux besoins robotiques pratiques, nous avons souhait\'e pouvoir
r\'esoudre des probl\`emes de d\'ecision s\'equentielle dans l'incertain de tailles
plus g\'en\'erales. Profitant \`a la fois du caract\`ere qualitatif de ces processus,
ainsi que de la structure des probl\`emes en pratique, un algorithme
symbolique de r\'esolution \cite{DBLP:conf/aaai/DrougardTFD14} est propos\'e. Pour cela, nous 
introduisons les $\pi$-MOMDPs factoris\'es, et d\'ecrivons sur les hypoth\`eses d'ind\'ependance
possibles pour de tels processus.
AFIN DE..
\\
ON a voulu que cet algorithme utilise les ADD pour pouvoir synthetiser les calculs.

L'algorithme r\'esultant, utilisant des arbres de d\'ecision
alg\'ebriques (ADDs) afin de synth\'etiser les calculs, est nomm\'e PPUDD pour \textit{Possibilistic
Planning using Decision Diagrams}
\\
POUR CELA, on a simulté des problemes de plannif variés.
 et a \'et\'e test\'e lors de la comp\'etition internationale 
de planification probabiliste (IPPC14). 
\\
EN TERMES DE REWARD EXPECTED

Ces tests sur des probl\`emes de planification vari\'es m\`enent \`a l'observation que 
ces m\'ethodes \`a ADDs, dites \textit{symboliques}, ne font pas le poids face aux
approches proc\'edant \`a une recherche heuristique dans l'espace d'\'etat \cite{DBLP:conf/aips/KellerE12} (en termes de rewards... expliquer!). 
Nous avons pu aussi prendre note des inconv\'enients du formalisme possibiliste qualitatif pour la mod\'elisation:
le crit\`ere utilis\'e \'etant global, son choix restreint la g\'en\'eralit\'e du mod\`ele.\\


%%% CHAP4
\subsection*{Qualitative possibilistic framework process for human-machine interaction modeling}
The \emph{fourth chapter} is \\* 

Les processus possibilistes qualitatifs ont toutefois leur place lorsque toutes les distributions
de probabilit\'e ne peuvent clairement pas \^etre d\'efinies dans le contexte: dans les syst\`emes
mod\'elisant le comportement human, seules des donn\'ees expertes peuvent \^etre utilis\'ees pour
la mod\'elisation. Pour cela, nous appliquons ces processus... \textbf{Application des processus possibilistes en diagnostique dans l'interaction homme-machine} \\

Processus $\pi$-HMP hidden markov processes, outils de diagnostic pour l'Intéraction Homme-Machine (avec Sergio Pizziol)
\begin{itemize}
\item \textbf{occurrences:} états de la machine et actions humaines;
\item \textbf{évaluation humaine} (de l'état de la machine);
\item \textbf{effets:} transitions, classées par degrés de possibilité.
\end{itemize}
\begin{itemize}
\item \textbf{estimation} de l'état selon l'opérateur humain;
\item \textbf{détection} des erreurs humaines d'évaluation de l'état; 
\item causes plausibles de ces erreurs (\textbf{diagnostique}).
\end{itemize}




%%% CHAP5
\subsection*{Probabilistic-possibilistic approach: an hybrid perspective}
Finally, the \emph{fith chapter} is \\* 

argues for a hydrid POMDP 
model with both probabilistic 
and possibilistic settings. 



