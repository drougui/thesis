\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
Contributions of this thesis mainly starts from 
the preliminary work of R\'egis Sabbadin \cite{Sabbadin:1999:pipomdp}.
The latter proposes a possibilistic counterpart 
of the POMDPs modeling uncertainty 
with qualitative possibility distributions.

%%% CONCL INTRO
In this work, qualitative possibilistic models
for planning under uncertainty are presented,
studied to adress some issues 
of the probabilistic POMDPs also formally introduced.
The major motivation of this study is the complexity reduction: 
while the probabilistic belief space is infinite,
the possibilistic one is finite.
The qualitative possibilistic framework thus offers
an appropriate belief space discretization.
Moreover, total ignorance
The agent belief is a possibility distribution 
over $\mathcal{S}$: total ignorance is defined 
by a belief equal to $1$ on all states, 
whereas a given state $s$ is perfectly known 
if the belief is equal to $1$ on this state 
and to $0$ on all other states.
Imprecision on probability distributions
are naturally encoded with this formalism,
leading to the choice of an optimistic 
and a pessimistic criterion for strategy selection.

The qualitative possibilistic modeling needs less information 
than the probabilistic one since the plausibilities of events 
are ``only'' classified in $\mathcal{L}$ but not quantified.
as in contingent/conformant planning \cite{Albore_atranslation-based},
more refined: \textit{non-determinism}
%% on aurait pu mettre d'ailleur enfant de prob et poss dans la figure gnagna

%%% LINK WITH MOTIVATION
expe PO: robot chap2; robot chap3 PPUDD IPPC; chap4 HMI (belief on the human assessment);

Through this thesis, we developped \dots
updates of possibilistic MDPs, 
modelization examples, 
demonstration of the accuracy of qualitative models,
graphical results (factorization),
simplification,
perspective in a hybrid model
theoretical contributions and practical, 
\textit{i.e.} experimental results and modeling examples
A more detailed review of the contributions follows

\subsection*{chap2}
%%%1) CONTRIBUTIONS
%%%%%%%%%%%%%
%%% chap2 %%%
%%%%%%%%%%%%%
%%THEORETICAL CONTRIBUTIONS
qualitative aggregations of the preferences over time
can be derived from properties of the qualitative framework,
and different approaches from pessimistic and optimistic.
contribution: 
preuve de convergence est alors n\'ecessaire dans le but de calculer une strat\'egie \cite{Drougard13}. puis une preuve est donnée à l'homologue de l'algorithme d'itération sur les valeurs. 

Qualitative possibilistic POMDPs, 
$\pi$-POMDPs \cite{Sabbadin:1999:pipomdp}, 
Mixed-Observable MDPs \cite{OngShaoHsuWee-IJRR10}, 
called $\pi$-MOMDP \cite{Drougard13}, 

%%%PRACTICAL ONES
The experiment uses both previous contributions.
Indeed, the mixed-observability property of the problem,
makes computations feasible in our example.
The proposed criterion is also really useful: 
it is convenient to allow the computation of strategies 
for robotic missions with unbounded durations.
For instance, in our example, 
it allows to define properly the mission: 
if the robot has not figured out the right target,
we want the mission to be continued.
can outperform probabilistic POMDP ones 
for a target recognition problem 
where the agent's observations dynamics 
is not properly defined.

%%%OBSERVATIONS
sa mise \`a jour Bayes rule \cite{Dubois199023}: 
poss\`ede la caract\'eristique int\'eressante d'accro\^itre la connaissance associ\'ee.
+ PROOFS


As proved in \cite{Drougard13}, there exists an optimal strategy $\delta^{\ast}$, 
which is stationary \textit{i.e.} 
which does not depend on the stage of the process $t$. 
It can be found by the following Dynamic Programming (DP) scheme: 
This dynamic programming scheme converges in at most 
$\# \mathcal{X}$ iterations.
The hypothesis of the action $\overline{a}$ existence
is not a constraint in practice
as it is only selected in some goal states.
Note that the value function is defined regardless of the mission duration.

The following application shows the efficiency 
of the $\pi$-MOMDPs framework to provide strategies
well-suited for a recognition mission. 
Consider a robot on a grid $g \times g$, 
which perfectly knows its location $(x,y) \in \{ 1, \ldots, g \}^2$:
each location is a visible state $s_v \in \mathcal{S}_v$. 
The more the robot is far from a target, 
the more the observation is noisy. 

The possibilistic belief update (\ref{possBelUpdate})
is responsible as it only takes into account 
more reliable observations:
the next belief is either more skeptic 
about a state if its confirms the prior belief; 
or changes to the opposite belief if it contradicts it.
On the contrary a probabilistic belief is modified at each step. 


%%% CHAP3
%%% THEORETICAL
GRAPHICAL PROOFS

%% WEE!
Our experiments and the  results 
of the International Probabilistic Planning Competition (IPPC 2014)
show that this possibilistic approach
can involve lower computation time
and produce better policies
than its probabilistic counterparts.
%% OHH!
Ces tests sur des probl\`emes de planification vari\'es m\`enent \`a l'observation que 
ces m\'ethodes \`a ADDs, dites \textit{symboliques}, ne font pas le poids face aux
approches proc\'edant \`a une recherche heuristique dans l'espace d'\'etat \cite{DBLP:conf/aips/KellerE12} (en termes de rewards... expliquer!). 
Nous avons pu aussi prendre note des inconv\'enients du formalisme possibiliste qualitatif pour la mod\'elisation:
le crit\`ere utilis\'e \'etant global, son choix restreint la g\'en\'eralit\'e du mod\`ele.\\
(etre optimiste ou pessimiste?)

%%CHAP3
Consider $\pi$-MOMDPs 
whose space $\mathcal{X}$ is represented by
$n$ boolean variables $X=(X_1, \ldots, X_n)$.
variables $X_i$ of a \textit{factorized $\pi$-MOMDP} 
are said post action independent as there are no
dependences (arrows) between variables of the same time step.

Algorithm PPUDD is a symbolic version of 
the Dynamic Programming scheme
Inspired by SPUDD \cite{Hoey99spudd:stochastic}, 
PPUDD means \emph{Possibilistic Planning Using Decision Diagrams}. 
As SPUDD, it operates on ADDs 
encoding value, preference and transition functions.
operators $\min$ and $\max$, 
ADDs are smaller, and thus computations are faster.

The implementation was performed 
with the \textit{CU Decision Diagram Package}
for the ADDs computations. 

A natural factorisation of $\mathcal{X}$ comes from some independence assumptions
As proved in \cite{DBLP:conf/aaai/DrougardTFD14}, 
beliefs of a $\pi$-MOMDP satisfacing these assumptions
are post action independent beliefs: $x = (s_{v,1}, \ldots, s_{v,m}, \beta_1, \ldots, \beta_{l})$, 
with $\beta_i$ a belief over variable $s_{h,i}$.
The independence of sensors and of corresponding hidden states suffice to fullfil these conditions.
A problem known as 
the \textit{RockSample} problem, 
looks like the one 
of section \ref{QPMOMDP}.
It is more complex 
as the number of targets
is greater than two: 
targets are rocks, 
and the robot is expected 
to sample only ``scientifically good'' ones,
estimating their nature 
through specific observation actions.
As this problem respects assumptions 
encoded by the graphical 
representation in Figure  
the model is naturally expressed 
as a factorized $\pi$-MOMDP. 
PPUDD performances are compared with 
the probabilistic MOMDP solver APPL \cite{Kurniawati08sarsop:efficient}
and the symbolic HSVI solver \cite{citeulike:2961378},
first in terms of computation time, 
and below with the average of 
the total reward at execution 
(see Figure \ref{figureexp2}.b). 
PPUDD computes a strategy maximizing 
exactly the possibilistic criterion 
(\ref{optimizationCriterion})
while APPL and symb. HSVI compute solutions
which approximately maximize the expected sum of rewards.
These experimental results show that using an exact algorithm 
(PPUDD) for an approximate model ($\pi$-MOMDPs) 
can run significantly faster 
than reasoning about exact models, 
while providing better policies 
than approximate algorithms (APPL) 
for exact models.

To focus on the behaviour 
of this possibilistic qualitative approach 
in a wide panel of probabilistic problems, 
we participated  in the fully observable track of the 
\textit{International Probabilistic Planning Competition 2014} 
(IPPC 2014\footnote{\url{https://cs.uwaterloo.ca/~mgrzes/IPPC_2014/}})
with PPUDD. 

Comparing only solvers using ADDs, 
PPUDD and a probabilistic solver called symbolic LRTDP 
as a variant of \cite{Bonet03labeledrtdp:}.
+ risk and optimism traffic
However, as highlighted by the domain ``Traffic''
of the competition (see Figure \ref{IPPC}), 
the possibilistic criterion (\ref{optcriterion})
can be too optimistic.

SPUDD algorithm \cite{Hoey99spudd:stochastic},
we conceived an algorithm named PPUDD \cite{DBLP:conf/aaai/DrougardTFD14} 
for solving factorized $\pi$-MOMDPs
using \textit{Algebraic Decision Diagrams} (ADD).
Our experiments and the  results 
of the International Probabilistic Planning Competition (IPPC 2014)
show that this possibilistic approach
can involve lower computation time
and produce better policies
than its probabilistic counterparts:
it highlights also some issues 
of these qualitative models.
Finally, points raised are added to 
the symbolic computations limits,
and lead to the description
of a future work.



%% fin 3 lien CHAP4-5
Experiments  
concerning the Navigation problem
is a good illustration: 
a robot which has to
reach a goal:
The classical criterion (\ref{optcriterion})
(M1, optimistic) 
leads to a poor strategy, 
while a cautious criterion (M2, pessimistic)
is more suited: 
for most of missions however 
the reverse occurs.
Proposed approaches involves then 
the choice of a criterion.
Note that in this example, 
SPUDD provides good strategies, but 
cannot solve big instances. 

%% IDEM
Presented models 
assume also that preference
and uncertainty degrees
share the same scale
which makes it hard to
set in practice: the use of
the aggregation operator leximin 
is a good alternative,
but makes the problem more complex.
Finally, state space search algorithms
(PROST \cite{conf/aips/KellerE12} and GOURMAND) won IPPC 2014,
and are yet far more efficient than ADD-based methods.

Nous avons observ\'e la difficult\'e du formalisme possibiliste qualitatif \`a la
description d'un probl\`eme de robotique autonome. Nous avons aussi confirm\'e
l'int\'er\^et d'utiliser ce formalisme dans la mod\'elisation de la croyance de l'agent.
De plus, les r\'esultats de la comp\'etition de planification incite \`a abandonner l'approche symbolique pour
une recherche dans l'espace d'\'etats. Pour cela, nous d\'eveloppons un mod\`ele hybride

%% chap HMI 4
fourth chapter shows that 
qualitative possibilistic processes
are an appropriate choice
to model situations 
where probabilities 
cannot be defined
use of leximin

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAP5
Scale $\mathcal{L}$ serves as well to model 
the \emph{preference} over states: 
function $\mu: \mathcal{S} \rightarrow \mathcal{L}$ assigns a degree
at each system state modeling how well this state fulfill the mission. 


A future work is motivated by these issues
and builds a POMDP with possibilistic beliefs in $B_h^{\pi}$,
rewards in $\mathbb{R}$ attached to them,
and probabilistic transition functions.
As ADDs are still efficient for preprocessing,
they are used to translate this hybrid
model into a classical Fully Observable MDP.
Efficient sampling MDP solvers such as PROST 
are then able to solve this simplified POMDP. 



%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%CONCL CONL
Optimal strategies for presented POMDPs (purely qualitative, and hybrid)
are computable in finite time, at most exponential in the description of the problem.


\section*{Perspectives}
%%%
%%% PERSPECTIVES
%%%
test hybrid POMDP\\
refined ranking\\
prob $\rightarrow$ poss\\
learning poss sabbadin\\

independence belief proba, impact on the exact resolution?
\cite{LIP61723} more discriminative criteria
PSR \ref{Littman01predictiverepresentations}
online POMDP solving (ref caro)
The work \cite{Bonet:2002:QMP:2073876.2073884} proposes an other framework
for planning under uncertainty, also qualified as qualitative. 
However, this work uses quantitative operations
as sum and product, and may be seen as a discretization of the probabilistic framework
(see \cite{Wilson:1995:OMC:2074158.2074221}).
The authors remark however that Qualitative Possibility Theory leads to 
criteria which have not enough information for discriminating
among optimal decisions.
%% perspec: ce travail ouvre plein de travaux
pessimistic $\pi$-MDP in order to solve unsafe problems. yiang shenoy
PERSPECTIVES: modest/audacious







%%
%% CONCLUSIONS PAPIERS
%%







%CHAP2
We have proposed a Value Iteration algorithm for pos-
sibilistic  MDPs,  which  can  produce  optimal  station-
ary  policies  in  infinite  horizon  contrary  to  previous
methods.  We have provided a complete proof of con-
vergence  that  relies  on  the  existence  of  intermediate
stay actions that vanish for non goal states in the final optimal policy. 
Finally, we have extended this al-
gorithm to a new Mixed-Observable possibilistic MDP
model, whose complexity is exponentially smaller than
possibilistic  POMDPs,  so  that  we  could  compare
$\pi$-MOMDPs with their probabilistic counterparts on re-
alistic  robotic  problems.    Our  experimental  results
show  that  possibilistic  policies  can  outperform  prob-
abilistic ones when the observation function yields im-
precise results.
Qualitative possibilistic frameworks can however be in-
appropriate when some probabilistic information is ac-
tually available:  POMDPs with Imprecise Parameters
(POMDPIP)  [7]  and  Bounded-parameter  POMDPs
(BPOMDP) [8] integrate the lack of knowledge by con-
sidering  spaces  of  possible  probability  distributions.
When such spaces can not be extracted or when a qual-
itative  modeling  suffices, $\pi$-POMDPs  can  be  a  good
alternative, especially as POMDPIPs and BPOMDPs
are  extremely  difficult to solve in practice.
The pessismistic version of $\pi$-MDPs can be easily con-
structed, but the optimality of the policy returned by
the associated value iteration algorithm seems hard to
prove, essentially because it is not enough to construct
a maximizing trajectory, as the proof of section A does.
The works [16, 10] could help us to get results about


%% CHAP3
We
presented PPUDD, the first algorithm to the best of our knowledge that 
solves factored possibilistic (MO)MDPs with symbolic calculations. In our opinion,
possibilistic models are a good tradeoff between non-deterministic ones, whose
uncertainties are not at all quantified yielding a very approximate model, and
probabilistic ones, where uncertainties are fully specified. Moreover,
$\pi$-MOMDPs reason about finite values in a qualitative scale whereas
probabilistic MOMDPs deal with values in $\mathbb{R}$, which implies larger ADDs
for symbolic algorithms. Also, the former reduce to finite-state belief
$\pi$-MDPs contrary to the latter that yield \emph{continuous}-state belief MDPs
of significantly higher complexity. Our experimental results highlight that using an
exact algorithm (PPUDD) for an approximate model ($\pi$-MDPs) can bring significantly faster computations
% run
%significantly faster 
than reasoning about exact models, while providing better
policies than approximate algorithms (APPL) for exact models. In the future, we
would like to generalize our possibilistic belief factorization theory to
probabilistic settings. %, where a related result has been proposed for
%probabilistic POMDPs but not in the case of mixed observability
%\cite{DBLP:conf/aips/ShaniPBS08}.

%% CHAP4
This paper proposes a model for human-machine interaction
based on a machine model and expert knowledge on a human
assessment error model. The human-machine interaction is
modelled as a possibilistic Hidden Markov Process. Qualitative
Possibility Theory has been chosen because it is well suited to
handle uncertainty defined by expert knowledge. The proposed
possibilistic analysis model provides an estimation of the
human assessment of the machine state and detects assessment
errors. The analysis model can also provide an explanation
(diagnosis) when an assessment error is detected.
This process of detection/identification could be used in real
time applications in order to inform the human operator of
their assessment errors. It can help them correct their situation
awareness and prevent the occurrence of other errors. For
instance they could provide new specific feedbacks meant to
correct the human operator assessment [DCT11]
This work is based on the simplifying assumption that the
human operator is certain about the state of the machine:
a possible extension of this model may be to drop this
assumption using a more refined representation of the human
state assessment, as a set of machine states, or an uncertainty
measure over the machine states.
In this work experts have to estimate the plausibility of
attentional errors once and for all. A more situation-consistent
approach could be to estimate the plausibility of attentional

%% CHAP5
This paper describes a hydrid translation 
of a POMDP into a finite state space MDP one. 
Qualitative Possibility Theory
is used to maintain an epistemic state during the process:
the belief space has a granulated representation,
instead of a continuous one as in the classical translation.
The resulting MDP is entirely defined computing transition and reward functions
over these epistemic states. 
Definitions of these functions 
use respectively the pignistic transformation,
used to recover a probability distribution 
from an epistemic state, 
and the Choquet integral with respect to the necessity, 
making the agent pessimistic about its ignorance. 
A practical way to implement this translation 
is then described:
with these computation tricks, 
a factored POMDP leads to a factored and tractable MDP. 
This promising approach 
will be tested on the POMDPs 
of the IPPC competition \cite{SannerIPPC11} in a future work:
the provided problem descriptions are indeed 
in the form of the factored POMDPs 
introduced in Section \ref{factorizationSection}.


