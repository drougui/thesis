\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
Contributions of this thesis are
mainly related to the preliminary work 
of R\'egis Sabbadin \cite{Sabbadin:1999:pipomdp}.
The latter proposes 
a possibilistic counterpart 
of the POMDPs 
modeling uncertainty 
with qualitative possibility distributions.
%%% CONCL INTRO
Hence, in our work, 
qualitative possibilistic models 
for planning under uncertainty
are developped and studied. 
This study is meant to adress some issues 
of the probabilistic POMDPs 
detailed in Introduction: 
the probabilistic framework is also 
formally introduced 
in the first chapter.

The major motivation of this study 
is the complexity reduction: 
while the probabilistic belief space is infinite,
the possibilistic one is finite.
The qualitative possibilistic framework 
thus offers an appropriate 
belief space discretization.
Moreover, as the belief state 
is a possibility distribution 
over the system space in this framework, 
total ignorance can be defined 
by a belief equal to $1$ on all states.
If a given system state is perfectly known
to be the actual one, 
the belief state equal to $1$ on this state 
and to $0$ on all other states 
is an appropriate representation.

Imprecisions of the probability distributions
are also naturally encoded 
with the possibilistic formalism, 
resulting in two criteria
for the selection of the strategy:
one is optimistic 
and the other pessimistic.
A more practical advantage is that 
the qualitative possibilistic modeling 
needs less information about the system
than the probabilistic one:
the plausibilities of events 
are ``only'' classified 
in the possibilistic scale $\mathcal{L}$ 
but not quantified.
Possibilistic models can be seen 
as a tradeoff between \textit{non-deterministic} ones, 
whose uncertainties are not at all quantified 
yielding a more imprecise model, 
and probabilistic ones, 
where uncertainties are fully specified.
Indeed, under the non-deterministic formalism,
an elementary event is either ``possible'',
or ``impossible'':
no degree is available
to differentiate a highly plausible event 
from an unlikely one.
The resolution of planning problems 
using the framework of non-determinism 
is called \textit{contingent/conformant planning} 
studied for instance in \cite{Albore_atranslation-based,bonet2014flexible}.
By the way, in the introduction of this thesis,
we might also add ``non-determinism''
as a particular case of Possibility Theory
in Figure \ref{uncertainty_theories},
as values of associated non additive measures
are $0$ or $1$ instead of a more flexible scale $\mathcal{L}$.

In a nutshell, 
this thesis consists in
theoretical and practical contributions:
on the one hand, theoretical contributions 
are for instance 
the proposed updates of the qualitative possibilistic processes
-- mixed-observability and management of unbounded executions -- 
or independence results on them, with associated proofs.
On the other hand, 
practical contributions
are the demonstration of the accuracy 
of qualitative possibilistic models
to simplify computations or for modeling,
via experimental results (e.g. IPPC 2014) 
and modeling examples (e.g. chapter on human-machine interaction).
Particular emphasis is being placed 
on the motivations developped in Introduction
besides complexity reduction and problem imprecisions:
namely, robotic applications and belief management.
For instance, target recognition missions 
are studied in the second and third chapters 
(e.g. \textit{Rocksample} problem, 
or even the mission described in Figure \ref{robotgridfig});
% robot chap2; robot reconnaissance chap navig/rocksampple PPUDD-IPPC-robotic system 
IPPC 2014 contains also problems comparable to robotic systems 
(e.g. \textit{Elevator} problem, or \textit{Tamarisk} problem, also used in 2014 RL competition). 
Moreover, the fourth chapter shows good results 
in estimating human assessment with qualitative possibility distributions
(\textit{i.e.} belief states on the human assessment of the machine state).
Finally, the last contribution, 
\textit{i.e.} the hybrid probabilistic-possibilistic POMDP, 
can be seen as a concluding work, taking into account
potential issues when using purely possibilistic models 
for planning under uncertainty.
A more detailed review of the contributions follows.

\subsection*{New features for the $\pi$-POMDPs
and first strategy executions}
%%%%%%%%%%%%%
%%% chap2 %%%
%%%%%%%%%%%%%
%%THEORETICAL CONTRIBUTIONS
The very first update 
of the Qualitative possibilistic POMDPs \cite{Sabbadin:1999:pipomdp} 
concerns the qualitative aggregations 
of the preferences over time:
we first show that, 
as in the probabilistic framework,
preference aggregation can be derived 
from the properties of the framework 
-- here the qualitative counterpart 
of the linearity
for Sugeno integrals for instance --
as proved in Annex \ref{theorem_intermpref_RETURN} 
using the Theorem \ref{piPOMDPoptrewriting} 
about belief-dependent value function.
The latter is also a contribution 
as the first formal construction
of the $\pi$-POMDP model.
Different approaches between the pessimistic and the optimistic criterion
are also presented: 
note that the mixed optimistic-pessimistic criterion 
(see Definition \ref{def_optpesscrit}) 
is mainly used along our work 
since it is equivalent to 
an optimistic criterion for $\pi$-MDP:
this criterion is compatible 
with algorithms for time-unbounded processes proposed next,
and produces better strategies 
for the treated problems.

We then adapted $\pi$-POMDPs 
to mixed-observability \cite{OngShaoHsuWee-IJRR10},
which takes part in the theoretical contributions of our work:
this contribution, called $\pi$-MOMDP, 
dramatically reduces the size of the belief space 
and thus allows the first computations of strategies from $\pi$-POMDPs. 
Finally, an other theoretical contribution 
is the qualitative counterpart 
of the value iteration algorithm,
with the associated criterion 
for time-unbounded executions. 
As proved in Annex \ref{theorem_DPpiMOMDP_RETURN}
(and our publication \cite{Drougard13}), 
there exists an optimal strategy, 
which is stationary 
\textit{i.e.} which does not depend on the stage of the process $t$. 
This strategy can be computed 
by the proposed dynamic programming scheme: 
it is also shown that 
the number of iteration 
to make this scheme converge 
is lower than the size of the state space.
The assumption of the existence of a ``stay'' action
is not a constraint in practice
as it is only selected 
in some goal states.
Note also that the target recognition missions 
presented in this thesis 
have typically unbounded durations.

%%%PRACTICAL ONES
As already pointed out,
the experiments in the second chapter 
use both previous theoretical contributions.
Indeed, the mixed-observability property of the problem,
makes computations feasible for our robotic example.
The proposed criterion is also really useful: 
it is convenient to allow 
the computation of strategies 
for robotic missions 
with unbounded durations.
For instance, in our example, 
it allows to define properly the mission: 
if the robot has not figured out 
which target is right one,
we want the mission to be continued.
The first practical contributions of this thesis 
is thus the computation of a strategy
for this robotic mission and its execution.
Indeed, we have shown 
that the qualitative possibilistic approach 
can outperform probabilistic POMDP ones 
for a target recognition problem 
where the agent's observations dynamics 
is not properly defined.
Note that the only information used 
to define system observation of the system,
is that the more the robot is far from a target, 
the more the observation is noisy:
it already shows that the possibilistic framework
may be useful in case of restricted knowledge 
about the problem.


%%%OBSERVATIONS
Finally, the second chapter highlights 
a very interesting behavior 
of the qualitative possibilistic belief state:
under some conditions, 
the belief update, 
which is defined from the counterpart of Bayes rule \cite{Dubois199023},
increases the knowledge associated to the belief state.
Indeed, in our example, 
the belief state is responsible to the imprecision 
as it only takes into account 
more reliable observations:
the next belief is either more skeptic 
about a state if its confirms the prior belief; 
or changes to the opposite belief if it contradicts it.
On the contrary a probabilistic belief 
is eventually modified at each step. 
Some conditions leading to this behavior 
are then presented,
and associated proofs are given 
(see Annex \ref{theorem_specificity_belief_RETURN}).

\subsection*{Graphical work on independence and factorization (PPUDD)}
%%%%%%%%%%%%%
%%% chap3 %%%
%%%%%%%%%%%%%
%%THEORETICAL CONTRIBUTIONS
%%CHAP3
Next theoretical contribution is 
the introduction of the factored $\pi$-MOMDPs:
indeed, we considered processes
whose space is represented by $n$ 
post action independent variables,
\textit{i.e.} processes without dependence
between state variables of the same time step.
%TODO 
An algorithm meant to exactly solve 
these factorized processes,
called PPUDD, is another contribution:
it is the symbolic version of 
the Dynamic Programming scheme 
proposed in the previous chapter.
Inspired by SPUDD \cite{Hoey99spudd:stochastic}, 
PPUDD means \emph{Possibilistic Planning Using Decision Diagrams}. 
As SPUDD, it operates on ADDs 
encoding value, preference and transition functions.
Finally, the last contributions of the third chapter
are the graphical proofs of independence
on the graphical models representing
factorization of the processes:
we have shown that
a natural factorisation of the state space 
comes from some independence assumptions
As proved in \cite{DBLP:conf/aaai/DrougardTFD14}, 
beliefs of a $\pi$-MOMDP satisfacing these assumptions
are post action independent beliefs: 
The independence of sensors 
and of corresponding hidden states 
suffice to fullfil these conditions.
The \textit{RockSample} problem, 
well illustrates these conditions.

%%%% PRACTICAL CONTRIB TODO
%% WEE!
The motivation leading to the conception of PPUDD,
\textit{i.e.} the guess that the use of operators $\min$ and $\max$ 
leads to smaller ADDs, and thus to faster computations,
has been verified in practice:
our experiments and the results 
of the International Probabilistic Planning Competition (IPPC 2014)
show that this possibilistic approach
can involve lower computation time
and produce better policies
than its probabilistic counterparts
when computation time is limited,
or for high dimensional problems.
PPUDD performances have been compared 
with the probabilistic MOMDP solver APPL \cite{Kurniawati-RSS08,OngShaoHsuWee-IJRR10}
and the symbolic HSVI solver \cite{Sim:2008:SHS:1620163.1620241},
in terms of computation time, 
and using the average of 
the total reward at execution. 
PPUDD computes a strategy maximizing 
exactly the possibilistic criterion 
(\ref{optimizationCriterion})
while APPL and symb. HSVI compute solutions
which approximately maximize the expected sum of rewards.
These experimental results show that using an exact algorithm 
(PPUDD) for an approximate model ($\pi$-MOMDPs) 
can run significantly faster 
than reasoning about exact models, 
while providing better policies 
than approximate algorithms (APPL) 
for exact models.

%% IPPC14
To focus on the behaviour 
of this possibilistic qualitative approach 
in a wide panel of probabilistic problems, 
the next practical contribution 
is the participation 
in the fully observable track of the 
\textit{International Probabilistic Planning Competition 2014} 
(IPPC 2014\footnote{\url{https://cs.uwaterloo.ca/~mgrzes/IPPC_2014/}})
with PPUDD. 
The implementation of PPUDD for this computation 
was performed with the \textit{CU Decision Diagram Package}
for the ADDs computations. 
Comparing only solvers using ADDs, 
PPUDD and a probabilistic solver called symbolic LRTDP 
as a variant of \cite{Bonet03labeledrtdp:},
our solver (PPUDD) produces clearly better strategies 
than the probabilistic one.

%% ??  after? critique
+ risk and optimism traffic
However, as highlighted by the domain ``Traffic''
of the competition (see Figure \ref{IPPC}), 
the possibilistic criterion (\ref{optcriterion})
can be too optimistic.
Ces tests sur des probl\`emes de planification vari\'es m\`enent \`a l'observation que 
ces m\'ethodes \`a ADDs, dites \textit{symboliques}, ne font pas le poids face aux
approches proc\'edant \`a une recherche heuristique dans l'espace d'\'etat \cite{DBLP:conf/aips/KellerE12} (en termes de rewards... expliquer!). 
Nous avons pu aussi prendre note des inconv\'enients du formalisme possibiliste qualitatif pour la mod\'elisation:
le crit\`ere utilis\'e \'etant global, son choix restreint la g\'en\'eralit\'e du mod\`ele.\\
(etre optimiste ou pessimiste?)
it highlights also some issues 
of these qualitative models.
Finally, points raised are added to 
the symbolic computations limits,
and lead to the description
of a future work.


\subsection*{Discussion about current results}
%% fin 3 lien CHAP4-5
Experiments  
concerning the Navigation problem
is a good illustration: 
a robot which has to
reach a goal:
The classical criterion (\ref{optcriterion})
(M1, optimistic) 
leads to a poor strategy, 
while a cautious criterion (M2, pessimistic)
is more suited: 
for most of missions however 
the reverse occurs.
Proposed approaches involves then 
the choice of a criterion.
Note that in this example, 
SPUDD provides good strategies, but 
cannot solve big instances. 

%% IDEM
Presented models 
assume also that preference
and uncertainty degrees
share the same scale
which makes it hard to
set in practice: the use of
the aggregation operator leximin 
is a good alternative,
but makes the problem more complex.
Finally, state space search algorithms
(PROST \cite{conf/aips/KellerE12} and GOURMAND) won IPPC 2014,
and are yet far more efficient than ADD-based methods.

Nous avons observ\'e la difficult\'e du formalisme possibiliste qualitatif \`a la
description d'un probl\`eme de robotique autonome. Nous avons aussi confirm\'e
l'int\'er\^et d'utiliser ce formalisme dans la mod\'elisation de la croyance de l'agent.
De plus, les r\'esultats de la comp\'etition de planification incite \`a abandonner l'approche symbolique pour
une recherche dans l'espace d'\'etats. Pour cela, nous d\'eveloppons un mod\`ele hybride

%% chap HMI 4
\subsection*{Estimating in a qualitative world (HMI)}
fourth chapter shows that 
qualitative possibilistic processes
are an appropriate choice
to model situations 
where probabilities 
cannot be defined
use of leximin

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAP5
\subsection*{A possibilistic discretization of the belief space (hybrid POMDP}
Scale $\mathcal{L}$ serves as well to model 
the \emph{preference} over states: 
function $\mu: \mathcal{S} \rightarrow \mathcal{L}$ assigns a degree
at each system state modeling how well this state fulfill the mission. 


A future work is motivated by these issues
and builds a POMDP with possibilistic beliefs in $B_h^{\pi}$,
rewards in $\mathbb{R}$ attached to them,
and probabilistic transition functions.
As ADDs are still efficient for preprocessing,
they are used to translate this hybrid
model into a classical Fully Observable MDP.
Efficient sampling MDP solvers such as PROST 
are then able to solve this simplified POMDP. 
perspective in a hybrid model.


%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%CONCL CONL
Optimal strategies for presented POMDPs (purely qualitative, and hybrid)
are computable in finite time, at most exponential in the description of the problem.


\section*{Perspectives}
%%%
%%% PERSPECTIVES
%%%
test hybrid POMDP\\
refined ranking\\
prob $\rightarrow$ poss\\
learning poss sabbadin\\

independence belief proba, impact on the exact resolution?
\cite{LIP61723} more discriminative criteria
PSR \ref{Littman01predictiverepresentations}
online POMDP solving (ref caro)
The work \cite{Bonet:2002:QMP:2073876.2073884} proposes an other framework
for planning under uncertainty, also qualified as qualitative. 
However, this work uses quantitative operations
as sum and product, and may be seen as a discretization of the probabilistic framework
(see \cite{Wilson:1995:OMC:2074158.2074221}).
The authors remark however that Qualitative Possibility Theory leads to 
criteria which have not enough information for discriminating
among optimal decisions.
%% perspec: ce travail ouvre plein de travaux
pessimistic $\pi$-MDP in order to solve unsafe problems. yiang shenoy
PERSPECTIVES: modest/audacious
modest: more stronger discretization






%%
%% CONCLUSIONS PAPIERS
%%







%CHAP2
We have proposed a Value Iteration algorithm for pos-
sibilistic  MDPs,  which  can  produce  optimal  station-
ary  policies  in  infinite  horizon  contrary  to  previous
methods.  We have provided a complete proof of con-
vergence  that  relies  on  the  existence  of  intermediate
stay actions that vanish for non goal states in the final optimal policy. 
Finally, we have extended this al-
gorithm to a new Mixed-Observable possibilistic MDP
model, whose complexity is exponentially smaller than
possibilistic  POMDPs,  so  that  we  could  compare
$\pi$-MOMDPs with their probabilistic counterparts on re-
alistic  robotic  problems.    Our  experimental  results
show  that  possibilistic  policies  can  outperform  prob-
abilistic ones when the observation function yields im-
precise results.
Qualitative possibilistic frameworks can however be in-
appropriate when some probabilistic information is ac-
tually available:  POMDPs with Imprecise Parameters
(POMDPIP)  [7]  and  Bounded-parameter  POMDPs
(BPOMDP) [8] integrate the lack of knowledge by con-
sidering  spaces  of  possible  probability  distributions.
When such spaces can not be extracted or when a qual-
itative  modeling  suffices, $\pi$-POMDPs  can  be  a  good
alternative, especially as POMDPIPs and BPOMDPs
are  extremely  difficult to solve in practice.
The pessismistic version of $\pi$-MDPs can be easily con-
structed, but the optimality of the policy returned by
the associated value iteration algorithm seems hard to
prove, essentially because it is not enough to construct
a maximizing trajectory, as the proof of section A does.
The works [16, 10] could help us to get results about


%% CHAP3
We
presented PPUDD, the first algorithm to the best of our knowledge that 
solves factored possibilistic (MO)MDPs with symbolic calculations. In our opinion,
possibilistic models are a good tradeoff between non-deterministic ones, whose
uncertainties are not at all quantified yielding a very approximate model, and
probabilistic ones, where uncertainties are fully specified. Moreover,
$\pi$-MOMDPs reason about finite values in a qualitative scale whereas
probabilistic MOMDPs deal with values in $\mathbb{R}$, which implies larger ADDs
for symbolic algorithms. Also, the former reduce to finite-state belief
$\pi$-MDPs contrary to the latter that yield \emph{continuous}-state belief MDPs
of significantly higher complexity. Our experimental results highlight that using an
exact algorithm (PPUDD) for an approximate model ($\pi$-MDPs) can bring significantly faster computations
% run
%significantly faster 
than reasoning about exact models, while providing better
policies than approximate algorithms (APPL) for exact models. In the future, we
would like to generalize our possibilistic belief factorization theory to
probabilistic settings. %, where a related result has been proposed for
%probabilistic POMDPs but not in the case of mixed observability
%\cite{DBLP:conf/aips/ShaniPBS08}.

%% CHAP4
This paper proposes a model for human-machine interaction
based on a machine model and expert knowledge on a human
assessment error model. The human-machine interaction is
modelled as a possibilistic Hidden Markov Process. Qualitative
Possibility Theory has been chosen because it is well suited to
handle uncertainty defined by expert knowledge. The proposed
possibilistic analysis model provides an estimation of the
human assessment of the machine state and detects assessment
errors. The analysis model can also provide an explanation
(diagnosis) when an assessment error is detected.
This process of detection/identification could be used in real
time applications in order to inform the human operator of
their assessment errors. It can help them correct their situation
awareness and prevent the occurrence of other errors. For
instance they could provide new specific feedbacks meant to
correct the human operator assessment [DCT11]
This work is based on the simplifying assumption that the
human operator is certain about the state of the machine:
a possible extension of this model may be to drop this
assumption using a more refined representation of the human
state assessment, as a set of machine states, or an uncertainty
measure over the machine states.
In this work experts have to estimate the plausibility of
attentional errors once and for all. A more situation-consistent
approach could be to estimate the plausibility of attentional

%% CHAP5
This paper describes a hydrid translation 
of a POMDP into a finite state space MDP one. 
Qualitative Possibility Theory
is used to maintain an epistemic state during the process:
the belief space has a granulated representation,
instead of a continuous one as in the classical translation.
The resulting MDP is entirely defined computing transition and reward functions
over these epistemic states. 
Definitions of these functions 
use respectively the pignistic transformation,
used to recover a probability distribution 
from an epistemic state, 
and the Choquet integral with respect to the necessity, 
making the agent pessimistic about its ignorance. 
A practical way to implement this translation 
is then described:
with these computation tricks, 
a factored POMDP leads to a factored and tractable MDP. 
This promising approach 
will be tested on the POMDPs 
of the IPPC competition \cite{SannerIPPC11} in a future work:
the provided problem descriptions are indeed 
in the form of the factored POMDPs 
introduced in Section \ref{factorizationSection}.


