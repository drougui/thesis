\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\section*{Contexte}
%%% CONTEXT
\malettrine{L'}{autonomie} décisionnelle d'un robot provient, 
entre autre, 
du calcul d'une fonction 
appelée \textit{stratégie} ou \textit{politique}: 
celle-ci retourne 
le symbole de l'action à exécuter
en fonction de l'historique 
des données des capteurs du robot.
Les caractéristiques d'intérêt du robot
et de son environnement 
forment un \textit{système}.
En général, 
pour une séquence donnée 
d'actions exécutées par le robot,
l'évolution de ce système n'est pas déterministe, 
mais son comportement peut être étudié 
en effectuant de nombreux tests 
sur le système robotique, 
ou en utilisant les connaissances d'un expert.
De même, 
les données provenant des capteurs, 
brutes ou traitées,
sont généralement 
des éléments incertains:
cependant, 
le comportement de ces données,
appelées aussi \textit{observations} du système, 
dépend des actions du robot 
et des états successifs du système.
Les relations entre les observations, 
les états du système et les actions 
peuvent aussi être obtenues 
à l'aide de tests des capteurs 
dans de nombreuses situations, 
ou en utilisant la description de ces même capteurs,
du traitement des données, 
ou n'importe quelle information experte.
Par exemple, 
dans le cas d'un robot 
utilisant une vision artificielle,
la sortie de l'algorithme 
de traitement d'image utilisé
est considérée 
comme une observation du système
puisque résultat 
d'un traitement des données des capteurs,
et entrée du modèle de décision:
ici, les données sont les images provenant de la camera.
Pour une camera données, et un algorithme de vision donné,
le comportement de l'observation
est lié à l'action
et à l'état du système
lors du processus de prise des images.

Ainsi, dans le but de rendre un robot autonome
pour une \textit{mission} donnée, %%%%%%
nous cherchons une stratégie, \textit{i.e.} 
une fonction précisant les actions à exécuter
conditionnellement à la séquence d'observations du système,
tenant compte de l'incertitude à propos
de l'évolution du système et de ses observations. 
Le domaine de recherche associé 
à ce type de problème,
\textit{i.e.} le calcul de stratégies,
n'est pas restreint à la robotique
et est appelé \textit{décision séquentielle dans l'incertain}:
dans le cas général, l'entité qui doit effectuer l'action est appelée
\textit{l'agent}.
Dans cette thèse, bien que les résultats fournis sont principalement théoriques
et assez généraux pour concerner des applications plus variées,
le problème du calcul de stratégie
est étudié ici dans le contexte de la robotique autonome,
et l'agent est la partie décisionnelle du robot.
Calculer une stratégie pour une mission robotique donnée
nécessite un cadre adapté:
le modèle le plus connu décrit le comportement de l'état du système 
et des observations en utilisant la théorie de probabilités.

\subsection*{Un modèle probabiliste pour le calcul de stratégies}

Les processus décisionnels de Markov (PDMs)
expriment aisément les problèmes de décision séquentielle
sous incertitude probabiliste \cite{Bel}.
Ils sont adaptés au calcul des stratégies
si l'état du système est connu par l'agent 
à chaque moment de la mission.
Dans le contexte robotique,
cette hypothèse signifie
que la mission considérée
nous permet de faire l'hypothèse
que le robot a une connaissance parfaite
des caractéristiques d'intérêt via ses capteurs.
Dans ce modèle,
l'état du système est noté $s$,
et l'ensemble fini de tous les états possibles du système 
est $\mathcal{S}$.
L'ensemble fini $\mathcal{A}$
est l'ensemble de toutes les actions disponibles pour l'agent.
Le temps est discrétisé en étapes de décision
représentées par les entiers $t \in \mathbb{N}$.

Il est supposé que la dynamique de l'état du système
est \textit{Markovienne}:
à chaque étape de temps $t$,
l'état suivant $s_{t+1} \in \mathcal{S}$,  
ne dépend que de l'état courant $s_t \in \mathcal{S}$
et de l'action choisie $a_t \in \mathcal{A}$.
Cette relation est décrite par la fonction de transition
$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t}$,
définie comme la distribution de probabilité
sur l'état suivant $s_{t+1}$ 
conditionnellement à l'état courant $s_t \in \mathcal{S}$ 
lors de l'exécution de l'action $a_t \in \mathcal{A}$.

La mission de l'agent est décrite en terme de \textit{récompenses}:
une fonction de récompense $r: (s,a) \mapsto r(s,a) \in \mathbb{R}$ 
est définie pour chaque action $a \in \mathcal{A}$
et chaque état du système $s \in \mathcal{S}$.
Elle modélise le but de l'agent.
Chaque valeur de récompense $r(s,a) \in \mathbb{R}$
est une motivation locale pour l'agent.
En effet, plus l'agent récupère de récompenses pendant l'exécution du processus,
mieux il a réalisé sa mission:
une mission est considéré bien remplie
si la séquence d'états du système $s_t \in \mathcal{S}$ rencontrés
et d'actions $a_t \in \mathcal{A}$ sélectionnées
mènent à des récompenses $r(s_t,a_t)$ grandes.
La résolution d'un PDM correspond au calcul
d'une stratégie optimale,
\textit{i.e.} d'une fonction prescrivant les actions $a \in \mathcal{A}$
qu'il faut exécuter au cours du temps 
afin de maximiser la moyenne
de la somme des récompenses obtenues durant une exécution:
cette moyenne est calculée 
en prenant compte le comportement probabiliste des états du système
décrit par les fonctions de transition
$\textbf{p} \paren{s_{t+1} \sachant s_t,a_t}$.
Par exemple, une bonne stratégie
peut être une fonction $d$ définie sur $\mathcal{S}$
et à valeurs dans $\mathcal{A}$, 
puisque l'on suppose ici que l'agent connaît l'état du système
durant le processus.

%%% ROBOT POMPIER
\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\node (rpomp) at (-1,4.7) {\includegraphics[scale=0.9]{robot_pompier}};
\node (bli) at (0,6.5) {};
\node (pomdp3) at (3.9,6) {\color{orange}{$s \in \mathcal{S}$}: \color{black}{\textbf{état du système}}};
\node (t1) at (2,6) {};
\node (r1) at (-2.3,5.5) {};
\node (r11) at (1,4.5) {};
\draw[->,>=latex,color=orange!60,line width=1mm] (t1) to (r1);
\draw[->,>=latex,color=orange!60,line width=1mm] (t1) to (r11);
\node (pomdp4) at (5.8,4.45) {\color{blue!60}{$o \in \mathcal{O}$:} \color{black} \textbf{observations du système}};
\node (t2) at (3.3,4.4) {};
\node (r2) at (-2.3,5.05) {};
\draw[->,>=latex,color=blue!40,line width=1mm] (t2) to[bend left] (r2);
\node (pomdp5) at (1.8,3) {\color{red}{$a \in \mathcal{A}$:} \color{black} \textbf{actions de l'agent}};
\node (t3) at (-0.3,3) {};
\node (r3) at (-2,4.2) {};
\draw[->,>=latex,color=red!50,line width=1mm] (t3) to (r3);
\node (pomdp6) at (-5.5,5.5) {\color{ggreen}{$b \in \mathbb{P}^{\mathcal{S}}$:} \color{black} }; %%% ORANGE?
\node (pomdp7) at (-5,5) {\color{black} \textbf{état de croyance} }; %%% ORANGE?
\node (pomdp6) at (-3.5,6) {\color{ggreen}{\Huge \textbf{?}}}; %%% ORANGE?
\end{tikzpicture}
\caption[Utilisation d'un PDMPO pour la modélisation du robot pompier]{
Utilisation d'un PDMPO pour la modélisation du robot pompier:
dans cet exemple, la mission du robot est la prévention des incendies.
L'état du système $s \in \mathcal{S}$ décrit par exemple la position du robot, 
l'orientation du jet d'eau, la quantité d'eau utilisée,
la position du feu et son niveau sur une échelle ``petit feu'' et ``feu important'', etc.
En utilisant une vision artificielle et des capteurs de chaleur,
le robot reçoit des \textbf{observations} $o \in \mathcal{O}$
qui sont les données brutes ou traitées provenant des capteurs:
la sortie d'un classifieur dont l'entrée est une image de la scène
(\textit{cf.} Figure \ref{observation_robot}), 
et qui renvoie le niveau ou la position du feu,
peut être modélisé par une observation.
Finallement, les \textbf{actions du robot} $a \in \mathcal{A}$ 
sont par exemple la mise en marche des moteurs
impactant la rotation des roues du robot, 
le débit de pompage,
l'orientation du jet d'eau ou des capteurs, etc.
%Uncertainty dynamics is described by conditional probability distributions:
La \textbf{fonction de récompense} $r(s,a)$
décroît avec le niveau de l'incendie.
Afin de ne pas gaspiller d'eau,
un coût proportionnel à la quantité d'eau
est soustrait à cette récompense:
puisque une stratégie optimale maximise la moyenne de la somme des récompenses,
le but du robot est donc d'attaquer les incendies sans gaspiller d'eau.
Cette moyenne peut-être calculée à l'aide des probabilités décrivant la dynamique stochastique du système.
Les actions du robot $a \in \mathcal{A}$ ont un effet probabiliste sur le système,
décrit par la \textbf{fonction de transition} $\textbf{p} \paren{s' \sachant s,a}$: 
par exemple, l'activation des roues du moteur modifie la position du robot,
et la probabilité sur chacune des positions suivantes possibles,
étant donné la position courante,
prend part à la définition du PDMPO.
Un autre exemple est l'action modifiant l'orientation du jet d'eau,
qui redéfinit la probabilité du nouveau niveau de feu étant donné l'état actuel du système.
Les actions du robot $a \in \mathcal{A}$
et les états suivants $s' \in \mathcal{S}$ 
peuvent aussi impacter les observations des capteurs:
cette influence est définie par la \textbf{fonction d'observation} $\textbf{p} \paren{o' \sachant s',a}$: 
par exemple, l'orientation du capteur de vision peut modifier
la probabilité de détection du feu, 
ou de l'évaluation de son intensité, 
qui font partie des observations $o' \in \mathcal{O}$. 
Finalement, l'état de croyance est la distribution de probabilité sur l'état courant du système 
conditionnellement à l'ensemble des observations et actions successives depuis le début du processus:
c'est la meilleure estimation possible puisque le robot n'a accès qu'aux actions et observations
lors de l'exécution.}
\label{robot_pompier}
\end{figure}

Il a été montré que de telles stratégies markoviennes sont optimales
pour certain critères
tels que celui basé sur la somme des récompenses décomptées:
en effet, un critère bien connu mesurant 
les performances d'une stratégie $d$
est l'espérance de la somme actualisée des récompenses: 
\begin{equation}
\label{criterion}
\mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,d_t) },
\end{equation}
où $d_t=d(s_t) \in \mathcal{A}$ 
et $0<\gamma<1$ est un facteur d'actualisation
assurant la convergence de la somme.

L'hypothèse que l'agent a une connaissance parfaite de l'état du système
est assez forte:
en particulier, dans le cas des robots réalisant des taches avec des capteurs conventionnels,
ces derniers sont souvent incapable de fournir au robot
toutes les caractéristiques d'intérêt pour la mission.
Ainsi, un modèle plus flexible, 
\textit{i.e.} qui tient compte de \textit{l'observation partielle} du système par l'agent, a été construit.
%%% ROBOT POMPIER FIN

%%% POMDP
En effet, les MDP Partiellement Observables 
(PDMPO) \cite{Smallwood_Sondik} 
ont une puissance de modélisation plus importante,
car ils peuvent représenter des situations 
dans lesquelles l'agent
ne connaît pas directement l'état courant du système:
ils modélisent de manière plus fine 
un agent exécutant des actions 
sous incertitude dans un environnement
partiellement observable. 

L'ensemble des états du système $\mathcal{S}$,
l'ensemble des actions $\mathcal{A}$, 
la fonction de transition $\textbf{p} \paren{ s_{t+1} \sachant s_t,a_t}$ 
et la fonction de récompense $r(s,a)$ 
restent les même que pour la définition des PDM.
Dans ce modèle, puisque l'état courant du système
$s \in \mathcal{S}$ 
ne peut pas être considéré comme une information accessible pour l'agent,
la connaissance de l'agent à propos de l'état du système 
provient des observations $o \in \mathcal{O}$, 
où $\mathcal{O}$ est un ensemble fini. 
%A full definition of this process 
%includes as well the set of 
%possible observations of the system, 
%$o \in \mathcal{O}$. 
La fonction d'observation 
$\textbf{p} \paren{ o_{t+1} \sachant s_{t+1},a_t }$
donne pour chaque action $a_t \in \mathcal{A}$
et état atteint $s_{t+1} \in \mathcal{S}$, 
la probabilité sur les observations possibles $o_{t+1} \in \mathcal{O}$. 
Enfin, \textit{l'état de croyance initial} $b_0(s)$
définit la distribution de probabilité \textit{a priori}  
sur l'état du système.
Un exemple d'usage des PDMPO est illustré 
dans la figure \ref{robot_pompier}.

Résoudre un POMDP consiste à calculer une stratégie 
qui renvoie une action adéquate à chaque étape du processus, 
et dépendente des observations reçues et des actions sélectionnées
\textit{i.e.} de toutes les données disponibles pour l'agent:
un critère pour la stratégie peut aussi être
l'espérance de la somme actualisée des récompenses (\ref{criterion}).

%
% belief
%
La plupart des algorithmes raisonnent sur \textit{l'état de croyance},
défini comme la distribution de probabilité sur l'état du système
conditionnellement à toutes les observations du système 
et les actions choisies par l'agent depuis le début du processus.
Cet état de croyance est mis à jour à chaque étape de temps en utilisant la règle de Bayes,
l'action courante,
et la nouvelle observation.
\`A une étape donnée $t \in \mathbb{N}$, 
$b_t(s)$ est défini
comme la probabilité que le $t^{ième}$ état du système soit 
$s \in \mathcal{S}$, connaissant les observations et actions précédentes,
ainsi que l'état de croyance initial $b_0$:
c'est une estimation de l'état du système
qui utilise uniquement les données disponibles
puisque l'état n'est pas directement observable.

Il peut être facilement calculé de manière récursive avec la règle de Bayes: 
à l'étape de temps $t$, 
si l'état de croyance est $b_t$, 
l'action choisie $a_t \in \mathcal{A}$ 
et la nouvelle observation 
$o_{t+1} \in \mathcal{O}$, 
l'état de croyance suivant
\begin{eqnarray}
\label{probBayesRule}
b_{t+1}(s')  \propto \textbf{p} \paren{ o_{t+1} \sachant s', a_t } \cdot \sum_{s \in \mathcal{S}} \textbf{p} \paren{s' \sachant s,a_t} \cdot b_t(s).
\end{eqnarray}
comme illustré par le réseau Bayesien de la figure \ref{BayesNetPOMDP}.
%
% BAYESNET
%
\begin{figure}\centering
\begin{tikzpicture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%vertex
\tikzstyle{vertex}=[circle,fill=black!40,minimum size=30pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red! 60,minimum size=25pt,inner sep=0pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=30pt,inner sep=0pt,draw=black,thick]
%nodes
\node[vertex] (state1) at (0,1.8) {$S_t$};
\node[vertex] (state2) at (7,1.8) {$S_{t+1}$};
\node[overtex] (obs) at (13,0) {$O_{t+1}$};
\node[avertex] (action) at (2.5,0) {$a_t$};
%%bels
\node (bel1) at (1,2.5) {$b_t$};
\node (bel2) at (8.5,2.5) {$b_{t+1}$};
%probas
\node (trans) at (4.5,1.5) {$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t }$};
\node (observ) at (9.6,0.3) {$\textbf{p} \paren{ o_{t+1} \sachant s_{t+1}, a_t }$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS
\draw[->,>=latex, thick] (state1) -- (state2);
\draw[->,>=latex, thick] (state2) -- (obs);
\draw[->,>=latex, thick] (action) -- (state2);
\draw[->,>=latex, thick] (action) -- (obs);
\end{tikzpicture}
\caption[Réseau Bayesien illustrant la mise à jour de l'état de croyance]{Réseau Bayesien illustrant la mise à jour de l'état de croyance: 
les états sont représentés par des ronds gris, 
l'action est représentées par le losange rouge,
et l'observation par le rond bleu.
La variable aléatoire $S_{t+1}$
représentant l'état suivant $s_{t+1}$ 
dépend de l'état courant $s_t$ 
et de l'action courante $a_t$.
La variable aléatoire $O_{t+1}$ 
réprésentant l'observation suivante $o_{t+1}$ 
dépend de l'état suivant $s_{t+1}$
et de l'action courante $a_t$.
L'état de croyance $b_{t}$ (resp. $b_{t+1}$)
est l'estimation probabiliste du l'état courant (resp. suivant) du système, 
$s_t$ (resp. $s_{t+1}$).}
\label{BayesNetPOMDP}
\end{figure}%

Puisque les états de croyance successifs sont calculés avec les observations perçues par l'agent,
ils sont considérés visible par l'agent. 
%Moreover, it can be easily shown that
%the expected total reward can be rewritten 
%\begin{equation}
%\label{probCriterion}
%\mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,d_t) } = \mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(b_t,d_t) }, 
%\end{equation}
%defining $r(b_t,a) = \sum_s r(s,a) \cdot b_t(s)$ as the reward of belief $b_t$.
Notons $\mathbb{P}^{\mathcal{S}}$ 
l'ensemble continu des distributions de probabilité
sur $\mathcal{S}$.
Une stratégie optimale peut être cherchée parmi
les fonction $d$ définies sur $\mathbb{P}^{\mathcal{S}}$ 
telles que les $d_t = d(b_t) \in \mathcal{A}$ successifs 
maximisent l'espérance des récompenses (\ref{criterion}):
les décisions de l'agent sont basées sur l'état de croyance.

%
% POMDP ROBOTICS
%
Les PDMPO fournissent un cadre flexible pour la robotique autonome,
comme illustré par l'exemple du robot pompier, \textit{cf.} figure \ref{robot_pompier}:
ils permettent de décrire le système regroupant le robot et son environnement,
ainsi que la mission du robot.
Ils sont fréquemment utilisés en robotique
\cite{PineauG05,OngShaoHsuWee-IJRR10,Marthi12,ChanelTL12,ChanelTL13}.
En effet, ils prennent en compte le fait que le robot ne reçoit que les données des capteurs,
et doit estimer l'état du système, qui lui est caché,
en utilisant ces données, appelées alors observations,
afin de réaliser sa mission.
Cependant, le modèle PDMPO soulève quelques problèmes,
en particulier dans le contexte robotique.

%
% HIGH COMPLEXITY
%
\section*{Problèmes pratiques des PDMPO}
\subsection*{Complexité}
Résoudre un PDMPO \textit{i.e.} 
calculer une stratégie optimale,
est PSPACE-hard en horizon fini \cite{Papadimitriou1987} 
et même indécidable en horizon infini \cite{Madani1999UPP315149.315395}.
De plus, un espace exponentiel en la description du problème peut être requis
pour une spécification explicite d'une telle stratégie.
Le travail \cite{Mundhenk2000CPP867838} 
est un bon résumé des l'analyses de complexité
des PDMPO.

Cette forte complexité
est très bien connue des utilisateurs des PDMPO:
l'optimalité ne peut être atteinte que pour des petits problèmes,
ou bien des problèmes très structurés.
Les approches classiques essaient de résoudre ce problème
en utilisant la programmation dynamique
et des techniques de programmation linéaire
\cite{Cassandra97incrementalpruning}.
Sinon, seules des solutions approchées peuvent être calculées,
et donc la stratégie n'a pas de garantie d'optimalité.
Par exemple, les approches populaire
telles que les méthodes basées sur les points,
\cite{Pineau_2003_4826,Kurniawati-RSS08,Smith2004HSV1036843.1036906}, 
celles basées sur des grilles \cite{Geffner98solvinglarge,Brafman97aheuristic,Bonet_newgrid-based}
ou bien les approches de Monte Carlo approaches \cite{NIPS2010_4031},
utilisent des calculs approchés.

%
% VISION IN ROBOTICS
%
\begin{figure} \centering
\includegraphics[scale=0.75]{fig2}
\caption[Exemple d'une méthode d'observation dans le contexte robotique]{
Exemple d'une méthode d'observation dans le contexte robotique:
le robot, ici un drone, est équippé d'une caméra
et utilise un classifieur calculé à partir d'une base de données d'images
(comme NORB, \textit{cf.} figure \ref{NORB}).
Le classifieur est généré avant la mission (off-line)
avec une base de données d'images
(\textit{cf.} partie droite de l'illustration), 
et la sortie du classifieur 
est utilisée lors de la mission (online) 
comme une observation pour l'agent (\textit{cf.} la partie gauche).
Ici, les observations sont donc générées par un algorithme de vision artificielle.}
\label{observation_robot}
\end{figure}
%
% VISION IN ROBOTICS END
%


%
% COMPUTER VISION
%
\subsection*{Imprécision des Paramètres et Vision Artificielle}
Considérons maintenant des robots utilisant la perception visuelle,
et dont les observation proviennent d'algorithmes de vision
basé sur de l'apprentissage statistique.
(\textit{cf.} figure \ref{observation_robot}).
Dans cette situation, le robot utilise un \textit{classifieur}
pour reconnaître les objects dans les images:
le classifieur est censé renvoyer le nom de l'objet
qui se trouve dans l'image,
et fait quelquefois des erreurs avec une faible probabilité.
(\textit{i.e.} matrice de confusion de la figure \ref{confusion_matrix}).

Le classifieur est calculé en utilisant une \textit{base de donnée d'entraînement} 
(comme NORB, \textit{cf.} figure \ref{NORB}, 
mis en ligne par les auteurs à l'adresse \url{http://www.cs.nyu.edu/~ylclab/data/norb-v1.0/}).
Un algorithme utilisant des descentes de gradient 
utilisé pour calculer le classifieur
en utilisant la base de données d'images
est appelé Réseau Convolutionnel \cite{Lecun98gradient-basedlearning}.
Les figures (\ref{NORB}) et (\ref{confusion_matrix})
illustrent l'exemple d'un classifieur calculé pour une mission dronique
dans laquelle les caractéristiques d'intérêt
(les état du système)
sont liées à la présence (ou l'abscence)
d'animaux, voitures, humains, avions ou camions: 
le problème statistique du calcul d'un classifieur 
permettant de reconnaitre de tels objets dans les images
est appelé \textit{classification multi-classes}.

%%%
%%% NORB DATA SET
%%%
\newcounter{moncompteur} %define counter
\begin{figure} \centering
\begin{tikzpicture}
\node (notes) at (1cm,7cm) { 
NORB dataset: \color{blue} $(\mbox{image}_i,\mbox{label}_i)_{i=1}^N$
};

\def\names{{"bla","human","car","truck","truck","nothing","nothing","nothing","truck"}}%
\def\namess{{"bla","airplane","car","human","animal","car","human","animal","animal"}}%

\setcounter{moncompteur}{1}
	\coordinate (norb11) at (6.2cm,3.05cm); 
	\coordinate (norb12) at (14cm,3.05cm); 
	\coordinate (norb11lab) at (6.2cm,6.2cm); 
	\coordinate (norb12lab) at (14cm,6.2cm); 
	\coordinate (norb11lab2) at (6.2cm,1.9cm); 
	\coordinate (norb12lab2) at (14cm,1.9cm); 
	\foreach \y in {0.25,0.5,...,2} {
		\coordinate (weight) at (barycentric cs:norb11= \y,norb12=1-\y);
		\def\reptemp{photos/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics[scale=0.5]{\reptemp}};
		
		\coordinate (weight2) at (barycentric cs:norb11lab= \y,norb12lab=1-\y);
		\node[scale=0.7] (leslabels) at (weight2) {\pgfmathparse{\names[\themoncompteur]}\pgfmathresult};
		\coordinate (weight3) at (barycentric cs:norb11lab2= \y,norb12lab2=1-\y);
		\node[scale=0.7] (leslabels) at (weight3) {\pgfmathparse{\namess[\themoncompteur]}\pgfmathresult};

		\addtocounter{moncompteur}{1}
	}

	\coordinate (norb21) at (6.2cm,5cm); 
	\coordinate (norb22) at (14cm,5cm); 
	\foreach \y in {0.25,0.5,...,2} {
		\coordinate (weight) at (barycentric cs:norb21= \y,norb22=1-\y);
		\def\reptemp{photos/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics[scale=0.5]{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

\end{tikzpicture}
\caption[Exemple de base de donnée pour la vision artificielle]{Exemple de base de donnée pour la vision artificielle: 
la base de données d'images étiquetées NORB \cite{LeCun.2004},
destinée à l'apprentissage statistique.
La taille de NORB est supérieure à $3.10^5$,
et les images de cette base de données représentent des objets de ces $5$ classes:
``animal'', ``car'', ``human'', ``nothing'', ``plane'' et ``truck''.
Chaque élément d'une base de donnée d'images est composé d'une image (par exemple une image montrant une voiture)
et une étiquette correspondant à la classe de l'objet représenté par l'image 
(dans l'exemple précédent, l'étiquette est ``car'').
Cette base de données peut être utilisée afin de calculer un classifieur par apprentissage supervisé.
Dans le but de discerner les positions des cibles,
une image est étiquetée avec le nom de l'objet qui est au centre
(``nothing'' si il n'y a rien au centre de l'image).}
\label{NORB}
\end{figure}
%%%%%
%%%%% NORB DATASET END
%%%%%

%
% IMPRECISION OBSERVATION
%
Puisque le classifieur est appris à partir d'une base de données d'images,
son comportement, et donc ses performances,
(\textit{i.e.} sa fréquence de bonne prédictions) 
est inévitablement dépendent de la base de données.
C'est un problème si la variabilité de la base de donnée
est trop faible:
dans ce cas, le comportement probabiliste du classifieur
sera dépendent de ces images en particulier
et le système robotique aura des mauvaises capacités d'observations
lorsque la mission implique des images trop différentes de celles présentes
dans la base de données.

Certaines bases de données à grande variabilité existent
(par exemple NORB, figure \ref{NORB}, bien que la variabilité pourrait être idéalement supérieure):
notons cependant qu'avec ces bases de données,
la performace de vision est réduite,
ou bien, au moins, de bonne performances
sont difficilement atteignables

Une matrice de confusion peut être calculée (\textit{cf.} figure \ref{confusion_matrix})
en utilisant une telle base de données étiquetées,
qui n'est pas utilisée pour l'entraînement,
et qui est appelée \textit{base de données de test}:
la fréquence des observations peut être déduit de cette matrice,
en normalisant les lignes en distributions de probabilité.
Une ligne correspond à un objet de la scène
et les probabilités de cette ligne sont les probabilité d'observation,
\textit{i.e.} chaque valeur de probabilité est la fréquence avec laquelle
le classifieur renvoie le nome de l'objet de la colonne correspondante.
Ces probabilités peuvent être utilisées pour définir la fonction d'observation
$\textbf{p} \paren{o' \sachant s',a}$ introduite au-dessus.
Cette approche soulève le problème de la représentativité
de la base de données pour la mission voulue.
Si la base de donnée de test n'est pas représentative,
ces probabilités d'observation risque de ne pas être fiables,
et le PDMPO mal défini:
cependant, comme montré par l'équation (\ref{probBayesRule})
la mise à jour de l'état de croyance nécessite la connaissance parfaite
de la fonction d'observation.

Finallement, si les bases de données considérée sont étiquetées plus précisément,
(comme NORB, qui inclut des informations telles que la luminosité, ou l'échelle de l'objet),
nous pouvons imaginer que les probabilités d'observation calculées (à partir de la matrice de confusion)
serait plus fiable, ou la performance de vision améliorée
(puisque la séparation demandée au classifieur 
est plus simple avec cette précision).
Cependant, comme plus d'observations ou d'états sont impliquées,
et le POMDP est plus dur à résoudre.

En guise de conclusion, l'utilisation du modèle PDMPO fait l'hypothèse
que les distributions de probabilité régissant le problème doivent être toutes connues parfaitement:
malheureusement ces fréquences ne sont pas connue précisément en pratique.
L'imprécision à propos de ces probabilités,
par exemple l'imprécision associée au comportement des sorties
des algorithmes de vision artificielle,
lorsque les images utilisées sont celles de la mission d'intérêt,
doit être prise en compte pour rendre le robot autonome en toute circonstances.
En général, le calcul des distributions de probabilité d'un PDMPO nécessite
assez de tests pour chaque paire état-action, ce qui est dur à effectuer en pratique.

\begin{figure}[b!] \centering
\begin{tabular}{c|c|c|c|c|c!{\vrule width 2pt}c!{\vrule width 2pt}c}%!{\vrule width 2pt}}
animal & human & plane & truck & car & nothing \\ \specialrule{.2em}{.0em}{.0em}  
$3688$ & $575$ & $256$ & $48$ & $144$ & $149$ &   animal & $75.885\%$ \\ \specialrule{.05em}{.0em}{.0em}  
$97$ & $4180$ & $81$ & $20$ & $225$ & $257$ & human & $86.008\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$292$ & $136$ & $3906$ & $237$ & $202$ & $87$ & plane & $80.370\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$95$ & $1$ & $44$ & $4073$ & $514$ & $133$ & truck & $83.807\%$  \\  \specialrule{.05em}{.0em}{.0em}  
$129$ & $3$ & $130$ & $1283$ & $3283$ & $32$ &  car & $67.551\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$154$ & $283$ & $36$ & $63$ & $61$ & $4263$ & nothing & $87.716\%$  \\ %\specialrule{.2em}{.0em}{.0em}  
\end{tabular}
\caption[Exemple de matrice de confusion illustrant les performances d'un classifieur multi-classes]{
Exemple d'une matrice de confusion pour la classification multi-classe:
cette matrice est calculée avec une base de donnée de tests,
différente de la base de données d'apprentissage.
Chaqye ligne ne considère que les images d'un certain objet,
et les nombres représentent les réponses du classifieur:
par exemple, $3688$ images d'animaux sont bien reconnus, 
mais $575$ sont confondus avec un humain.
La moyenne de réponses correctes pour cet objet et de $80.223\%$.
L'environnement Torch7 \cite{Collobert_NIPSWORKSHOP_2011}
a été utilisé pour obtenir un classifieur, 
et pour calculer cette matrice à partir de ce dernier
et de la base de donnée de test.}
\label{confusion_matrix}
\end{figure}


%
% IMPRECISION POMDP WORKS
%
Quelques variations du cadre PDMPO
a été constuit dans le but de prendre en compte
l'imprécision sur les distributions de probabilité du modèle,
aussi appelé \textit{l'imprécision des paramètres}.
\subsubsection{Travaux Tenant Compte de l'Imprécision des Paramètres} %% TODO
Ici, les fonctions de transition et d'observation,
\textit{i.e.} $\textbf{p} \paren{ s' \sachant s,a }$ 
et $\textbf{p} \paren{ o' \sachant s',a }$,
$\forall (s,s',o',a) \in \mathcal{S}^2 \times \mathcal{O} \times \mathcal{A}$,
sont appelés \textit{paramètres} du PDMPO, 
ou aussi les \textit{paramètres} du modèle. 
A notre connaissance,
le premier modèle construit dans le but de gérer l'imprécision
des paramètres est nommé PDMPOPI, 
pour \textit{PDMPD à paramètres imprécis} \cite{Itoh2007453}. 
Dans ce travail, chacun des paramètres du PDMPO
est remplacé par un ensemble de paramètres possibles.
Dans ce travail, une \textit{croyance du second ordre}
est introduite: elle est définie comme étant
la distribution de probabilité sur les paramètres du modèles.

Un autre travail, appelé \textit{PDMPO à Paramètres Bornés} 
(PDMPOPB) \cite{NiYaLiaZhi},
traite aussi de l'imprécision des paramètres:
dans ce travail, l'imprécision sur chaque paramètre
est décrit à l'aide d'une borde supérieure et inférieure
sur les distributions possibles.
Aucune croyance du second ordre n'est introduite ici.
Cependant, résoudre un PDMPOPB est similaire, dans l'esprit, 
à la résolution des PDMPOPI \cite{Itoh2007453}:
la flexibilité amenée par l'imprécision de paramètres
est utilisée pour rendre les calculs les plus faciles possible,
et le critère utilisé n'est pas explicite.
Le problème majeur de ces approches (PDMPOPI et PDMPOPB)
est que l'imprécision des paramètres n'est pas géré dans un but de robustesse,
comme une approche pessimiste (pire cas),
mais dans un but de simplification. 

Un travail plus récent traite le problème de manière pessimiste 
et est donc appelé \textit{PDMPO robuste} \cite{Osogami15}.
Inspiré par le travail correspondant dans le cas complètement observable
(appelé \textit{PDM uncertain} \cite{NE05}),
ce travail utilise le critère dit \textit{maximin},
ou du \textit{pire cas},
qui vient de la \textit{Théorie des jeux}: 
dans ce cadre, une stratégie optimale
maximise le plus petit critère
parmi ceux induits par chaque paramètre possible.
Si l'imprécision des paramètres n'est pas stationnaire,
\textit{i.e.} peut changer à chaque étape de temps,
la stratégie optimale associée (au sens du maximin)
peut être facilement calculé en utilisant la
\textit{Programmation Dynamique} \cite{bellman54}.
Cependant, lorsque l'imprécision des paramètres est stationnaire,
les choses se compliquent:
les calculs proposés mènent à une approximation de la stratégie optimale,
puisque le critère utilisé est une borne inférieur
du critère désiré.
Pourtant, une hypothèse stationnaire pour l'imprécision
des paramètres semble mieux adaptée lorsque le PDMPO est stationnaire.

Bien que l'utilisation d'ensembles de distributions de probabilité 
rend le modèle plus adapté à la réalité du problème
(les paramètres sont imprécis en pratique),
les prendre en compte augmente violemment 
la complexité du calcul d'une politique optimale
(par exemple, lors de l'utilisation du critère maximin).
Comme expliqué précédemment,
résoudre un PDMPO est déjà une tâche très ardue,
donc l'utilisation d'un cadre menant à des calculs plus complexes
ne semble pas être une approche satisfaisante.
%and optimization may require linear programs 
%As discussed during this thesis 
%(for instance see Section \ref{section_expe_PPUDD} of Chapter \ref{chap_symb}),
En effet, modéliser le problème de manière trop fine
mène à l'utilisation de nombreuse approximations
dans les calculs en pratique,
sans réel contrôle ou estimation de ces approximations
comme dans le cas des PDMPOPI et des PDMPOPB.
Il est peut-être plus judicieux de commencer avec un modèle plus simple
qui peut être résolu plus facilement en pratique.

Un autre problème pratique du modèle PDMPO
peut aussi être mentionné: 
cela concerne la définition de l'état de croyance
durant les premières étapes du processus,
et plus généralement, la manière avec laquelle
la connaissance de l'agent est représentée.
%%
%%  FULL IGNORANCE/ KNOWLEDGE OF THE AGENT
%%
\subsection*{Modéliser l'Ignorance de l'Agent}
The initial belief state $b_0$, 
or \textit{prior} probability distribution 
over the system states, 
takes part in the definition of the POMDP problem.
Given a system state $s \in \mathcal{S}$, 
$b_0(s)$ is the frequency 
of the event ``the initial state is $s$''. 
This quantity may be hard to properly compute,
especially when the amount of available past experiments 
is limited: this reason has been already invoked above,
leading to the imprecision of the transition and observation 
functions.

As an example, consider a robot that is for the first time in a room
with an unknown exit location (initial belief state) 
and has to find the exit and 
reach it. 
In practice, no experience can be repeated 
in order to extract a frequency of the location of
the exit. 
In this kind of situation,
uncertainty is not due to a random fact, 
but to a lack of knowledge: 
no frequentist initial belief state can be used to define the model.

In other cases, 
the agent may strongly believe 
that the exit is located 
in a wall as in the vast majority of rooms, 
but it still grants a very small probability $p_{\epsilon}$ 
to the fact that the exit may be a staircase 
in the middle of the room. 
Even if this is very unlikely to be the case,
this second option must be taken into account in the belief state, 
otherwise Bayes rule (see Equation \ref{probBayesRule}) 
cannot correctly update it 
if the exit is actually in the middle of the room. 
Eliciting $p_{\epsilon}$ without past experience 
is not obvious at all 
and does not rely on any rational reasons, 
yet it dramatically impacts the agent's strategy. 

The initial system state 
may be deliberately stated as unknown by the agent 
with absolutely no probabilistic information:
consider robotic missions 
for which a part of the system state,
describing something that the robot
is supposed to infer by itself, 
is initially fully unknown.
In a robotic exploration context, 
the location or the nature of a target, 
or even the initial location of the robot
may be defined as absent from the knowledge of the agent.
Classical approaches initialize the belief state as 
a uniform probability distribution
(\textit{e.g.} over all robot/target possible locations, 
or over possible target natures), 
but it is a subjectivist answer \cite{de1974theory,Dubois96representingpartial}.
Indeed, all probabilities are the same
because no event is more plausible than another:
it corresponds to equal betting rates.
However following belief updates (see Equation \ref{probBayesRule}) 
will eventually mix up frequentist probability distributions,
given by transition and observation functions, 
with this initial belief which is a subjective probability distribution:
it does not always make sense
and it is questionable in many cases. 
Thus, the use of POMDPs in these contexts, 
faces the difficulty to encode agent ignorance.
 
Since the knowledge of the agent 
about given features of the system 
may be initially partial (or even absent),
it makes sense to pay attention to the evolution of 
this knowledge during the execution of the considered process.
Some works inspired by the research in \textit{Active Perception}, 
have been focusing on the information gathered
by the agent in the POMDP framework
\cite{ChanelFTI10,oatao11449}.
In these works, 
the \textit{entropy} of the belief state
is taken into account in the criterion, 
ensuring the computed strategy 
to make the process 
reach belief states that have a lower entropy
(\textit{i.e.} are closer to a deterministic probability distribution)
while always satisfying the requirement of an high expected total reward.
This approach leads to really interesting results
in practice.

Nevertheless, a tradeoff parameter is introduced
making the entropy part 
of the criterion more or less important: 
tuning the latter may
add additional computations.
Moreover this approach
does not distinguish
between the actual frequentist behavior of the system state, 
and the lack of knowledge about the actual belief state
(as a probability distribution)
due to the imprecision of the initial belief state,
or even of the transition and observation function:
it just tends to make the belief as deterministic as
possible, even if it is not possible due to the actual
probability distributions of the model 
(e.g. if the transition probability distribution 
$\textbf{p} \paren{s_{t+1} \sachant s_t,a_t}$ 
has an high entropy for each action $a_t \in \mathcal{A}$),
and even if it is not needed 
(e.g. if each system state $s \in \mathcal{S}$ such that the belief state $b_t$
is not zero, $b_t(s)>0$, leads to an high reward).
%In other words, the POMDP framework 
%defines the successive belief states
%as probability distributions
%instead of sets of possible ones:
%measure of knowledge about the actual system state 
%(in a frequentist point of view)
%is mixed up with the measure of knowledge about the model.

However, it has been shown that 
the proposed criterion 
(including the entropy of the belief state) 
makes the agent estimate faster its real state,
which is really useful in some practical problems.
Other models have been introduced, making the reward function dependent
on the belief state 
\cite{Araya-LopezBTC10,oatao11437}:
this enables to make the agent behavior vary with respect to
the probabilistic representation of its knowledge.

%it may make the  actually the real  the POMDP is in practice, a belief state may have an high entropy
%not because of the imprecision of the parameters, 
%may be not related to the agent's lack of knowledge in the probabilistic about the ,
%but rather to the 
%(fully known) 
%variance of the system state:
%hence the agent does not know 
%which is the actual system state 
%since its  is far from being deterministic,
%but it may perfectly know how the state behaves,
%and then nothing can be improved concerning its knowledge.

%and knowledge of the agent CARO \\

%
% WHAT WE WOULD LIKE TO DO
%
\section*{Problème Général}
Previous sections presented some issues encountered in practice 
when using the POMDP framework to compute strategies, 
especially in the robotic context. 
The really high complexity of the problem 
of computing an optimal strategy 
is a first issue: 
robotic missions often result in high dimensional problems,
preventing any algorithm 
to computed a
sufficiently near optimal strategy,
because of the prohibitive computation time 
or memory needed for this task.
Second we highlighted 
the difficulty of defining 
the probability distributions
describing the problem:
for instance the observation function 
can be hard to define
when the observations come from 
complex computer vision algorithms.
Finally the problem of managing 
the knowledge 
and the ignorance
of the agent 
has been discussed:
there is no clear answer
concerning the way to
represent the initial 
lack of knowledge 
about the actual world
for the agent.
Also, how to handle 
and take into account 
the current agent knowledge
is still open to question: 
the difficulty comes 
from the classical POMDP definition
which only allows 
the use of frequentist probability distributions,
while more expressive mathematical tools seem necessary.

These problems are the starting points of our work.
Indeed, the latter consists in contributing 
to the problem
of computing appropriate strategies 
for partially observable domains.
The computed strategies 
have to make the robot 
fulfill the mission
as well as possible,
from the very first execution,
\textit{i.e.} strategy computations are performed 
before any real execution of the mission.
In order to make the robot more powerful
in the long term, 
RL algorithms may improve
the prior strategies 
that we propose 
to compute in this work,
taking into account each past execution 
of the robotic mission.
However this idea 
is not considered 
here since we focus 
on fulfilling the robotic mission 
as well as possible, 
especially for 
the first executions 
of the process:
RL algorithms are useless
in this context
since the dataset of recorded missions 
is not sufficiently large
during the first mission executions.
%and thus without the knowledge 
%to efficiently use RL.
%to this purpose, a criterion
%(e.g the discounted expected total reward of Equation \ref{criterion})
%is maximized.

The general challenge guiding this work 
is to proceed with strategy computations 
only using data and knowledge %about the problem 
really available in practice,
possibly making the strategy computation easier,
instead of using the highly complex 
and hard to define POMDP framework.
In other words, it consists in paying 
particular attention 
to the issues pointed out above:
namely, the complexity 
of the strategy computation,
the imprecision of the model, 
and the management 
of the knowledge of the agent.


\section*{Une Théorie Qualitative}
Finally, in the qualitative framework, 
the possibility values 
assigned to each of the elementary events 
are not really taken into account
in terms of real numbers
since only qualitative comparisons 
are performed via the use of 
$\max$ and $\min$ operators.
Hence, Qualitative Possibility Theory
is classically defined 
with the introduction of
a qualitative scale $\mathcal{L}$,
which may be defined as $\set{l_1,l_2,\ldots,0 }$,
or as any other totally ordered set indifferently.
As they are qualitative data, 
we use the term \textit{possibility degrees} 
instead of possibility values.
The next section clarifies why 
the use of a qualitative framework
is beneficial in terms of complexity
and modeling to our study.

Note the similarities between 
Possibility and Probability Theories, 
replacing $\max$ by $+$, and $\min$ by $\times$
(in the qualitative case). 
Note also that Qualitative 
Possibility Theory defines
the semiring $(\mathcal{L},\max,\min)$,
and Quantitative Possibility Theory
leads to the semiring $([0,1]\max,\times)$.
These algebraic structures are
close to the well known 
tropical semirings \cite{pinhal-00113779},
as the \textit{max-plus} semiring 
$(\mathbb{R} \cup \set{-\infty} \max,+)$ 
and the \textit{min-plus} one 
$(\mathbb{R} \cup \set{+\infty}, \min,+)$.

%%% PIPOMDP
\subsection*{PDMPO Qualitatifs Possibilistes}
A qualitative possibilistic counterpart 
of the POMDP framework
has been proposed in \cite{Sabbadin1999pipomdp}:
this model is called Qualitative Possibilistic POMDP 
and denoted by $\pi$-POMDP. 
A $\pi$-POMDP is simply a POMDP
with qualitative possibility distributions as parameters,
instead of probability distributions.
As the $\pi$-POMDP framework is qualitative,
the counterpart of the reward function,
called \textit{preference function},
is a qualitative function: 
indeed the preference function returns values 
from the finite qualitative scale $\mathcal{L}$,
and thus is non-additive.

%%IMPRECISION CRITERES
As a particular case of BFT
and \textit{a fortiori} of IPT,
Possibility Theory expresses 
a partial knowledge
of the actual probability distribution,
as presented above.
A qualitative possibility distribution
is even more imprecise
since only qualitative information
is given by such a distribution.
This imprecision results in the measures
presented above: possibility and necessity measures. 
As detailed in Section \ref{subsection_qualcrit},
two qualitative criteria have been proposed 
in \cite{SabbadinFL98},
counterparts of the probabilistic criterion (\ref{criterion}): 
a pessimistic one, which is a
qualitative counterpart of the maximin 
(worst-case) criterion in IPT.
The other criterion is qualified as optimistic, 
as a qualitative counterpart of 
the best case criterion in IPT.

%%%% COMPLEXITY 
One of the most interesting property
of $\pi$-POMDPs is
the simplification of the strategy computation.
Indeed, algorithms proposed 
for solving classical (probabilistic) POMDPs
are often based on the set of the belief states
called \textit{belief space}.
The belief space is infinite in the general case: 
each time step leads eventually 
to a finite number of new belief states,
which makes this set countable. 
In order to get useful properties
and means
for the strategy computation,
the set of all probability distributions
over the system space $\mathcal{S}$ is considered, 
\textit{i.e.} the continuous simplex
$\mathbb{P}^{\mathcal{S}} 
= \set{ \textbf{p}:\mathcal{S} \rightarrow [0,1] \sachant \sum_{s \in \mathcal{S}} \textbf{p}(s) 
= 1, \mbox{ and } \textbf{p}(s)\geqslant 0, \forall s \in \mathcal{S} }$.
The infinite size of the belief space 
partly explains why probabilistic POMDPs
are very hard to resolve.
On the contrary, $\pi$-POMDPs have a finite belief space.
Indeed, the number of qualitative possibility distributions
over the system space $\mathcal{S}$
is lower than $\mathcal{L}^{\# \mathcal{L}}$,
as the qualitative scale $\mathcal{L}$ is finite.
The fully observable version 
of the $\pi$-POMDP model is called $\pi$-MDP:
as explained in Section \ref{section_piPOMDP} of Chapter \ref{chap_SOTA}, 
any $\pi$-POMDP reduces to a $\pi$-MDP whose system space is
the qualitative possibilistic belief space, and
whose size is exponential in the number of states.
In works \cite{abs-1202-3718,Garcia20081018,Sabbadin1999pipomdp}, 
complexity of $\pi$-MDP 
appears to be lower than complexity of MDP, 
which is polynomial \cite{Papadimitriou1987}:
complexity of $\pi$-POMDP is then
at worst exponential in the problem description,
whereas POMDP solving may be undecidable \cite{Madani1999UPP315149.315395}.
%The approach of using a $\pi$-POMDP 
%instead of a probabilistic POMDP
%in order to simplify the resolution, 
%can be compared to 


%%% IMPRECISION MODELISATION
In addition to simplifying the computations,
the $\pi$-POMDP framework may be very interesting 
to our use case.
Indeed, when we considered 
a robot using computer vision algorithms
to get observations
(see Figure \ref{observation_robot}),
we previously highlighted the difficulty
of properly defining
the probabilistic observation function:
probability values of the answers
of the vision algorithms
in the context of the robotic mission
are imprecisely known 
and hard to define in practice.
Finding qualitative estimates 
of their recognition performance is easier: 
the $\pi$-POMDP model 
only requires qualitative data, 
thus it allows to build the model 
without the use of information 
other than the one really available. 
For instance, 
the confusion matrix 
in Figure \ref{confusion_matrix}
may lead to a qualitative possibilistic 
observation function 
which only takes into account
how answer frequencies are sorted:
in the presence of a human (see second line),
the most frequent answer is ``human'',
the second one is ``nothing'',
the third one is ``car'', etc.
Thus, the corresponding possibility distribution
is such that, 
conditioned on the presence of a human,
the possibility degree 
of the answer ``human''
is higher than the possibility degree 
of the answer ``nothing'',
which is higher than the possibility degree 
of the answer ``car'', etc.
Instead of assigning frequencies 
which are not really reliable in practice, 
the qualitative possibilistic model
naturally expresses imprecisions
about the problem.

%%%IGNORANCE
Finally, let us recall that 
the constant possibility distribution
whose possibility degrees are all equal to $1$ 
(maximal element of $\mathcal{L}$),
represents total ignorance: 
this distribution can be used 
to define the initial belief state
when it has to represent 
an agent which initially ignores
a given situation.
Thus, the $\pi$-POMDP framework 
allows for a formal modeling 
of the agent's lack of knowledge.

%%CORRESPONDS TO OUR ISSUES 
The use of the Qualitative Possibility Theory 
\cite{DuboisPS01}
is thus studied in this work,
as it appears to be able
to both simplify a POMDP, 
and to model parameter imprecision 
and ignorance
related to robotic missions.
This framework indeed simplifies computations,
is able to encode the problem with only available data
and formally models the lack of knowledge:
thus this theory offers solutions to
the three issues highlighted previously.
However we note that, as a qualitative framework, 
it does not allow for 
frequentist information encoding.

%%%NOT WIDELY STUDIED
To the best of our knowledge,
a rather limited study of the $\pi$-POMDP model
exists in the literature up to now:
in fact, the work \cite{Sabbadin1999pipomdp}
seems to be the only one proposing 
both a definition of $\pi$-POMDPs
and a toy example to illustrate this model.
The fully observable version ($\pi$-MDP)
has generated some more interest
in the research community
\cite{Sabbadin2001287,Sabbadi00,LIP61498}.

\section*{Description de notre étude}
% Le sujet de la thèse
This thesis contributes to determine
to which extend Qualitative Possibility Theory 
can contribute to \textit{planning under uncertainty
in partially observable domains}, 
and more generally to 
\textit{sequential uncertainty management}, 
in terms of computation simplification 
and modeling. 
It presents recent contributions 
in the use of this theory
for planning under uncertainty 
and knowledge representation,
with an almost systematic use of 
graphical models 
\cite{Koller2009PGM1795555,Be2002.7,Borgelt02graphicalmodels}.

The end of this introduction describes how this thesis is structured.
Indeed, each of the following sections 
corresponds to a chapter of our work
and details its contents.

%%%% SOTA
\subsection*{\'Etat de l'Art}
As probabilistic POMDPs and qualitative possibilistic ones 
are the central objects of this thesis,
the \emph{first chapter} presents these models
starting from low level definitions.
Most of the classical results presented 
are accompanied by proofs
making this thesis self-contained:
this work is thus accessible 
to all researchers 
and students
with basic mathematical competences, 
even if they are not familiar 
with alternative uncertainty theories,
nor member of the \textit{planning under uncertainty} community.

The first part of this chapter focuses on
the classical (probabilistic) POMDP:
it begins with the presentation of the less complex 
fully observable case, called MDP.
The well-know POMDP is then set up, 
giving some formal results and proofs 
that are sometimes hard to find in the literature. 
This part ends with a review 
about some of the most
successful POMDP algorithms
including the ones used in the next chapters. 

The second part is naturally devoted to
the qualitative possibilistic counterpart of the POMDP.
It begins with a presentation of the Possibility Theory,
with a specific focus on the qualitative version.
It includes the qualitative counterparts of conditioning,
and averaging.
Finally, as all the needed theoretical tools have been presented,
the fully observable model ($\pi$-MDP) can be defined,
followed by the partially observable one ($\pi$-POMDP).
As noted above, to the best of our knowledge,
only one ten-paged paper has already dealt with
the $\pi$-POMDPs: 
the second part of this chapter is thus the first work 
formally detailing the construction of this model.

Note that the notations and the terminology 
set up in this initial chapter,
and thus used throughout this thesis,
are those classically used in the POMDP framework:
it should make our work more readable to the POMDP community 
or more generally to the probabilistic community.
Moreover, dedicating 
this first chapter 
to a formal presentation of 
the probabilistic and the qualitative possibilistic models
is meant to highlight their common structure.

%%% CHAP1
\subsection*{Mises \`a jour naturelles du modèle possibiliste qualitatif}
The \emph{second chapter} proposes several extensions 
of the work \cite{Sabbadin1999pipomdp}.
It begins with the presentation of some variants 
of the qualitative criteria presented in the first chapter:
the latter are related to the aggregations of the preferences over time,
and to the chosen approach \textit{i.e.} pessimistic or optimistic.

A qualitative possibilistic version of the
Mixed-Observable MDPs 
\cite{OngShaoHsuWee-IJRR10,AraThoBufCha-ICTAI10},
where some state variables are fully observable, 
is then built.
It is named $\pi$-MOMDP and generalizes both $\pi$-MDP and $\pi$-POMDP.
This contribution reduces dramatically 
the complexity of solving $\pi$-POMDPs, 
while catching finer information about the environment
some state variables of which are fully observable. 
For instance, the battery level of a robot
may be considered as directly observable information
for decision making, leading to easier computations.
More generally, the existence of visible variables 
is common in robotics \cite{OngShaoHsuWee-IJRR10}.

Next, a qualitative criterion
for missions with unbounded durations
is proposed, along with
an algorithm for computing the associated
optimal strategy.
This algorithm is used to
compute a strategy 
for a target recognition mission:
experimental results compare 
executions using this strategy to
those using the strategy from a probabilistic algorithm, 
in situations where the probabilistic dynamic of the observations 
is actually not properly defined.

Note that this experiment is the first practical use
of the $\pi$-POMDP framework
to the best of our knowledge.
It also highlights interesting behaviors
of the qualitative possibilistic belief state:
they are then clarified theoretically 
in the end of the chapter.

The main contributions of this chapter 
have been published in \cite{Drougard13}.
The experiments illustrate that 
these contributions are necessary in practice.
However they are not sufficient
to reach a competitive computation time
or to deal with realistic robotic problems:
the orientation of the next chapter
stems from this observation.

%%% CHAP2
\subsection*{Modèles factorisés et algorithmes symboliques}

%% INTRO
The size of the robotic problem dealt with 
in the previous chapter
is small enough to allow the proposed algorithm 
to compute a strategy within a reasonable amount of time.
The contributions of the \emph{third chapter}
are meant to make computations possible 
for larger structured planning problems. 

%% PPUDD and factored models
The first part of this chapter 
proposes a definition 
for the \textit{factored $\pi$-MOMDPs}:
additional independence assumptions 
are provided to these processes --
in a qualitative possibilistic sense,
as defined in the first chapter.
Large planning problems satisfying these assumptions 
can be solved more easily:
building upon the probabilistic 
SPUDD algorithm \cite{Hoey99spuddstochastic},
we conceived a possibilistic algorithm named PPUDD 
for solving factorized $\pi$-MOMDPs
using \textit{Algebraic Decision Diagrams} (ADD).
The guess motivating this contribution 
is that computations between ADDs 
is less time and memory consuming
when performed in the possibilistic framework 
than in the probabilistic one:
qualitative operations
should lead to smaller ADDs
when sum and product
are replaced with $\min$ and $\max$ operators
because the formers
produce ADDs with potentially
more leaves.

%% belief factorization
Independence assumptions defining 
a factored $\pi$-MOMDP 
concern the variables encoding 
successive belief states.
Moreover variables defining a factored $\pi$-MOMDP
are those representing 
successive system states and observations.
That is why, the following section
of this chapter exhibits sufficient conditions 
on the system state and observation variables
leading to the desired independence 
between belief state variables.
A robotic example is used as an illustration
of these conditions.
As proofs use the graphical concept 
called \textit{$d$-Separation} \cite{pearl88},
these conditions also lead to the
belief variable independence 
in the probabilistic MOMDP framework.
 
%% first tests, larger robotic missions  
Performances of our solver PPUDD 
are next compared to
those of its probabilistic counterparts, 
in terms of computation time, 
and with quality criteria measuring 
some mission achievement metrics. 
Finally, the last part of this chapter
describes the results of PPUDD 
at the 2014 International Probabilistic 
Planning Competition\footnote{\url{https://cs.uwaterloo.ca/~mgrzes/IPPC_2014/}}.
We participated in the competition
in order to test the performances
of our algorithm
against probabilistic ones,
in terms of expected reward gathering,
with various
probabilistic planning problems. 

Some contributions of this chapter 
have been published in 
\cite{DrougardTFD14}.
The various planning problems
of the competition also highlight 
some issues of our algorithm
when used to find an approximate strategy
for a probabilistic problem
in order to
benefit from qualitative computations
which are simpler.
Although modeling points are part of these issues,
the next chapter shows that 
a possibilistic approach
is very useful 
where only qualitative
data are available.
The approach proposed in the final chapter 
takes into account 
the highlighted modeling issues.
Moreover, time can be entirely consumed 
before actual computations of PPUDD begin,
when loading ADDs encoding some 
high dimensional planning problems:
memory issues are also observed in practice.
MDP algorithms using state space search
\cite{KellerE12,KolobovMW12} 
do not face these issues due to ADD-encoding of MDP instances.
%%%%%%%%% 
They can be used to solve
the problem resulting from 
the last chapter.
The latter focuses on 
the belief state management,
and proposes a hybrid POMDP
technically resulting in a
classical probabilistic MDP.

%%% CHAP3
\subsection*{Un processus qualitatif possibiliste pour la modélisation de l'intéraction homme-machine}
%%% why? qualitative info only
The \emph{fourth chapter} deals with
situations where probability distributions
are clearly not available:
the managed problem comes from the work
\cite{SERGIOTHESIS} and is a joint work with the author,
Sergio Pizziol, working in the field of 
\textit{Human-Machine Interactions} (HMI).
In systems representing human behavior 
in certain situations,
for instance interacting with a control panel
of an aerial vehicle,
a sufficient amount of statistical data is lacking,
especially concerning the human operator's state of mind:
only expert knowledge can be used in practice
as no frequentist information about the problem
is available.

%% like with previous processes
In this chapter, used processes 
are similar to those
studied in the previous chapters:
the only difference is that actions 
are not chosen anymore, but used as observations
to infer the state of the human-machine system.
This process is called \textit{qualitative possibilistic Hidden Markov Process}
($\pi$-HMP).

This process is used as a tool
to produce diagnosis for
HMI systems: machine state 
and human actions are called \textit{occurrences},
and the possible transitions of the system
are called \textit{effects}.
After the model construction 
describing the problem in terms
of occurrences and effects
from expert qualitative data
and the machine model,
the human assessment of the situation can be estimated.
The proposed model can also detect
human assessment errors, 
and produce a diagnosis for them.

%%% CHAP4
\subsection*{Approche probabiliste et possibiliste: une perspective hybride}
The last chapter of this thesis
persists to show the possible 
improvement of the POMDP framework
using the qualitative possibility theory,
especially for the belief state management.
It also takes into account the issues
highlighted in the third chapter.

Indeed, the \emph{fifth chapter} 
argues for a hydrid POMDP 
with both probabilistic 
and possibilistic settings
coherently mixed up. 
A new translation from POMDPs
into Fully Observable MDPs is described here. 
Unlike the classical translation
presented in the first chapter, 
the resulting problem state space is finite, 
making probabilistic MDP solvers able to solve
this simplified version 
of the initial partially observable problem.
Indeed, this approach encodes agent beliefs 
with possibility distributions over states,
leading to an MDP whose state space 
is a finite set of epistemic states.

Additional simplifications of the computations
are described for \textit{factored POMDPs} \cite{Sim2008SHS1620163.1620241,Williams05factoredpartially,
Veiga14aaai,abs-1301-6719}, \textit{i.e.}
POMDPs with a particular independence 
structure.
These last contributions have been published in
\cite{DrougardDFT15}.
