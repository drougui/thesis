Dans ce chapitre, nous proposons
l'étude des modèles $\pi$-PDMOM 
dans le but de résoudre 
de très grands problèmes de planification
lorsqu'ils sont structurés.
Inspirés par l'algorithme \textit{Stochastic Planning Using Decision Diagrams} (SPUDD)
construit pour résoudre les PDM probabilistes factorisés, 
nous avons mis en place un algorithme symbolique
appelé PPUDD conçu pour résoudre les $\pi$-PDMOM. 
Tandis que le nombre de feuilles
des arbres de décisions utilisés par SPUDD
peuvent devenir aussi grand que la taille de l'espace des états,
puisque leurs valeurs sont des nombres réels 
agrégés par des additions et des multiplications,
le nombre de feuilles de PPUDD 
est borné par le nombre d'éléments dans $\mathcal{L}$
car leurs valeurs restent dans l'échelle finie $\mathcal{L}$
via les opérations $\min$ et $\max$ seulement.
Enfin, nous présentons un $\pi$-PDMOM
satisfaisant certaines hypothèses d'indépendance 
sur ces variables visibles, cachées,
et d'observation.
Ce dernier résulte en un $\pi$-PDM factorisé,
sur lequel PPUDD peut être lancé.
Nos résultats expérimentaux montrent que 
le temps de calcul de PPUDD 
est beaucoup plus petit que 
Symbolic-HSVI et APPL
pour les versions possibilistes 
et probabilistes des mêmes benchmarks, 
tout en fournissant des stratégies de bonne qualité.
Les performances des stratégies
calculées par PPUDD ont été testées
pendant la compétition internationale de planification probabiliste (IPPC 2014)
dont les résultats sont exposés ici.

\section{Introduction}
Les travaux sur les $\pi$-PDM(MO)
présentés précédemment ne tirent pas totalement avantage
de la structure du problème, \textit{i.e.}
les parties visibles ou cachées de l'état peuvent être elles-même
factorisées en plusieurs variables d'états. 
Dans le cadre probabiliste, 
les PDM factorisés et les méthodes de calcul symboliques 
\cite{BoutilierDG00,Hoey99spuddstochastic} ont été étudiés
intensivement dans le but de raisonner directement au niveau des variables
plutôt que sur l'espace d'état.
Un travail récent sur ces problématiques est par exemple \cite{RadoszyckiPS14}. 
Le célèbre algorithme SPUDD
\cite{Hoey99spudd:stochastic} 
résout des PDM factorisés en utilisant
des représentation symboliques de la fonction valeur et des stratégies
sous la forme d'arbre de décision algébriques (ADDs) \cite{Bahar1997ADD}, 
qui représentent de manière compacte
les fonctions réelles de variables booléennes:
les ADDs sont des arbres 
dont les noeuds représentent les variables d'état
et les feuilles sont les valeurs de la fonction. 
Au lieu de mettre à jour les valeurs de la fonction pour chaque état
individuellement à chaque itération de l'algorithme,
ils sont aggrégés dans les ADDs
et les opérations sont effectués de manière symbolique
directement entre les ADDs sur plusieurs variables en même temps.
Cependant, SPUDD est limité par la potentielle manipulation
d'énormes ADDs dans le pire des cas: par exemple,
l'espérance implique des additions et des multiplications sur des valeurs réelles
(probabilités et récompenses), 
créant de nouvelles valeurs entres elles,
de manière à ce que le nombre de feuille des ADDs
peut devenir égal à l'espace d'état,
\textit{i.e.} exponentiel en le nombre de variables d'état.

Ainsi,
le travail présenté ici
est motivé par la simple observation
que les \textbf{opérations symboliques avec des PDM possibilistes
devraient nécessairement limiter la taille des ADDs}: 
en effet, ce formalisme opère sur une échelle \emph{finie}
$\mathcal{L}$ avec seulement les opération $\max$ et $\min$,
ce qui implique que les valeurs manipulée restent dans
l'échelle finie $\mathcal{L}$, 
qui est généralement beaucoup plus petite
que le nombre d'états.
\begin{figure} \centering
\begin{tikzpicture}
\begin{axis}[grid=major,xmax=100,
legend entries={ $8$ variables: cadre qualitatif, cadre quantitatif,  
$10$ variables: cadre qualitatif, cadre quantitatif},legend style={at={(2,0.6)}},
xlabel={Taille de l'échelle $\mathcal{L}$},
ylabel={Nombre maximal de noeuds},
title={Nombre maximal de noeud d'un ADD: feuilles dans $\mathcal{L}$ vs dans $\mathbb{R}$}]
\addplot table[x=scaleSize, y=MaxNbNodesPoss]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_8VARS.txt};
\addplot table[x=scaleSize, y=MaxNbNodes]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_8VARS.txt};
\addplot table[x=scaleSize,y=MaxNbNodesPoss]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_10VARS.txt};
\addplot table[x=scaleSize,y=MaxNbNodes]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_10VARS.txt};
\end{axis}
\end{tikzpicture}
\caption[Limitations de la taille maximale d'un ADD dans le cadre qualitatif]{
La taille maximale (nombre total de noeuds)
d'un ADD dont les valeurs sont dans $\mathcal{L}$ is limited:
cette taille maximale est représentée par les courbes avec les ronds bleus et marrons,
en fonction de la taille de $\mathcal{L}$.
Lorsque les feuilles de ADDs sont dans $\mathbb{R}$,
le nombre de ses noeuds est potentiellement exponentiel
en le nombre de variables:
la borne supérieure est représentée par les courbes
avec les carrés rouges et noirs.
(fonctions constantes de la taille de $\mathcal{L}$).}
\label{ADDsize}
\end{figure}

La figure \ref{ADDsize} montre que
les ADDs utilisés dans le cadre possibiliste
a un nombre limité de noeuds
puisque le nombre de feuilled
est au plus égal au cardinal de
l'échelle possibiliste qualitative $\mathcal{L}$:
%which is generally far smaller than the number of states.  
%Figure \ref{ADDsize} shows 
la taille maximale (
nombre maximal de noeud)
d'un ADD
dont les feuilles sont dans $\mathcal{L}$, 
est représenté comme une fonction
de $\# \mathcal{L}$,
dans le cas de $8$ et $10$ variables. 

Dans ce chapitre, nous présentons
un algorithme basé sur la programmation dynamique symbolique
pour résoudre les $\pi$-PDMMO factorisés
appelé Possibilistic Planning Using
Decision Diagram (PPUDD). 
Cette contribution seule
n'est pas suffisante
puisque les variables de croyance
ont un nombre de valeurs
exponentiel en la taille de l'espace des états cachés.
Donc, notre seconde contribution est un théorème 
pour factoriser l'état de croyance
en de nombreuses variables de croyance marginales
lorsque certaines hypothèses d'indépendance 
sur les variables d'état et d'observation
d'un $\pi$-PDMOM sont vérifiées:
cela permet de résoudre certains problèmes
dont les calculs sont inabordable tels quels. 
Notons que notre idée de factorisation
de l'état de croyance est assez général
pour être valable pour les modèles probabilistes.
Enfin, les performances de PPUDD sont comparées à celles de symbolic
HSVI \cite{Sim2008SHS1620163.1620241}
(une version symbolique de l'algorithme pour PDMPO called HSVI) 
et APPL \cite{Kurniawati-RSS08,OngShaoHsuWee-IJRR10}
(déjà utilisé dans le chapitre précédent, 
et basé sur SARSOP)
sous observabilité mixte.
Les résultats obtenus étant prometteurs,
nous avons participé à la compétition internationale de planification probabiliste (IPPC 2014)
et les résultats de PPUDD durant la session entièrement observable d'IPPC 2014
sont présentés et discutés.
Un algorithme dédié à la résolution des $\pi$-PDMOM en utilisant des ADDs
est disponible dans le dépôt \url{https://github.com/drougui/ppudd}).

\section{Résoudre des $\pi$-PDMOM en utilisant la programmation dynamique symbolique} 
\label{section_PPUDD}
Les PDM factorisés \cite{Hoey99spuddstochastic} 
ont été utilisés pour résoudre plus rapidement les
problèmes structurés de décision séquentielle, 
sous incertitude probabiliste, 
en raisonnant symboliquement sur les fonctions
des états du systèmes, à travers des arbres de décision algébriques. 
Inspiré par ce travail,
cette section présente la résolution symbolique
des $\pi$-PDMOM factorisés:
dans ce modèle, l'espace des états visibles $\mathcal{S}_v$, 
l'espace des états cachés $\mathcal{S}_h$
et l'ensemble des observations $\mathcal{O}_h$
sont tels que l'espace d'état du $\pi$-PDM résultant 
(basé sur l'espace des états de croyances et l'espace des variables visibles)
est sous la forme 
$\mathcal{S}^1_v \times \cdots \times \mathcal{S}^m_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$, 
où chacun de ces espaces sont finis.
Nous verrons dans la section suivante comment $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
peut être factorisé de cette manière
grâce à la factorisation de $\mathcal{S}_h$
et $\mathcal{O}_h$. 
La factorisation de la variable de la croyance probabiliste
dans \cite{DBLP:conf/aaai/BoyenK99,DBLP:conf/aips/ShaniPBS08} 
est approximative, tandis que celle présentée ici est exacte.
Puisque les espaces finis de taille $K$
peuvent être eux-même factorisés
en $\lceil \log_2 K \rceil$ espaces binaires \cite{Hoey99spudd:stochastic}, 
nous poubons faire l'hypothèse que nous raisonnons 
sur un $\pi$-PDM dont l'espace d'état est noté $\mathcal{X}$
est entièrement décrit par les variables $(X^1,\ldots,X^n)$, 
avec $n \in \mathbb{N}^{\ast}$ et $\forall i$, $X^i \in \set{\top,\bot}$:
$\mathcal{X} = \set{\top,\bot}^n$.

Rappelons que les réseaux bayésiens dynamiques (DBNs) \cite{Dean:1989:DBN}
déjà utilisés en section \ref{section_Markov2POMDP} 
(par exemple dans le diagramme d'influence figure \ref{POMDP})
et dans le chapitre précédent (figure \ref{piMOMDP} 
illustrant la structure d'observabilité mixte)
sont des représentations graphique très utiles pour les processus étudiés. 
Un DBN représentant la structure d'un $\pi$-PDM factorisé
est dessiné dans la figure \ref{fig_piMDPFact}:
les variables d'état à une étape de temps donnée $t \geqslant 0$
sont notées $X_t = (X_t^i)_{i=1}^n$ (variables courantes),
et $(X^i_{t+1})_{i=1}^n$ sont les variables d'état à l'étape de temps $t+1$ (variable suivante).
\begin{figure}\centering
\begin{tikzpicture}[scale=1.5,transform shape]
%% vertex shape and color
\tikzstyle{vertex}=[circle,fill=black!30,minimum size=27pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red!60,minimum size=22pt,inner sep=0pt,draw=black,thick]
\tikzstyle{rvertex}=[fill=yellow!60,decision=3,inner sep=-1pt,minimum size=35pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=35pt,inner sep=0pt,draw=black,thick]
%TIME
%\node [font=\huge] (statet) at (4,0.2) {$t$};
%\node [font=\huge] (statetplus1) at (9.2,0.2) {$t+1$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1
\node[vertex] (state1) at (3.3,3) {$X^1_t$};
\node[vertex] (state12) at (3.3,1) {$X^2_t$};
\node (state13) at (3.3,0.2) {\begin{Large} $\vdots$ \end{Large}};
%2
\node[vertex] (state2) at (8,3) {$X^1_{t+1}$};
\node[vertex] (state22) at (8,1) {$X^2_{t+1}$};
\node (state23) at (8,0.2) {\begin{Large} $\vdots$ \end{Large}};

%0
\node (state0) at (0.5,3) {};
\node (state02) at (0.5,1) {};
%3
\node (state3) at (11,3) {};
\node (state32) at (11,1) {};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%action
\node[avertex] (action) at (5.7,-1) {$a_t$};
\node[avertex] (action0) at (1.3,-1) {$a_{t-1}$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS

%SV
% arrows sv->sv
%1->2
\draw[->,>=latex] (state1) -- (state2);
\draw[->,>=latex] (state1) -- (state22);
\draw[->,>=latex] (state12) -- (state2);
\draw[->,>=latex] (state12) -- (state22);
%0->1
\draw[->,>=latex,dashed] (state0) -- (state1);
\draw[->,>=latex,dashed] (state0) -- (state12);
\draw[->,>=latex,dashed] (state02) -- (state1);
\draw[->,>=latex,dashed] (state02) -- (state12);
%2->3
\draw[->,>=latex,dashed] (state2) -- (state3);
\draw[->,>=latex,dashed] (state2) -- (state32);
\draw[->,>=latex,dashed] (state22) -- (state3);
\draw[->,>=latex,dashed] (state22) -- (state32);

%A
% a->s
%1
\draw[->,>=latex] (action) -- (state2);
\draw[->,>=latex] (action) -- (state22);
%0
\draw[->,>=latex,dashed] (action0) -- (state1);
\draw[->,>=latex,dashed] (action0) -- (state12);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRANSITION FUNCTIONS
\node (T1) at (6.4,2.7) {$T^1_a$};
\node (T2) at (6.4,1.3) {$T^2_a$};
\end{tikzpicture}
\caption[Réseau bayésien dynamique d'un ($\pi$-)PDM factorisé]
{Réseau dynamique bayésien pour des ($\pi$-)PDM:
dans le cadre possibiliste (resp. probabiliste) 
$T^i_a$ est la distribution de possibilité (resp. probabilité) de transition
sur la variable d'état $X^i_{t+1}$
conditionnellement à
l'action sélectionnée $a \in \mathcal{A}$
et à ses parents $parents(X^i_{t+1}) \subseteq \set{X^1_t,\ldots,X^n_t}$
(\textit{i.e.} $parents(X^i_{t+1})$ est un sous ensemble de l'ensemble des variables d'état courantes)
où $n\geqslant1$ est le nombre de variables décrivant l'espace d'état.
}
\label{fig_piMDPFact}
\end{figure}
Dans le cadre des DBNs, $parents(X^i_{t+1})$ 
est l'ensemble des variables d'état
desquelles la variable d'état suivante $X^i_{t+1}$ dépend,
\textit{i.e.} une variable $Y$, 
représenté par un noeud dans le DBN,
est dans $parents(X^i_{t+1})$ 
si et seulement si il y a une flèche de $Y$ à $X^j_{t+1}$.
Nous supposons que $parents(X^i_{t+1}) \subseteq \set{X^1_t,\ldots,X^n_t}$,
\textit{i.e.} les parents de la variable d'état suivante $X^i_{t+1}$
font partie des variables d'état courantes $\set{X^1_t,\ldots,X^n_t}$:
il ne peut y avoir de flèches entre
les variables d'état de la même étape de temps.

Avec les notations du $\pi$-PDMOM, 
les hypothèses du réseau bayésien
de la figure \ref{fig_piMDPFact}
nous permettent de calculer la distribution de possibilité jointe: 
$\pi \paren{s_v',\beta_h' \sachant s_v,\beta_h,a} = \pi \paren{ X' \sachant X,a } = \min_{i=1}^{n} \pi \paren{ X_i' \sachant parents(X_i'),a}$,
où, étant donné l'étape de temps $t$,
les variables primées sont les variables concernant l'étape de temps $t+1$
(variables suivantes), 
et les variables non-primées sont les 
variables courantes
(à l'étape de temps $t$): 
par exemple, 
$X_i'$ est la notation pour $X^i_{t+1}$,
et $X_i$ celle pour $X_t^i$.
Ainsi, un $\pi$-PDMOM factorisé peut être défini
par des fonctions de transitions 
$T^i_a= \pi \paren{X_i' \sachant parents(X_i'),a }$ 
pour chaque action $a$
et chaque variable $X_i'$
(si les transitions sont stationnaires).
% such that $T^{a,i}(parents(X_i'),a,X_i') = \pi \paren{\mathbb{X}_i' \sachant parents(\mathbb{X}_i'),a }$.

Chaque fonction de transition
peut être représentée par un arbre de décision algébrique 
(ADD) \cite{Bahar:1997:ADD}. 
Un ADD, comme illustré dans la figure \ref{fig_ADDtransition}, 
est un arbre représentant de manière compacte
une fonction réelle de variables binaires, 
dont les sous-graphes identiques sont confondus
et les feuilles valant zéro ne sont pas mémorisées.
Les notations suivantes
sont utilisées pour rendre explicite
le fait que nous travaillons avec des fonctions symboliques
représentées par des ADDs:
\begin{itemize}
\item $\ovalbox{$\min$} \set{f,g}$ où $f$ et $g$ sont deux ADDs;
\item $\ovalbox{$\max$}_{X_i} f$ = $\ovalbox{$\max$} \set{ f^{X_i=0},f^{X_i=1} }$, 
\end{itemize}
qui peut être facilement calculé car les ADDs
sont construit sur la base
de l'expension de Shanon: $f = \overline{X_i} \cdot f^{X_i=0} + X_i \cdot f^{X_i=1}$ 
où $f^{X_i=1}$ et $f^{X_i=0}$ 
sont les sous graphes représentant
les cofacteurs de Shanon positifs et négatifs 
(\textit{cf.} figure \ref{fig_ADDtransition}). 

Le schéma de programmation dynamique, 
\emph{i.e.} la ligne \ref{VIupdate_OptAlgo} 
de l'algorithme d'itération sur les valeurs \ref{algorithmIVPIMDP} 
du chapitre précédent,
%equation of Line \ref{alg_piMDP_DP} of Algorithm \ref{algorithmIVPIMDP}
peut être réécrite sous forme symbolique, 
de telle sorte que les états soient
globalement mis à jour en un coup,
plutôt qu'individuellement:
en notant $X=(X_1,\ldots,X_n)$ 
la variable d'état courante
et $X'=(X_1',\ldots,X_n')$ la suivante,
la Q-valeur pour une action $a \in \mathcal{A}$
%in a state $x \in \mathcal{X}$,
est $\overline{q^a} = \overline{q^a}(X)= \ovalbox{$\max$}_{X'} \ovalbox{$\min$} \set{ \pi \paren{ X' \sachant X,a }, \overline{U^*}(X') }$.
Le calcul de cet ADD ($\overline{q^a}$) 
peut être décomposé en plusieurs calculs 
en utilisant la proposition suivante:
\begin{Property}[Régression possibiliste de la fonction valeur]
\label{propositionRegression}
Considérons la fonction valeur courante
 $\overline{U^*}: \set{\top,\bot}^n \rightarrow \mathcal{L}$. 
Pour une action donnée $a \in \mathcal{A}$, 
définissons :
\begin{itemize}
\item $\overline{q^a_0} = \overline{U^*}(X'_1,\cdots,X'_n)$, 
\item $\overline{q^a_i} = \max_{X'_i \in \set{\top,\bot}} \min \Big\{ \pi \paren{X_i' \sachant parents(X_i'),a} , \overline{q^a_{i-1}} \Big\}$.
\end{itemize}
Alors, la  Q-valeur possibiliste d'une action $a$ est: 
$\overline{q^a} = \overline{q^a_n}$,
qui dépend des variables $X_1,\ldots,X_n$,
et de la fonction valeur suivante, est $\overline{U^*}(X_1,\ldots,X_n) = \max_{a \in \mathcal{A}} \overline{q^a_n}(X_1,\ldots,X_n)$.
\end{Property}

La Q-valeur de l'action $a$, 
représentée par un ADD,
peut être alors calculée en plusieurs étapes,
une pour chaque variable d'état suivante $X'_i,
1 \leqslant i \leqslant n$. 
\begin{figure}
\centering
\begin{subfigure}[b]{0.35\linewidth}
\flushleft
\begin{tikzpicture}
\draw (-1.2,5.5) rectangle (2.8,7.3);
\draw (0.75,7) node (key) {KEY};
\draw (0.25,5.5) node (Lt) {};
\draw (1.25,5.5) node (Lf) {};
\draw (key) -- (Lt) node [left,midway] {$\top$ (true)};
\draw [dashed] (key) -- (Lf) node [right,midway] {$\bot$ (false)};

\draw (1,4.5) node (Xp1) {$X_1'$};
\draw (0,0.5) node (L3) {$1$};
\draw (1.5,3.2) node (X11) {$X_1$};
\draw (2,2) node (X2) {$X_2$};
\draw (1,0.5) node (L1) {$\frac{2}{3}$};
\draw (2.6,0.5) node (L2) {$\frac{1}{3}$};
\draw [dashed] (Xp1) -- (X11);
\draw (X11) -- (L1);
\draw [dashed] (X11) -- (X2);
\draw (X2) -- (L1);
\draw [dashed] (X2) -- (L2);
\draw (Xp1) -- (L3);
\end{tikzpicture}
\caption{ADD encoding $T^a_1$ of Fig. \ref{fig_piMDPFact}\\\\}
\label{fig_ADDtransition}
\end{subfigure}
\qquad
\begin{subfigure}[b]{0.58\linewidth}
\flushright
\begin{tikzpicture}

\draw (-2,3) node {\ovalbox{$\min$} $\Bigg\{$};

\draw (0,4.5) node (Xp1) {$X'_1$};
\draw (1,3) node (Xp2) {$X'_2$};
\draw (-1,1.5) node (Lp1) {$\frac{1}{3}$};
\draw (0.5,1.5) node (Lp2) {$\frac{2}{3}$};
\draw (2,1.5) node (Lp3) {$0$};
\draw (Xp1) -- (Lp1);
\draw [dashed] (Xp1) -- (Xp2);
\draw (Xp2) -- (Lp2);
\draw [dashed] (Xp2) -- (Lp3);

\draw (2.2,3) node {,};

\draw (4,4.5) node (Xp1) {$X_1'$};
\draw (3,1.5) node (L3) {$1$};
\draw  (Xp1) -- (L3);
\draw (4.6,3.5) node (X11) {$X_1$};
\draw (5.2,2.5) node (X2) {$X_2$};
\draw (4,1.5) node (L1) {$\frac{2}{3}$};
\draw (5.7,1.5) node (L2) {$\frac{1}{3}$};
\draw (X11) -- (L1);
\draw [dashed] (X11) -- (X2);
\draw (X2) -- (L1);
\draw [dashed] (X2) -- (L2);
\draw [dashed] (Xp1) -- (X11);


\draw (6.3,3	) node {$\Bigg\}$};



\draw (-2,-1.3) node {=};
%\draw (-0.5,-1.3) node {=};


\draw (-0.7,0.5) node (rXp1) {$X'_1$};
\draw (0.5,-0.2) node (rXp2) {$X'_2$};
\draw (-0.2,-1.2) node (rX1) {$X_1$};
\draw (0.7,-2) node (rX2) {$X_2$};
\draw (-0.5,-3.2) node (rL1) {$\frac{2}{3}$};
\draw (1.3,-3.2) node (rL2) {$\frac{1}{3}$};
\draw (-1.5,-3.2) node (rL3) {$\frac{1}{3}$};
\draw (2,-3.2) node (rL4) {$0$};

\draw [dashed] (rXp1) -- (rXp2);
\draw  (rXp2) -- (rX1);
\draw [dashed] (rXp2) -- (rL4);
%\draw (rXp2) -- (rX1);
%\draw [dashed] (rXp2) -- (rL3);
\draw (rXp1) -- (rL3);
\draw (rX1) -- (rL1);
\draw [dashed] (rX1) -- (rX2);
\draw (rX2) -- (rL1);
\draw [dashed] (rX2) -- (rL2);

%\draw (0.3,-2) node (dL1) {$\frac{1}{3}$};
%\draw (rXp1) -- (rL1);

\draw (2.5,-1.3) node {$\xrightarrow[\text{\ovalbox{$\max$}}_{X'_1}]{}$};

%\draw (4.5,-2) node (rrXp2) {$\mathbb{X}'_2$};
\draw (4.8,0.5) node (rrpX2) {$X_2'$};
\draw (4,-0.5) node (rrX1) {$X_1$};
\draw (4.5,-1.7) node (rrX2) {$X_2$};
\draw (3.5,-3.2) node (rrL1) {$\frac{2}{3}$};
\draw (5.7,-3.2) node (rrL2) {$\frac{1}{3}$};
\draw (rrpX2) -- (rrX1);
\draw (rrX1) -- (rrL1);
%\draw (rrXp2) -- (rrX1);
\draw [dashed] (rrX1) -- (rrX2);
\draw (rrX2) -- (rrL1);
\draw [dashed] (rrX2) -- (rrL2);
\draw [dashed] (rrpX2) -- (rrL2);

\end{tikzpicture}
\caption{Calcul symbolique de la Q-valeur courante sous forme d'un ADD: 
elle est combiné à la fonction de transition
représentée en ADD dans la figure \ref{fig_ADDtransition}\\}
\label{fig_ADDregression}
\end{subfigure}
\caption{Arbre de décision algébrique pour PPUDD}
\label{fig_ADD}
\end{figure}	
La figure \ref{fig_ADDregression}
illustre les calculs possibilistes 
effectués entre les arbres de décision algébriques (ADDs)
pour calculer la Q-valeur d'une action.

\begin{algorithm} \caption{PPUDD (calcul pour un horizon indéterminé)} \label{PPUDD} 
 $\overline{U^*} \gets 0$ ;
 $\overline{U^c} \gets \Psi$ ;
 $\overline{\delta} \gets \widehat{a}$ \;

\While {$\overline{U^*} \neq \overline{U^c}$ \label{while_PPUDD} }{
 $\overline{U^*} \gets \overline{U^c}$ \;
 \For {$a \in \mathcal{A}$ \label{ppuddBeginQ}}{
	$\overline{q^a}  \gets $ swap each $X_i$ variable in $\overline{U^*}$ with $X_i'$ \label{ppuddSwap} \;
	\For {$1 \leqslant i \leqslant n$}{
	 $\overline{q^a} \gets \ovalbox{$\min$} \ \bigg\{ \overline{q^a} , \pi \Big( X_i' \ \Big\vert parents(X_i'),a \Big) \bigg\}$ \;
	 $\overline{q^a} \gets \ovalbox{$\max$}_{X_i'} \overline{q^a}$ \;
	}
	$\overline{U^c} \gets \ovalbox{$\max$} \set{\overline{q^a},\overline{U^c}  } $ \label{ppuddEndQ} \;
	update $\overline{\delta}$ to $a$ where $\overline{q^a}=\overline{U^c}$ and $\overline{U^c} > \overline{U^*}$ \;
}
}
\Return $\overline{U^*}$, $\overline{\delta^*}$ \;
\end{algorithm}

L'algorithme \ref{PPUDD}
est une version symbolique
de l'algorithme d'itération sur les valeurs
pour $\pi$-PDM \ref{algorithmIVPIMDP} dans le chapitre précédent),
qui utilise le schéma de régression défini
dans la proposition \ref{propositionRegression}. 
Inspiré de SPUDD \cite{Hoey99spuddstochastic},
PPUDD signifie \emph{Possibilistic Planning Using Decision Diagrams}. 
Comme pour SPUDD,
l'action de transformer les variables non primées
en variables primées est nécessaire à chaque itération
(\textit{cf.} ligne \ref{ppuddSwap} de l'algorithme \ref{PPUDD} 
et figure \ref{fig_ADDregression}). 
Cette opération sert à différentier les états suivants des états courants
lors des opérations entre les ADDs. 
Les lignes \ref{ppuddBeginQ}-\ref{ppuddEndQ} 
appliquent la proposition \ref{propositionRegression} 
et correspond à la ligne
\ref{VIupdate_OptAlgo} de l'algorithme \ref{algorithmIVPIMDP}.

Nous avons mentionné au début de la section
que l'espace de états de croyance $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
pourrait être décrit par $\lceil \log_2 K \rceil$ 
variables binaires où $K=\#
\mathcal{L}^{\#\mathcal{S}_h} - (\# \mathcal{L}-1)^{\# \mathcal{S}_h}$. 
Cependant, ce $K$ peut être très grand,
ainsi nous proposons
dans la section qui suit,
une méthode pour exploiter la factorisation
de $\mathcal{S}_h$ et $\mathcal{O}_h$
dans le but de factoriser $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
en plusieur variables, 
ce qui décomposera les ADDs de transition
en plus petits ADDs facilement manipulables.
Notons que PPUDD peut résoudre les $\pi$-PDMOM
même si cette factorisation de la croyance n'est pas
possible, mais les ADDs manipulés sont plus gros dans ce cas.

Le chapitre précédent met en évidence qu'un prétraitement
est nécessaire
pour traduire un
$\pi$-PDMOM en un $\pi$-PDM dont l'espace d'état est $\mathcal{X}$.
Nous pouvons alors raisonner sur l'espace d'état
entièrement observable pour
l'agent $\mathcal{X} = S_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$
et résoudre le $\pi$-PDMOM en tant que $\pi$-MDP.
La section suivante fait le lien entre les propriétés structurelles
d'un $\pi$-PDMOM,
concernant les dépendences des variables originales 
(visibles, cachées et d'observation), 
à la factorisation du problème traité 
\textit{i.e.} du $\pi$-PDM
résultant, défini sur l'espace d'état $S_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$:
la factorisation résultante concerne alors 
les dépendances des variables visibles et des variables de croyance).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Factorisation de la variable de croyance d'un $\pi$-PDMOM}

La factorisation de la variable de croyance 
requiert trois hypothèse d'indépendance
sur les variables du $\pi$-PDMOM, 
qui sont illustrées à travers le problème classique \textit{Rocksample} 
\cite{Smith:2004HSV1036843.1036906}.
\subsection{Exemple de motivation}
\label{section_RS_motivatingEx}
Un robot se déplaçant sur une grille $g \times g$ 
doit collecter des échantillons scientifiques 
à partir de pierres dites intéressantes (``bonne'') pierres
parmi $R$ pierres, et enfin d'atteindre la sortie de la grille.
% It is 
%equipped with a noisy long-range sensor that can be used to determine if a rock 
%is ``good'' or not.
Il connait les positions des $R$ pierres $(x_i,y_i)_{i=1}^R$
mais pas lequelles sont intéressantes 
(appelées les ``bonnes'' pierres).
Cependant, échantillonner une pierre coûte cher:
le robot est donc équipé d'un capteur 
qu'il peut utiliser that can be used to déterminer 
si une pierre est ``bonne'' ou non (``mauvaise''). 
Lorsqu'une pierre est échantillonnée,
elle devient (ou reste) ``mauvaise'' 
(plus intéressante scientifiquement). 
A la fin de la mission, le robot doit atteindre la position de sortie de la grille 
à droite de la grille:
\begin{itemize} 
\item $\mathcal{S}_{v}$ représente l'ensemble des positions possibles du robot %$(x_r,y_r)$
en plus de la sortie ($\# \mathcal{S}_v = g^2 +1$);
\item $\mathcal{S}_h$ représente l'ensemble des natures possibles de chacune des pierres:
$\mathcal{S}_h = \mathcal{S}^1_h \times \ldots \times \mathcal{S}^R_h$,
avec $\forall 1 \leqslant i \leqslant R$, $\mathcal{S}^i_h=\set{ bonne,mauvaise }$;
\item $\mathcal{A}$ contient les déplacements (déterministes) dans les $4$ directions ($a_{north},a_{east},a_{south},a_{west}$),  
tester la pierre numéro $i$, ($a_{check_i}$) 
$\forall 1 \leqslant i \leqslant R $, 
et échantillonner la pierre courante, ($a_{sample}$);
\item $\mathcal{O} = \set{ o_{good},o_{bad} }$ sont les différentes réponses
possibles des capteurs pour la pierre testée. 
% $\mathcal{O}_{1} \times \ldots \times \mathcal{O}_{R}$ where $\forall 1 \leqslant i \leqslant R$, $\mathcal{O}_{i}=\set{ o_{good},o_{bad} }$ are observations concerning the $i^{th}$ rock. \\
\end{itemize}

La dynamique des observations est la suivante: 
plus le robot est proche de la pierre testée, 
mieux il observe sa nature.
Dans le problème probabiliste original,
la probabilité d'une observation correcte
est égale à
$\frac{1}{2}\paren{ 1 + e^{-c \sqrt{(x_r-x_i)^2 + (y_r-y_i)^2} }} $ avec $c>0$,
une constante (plus $c$ est petit, plus le capteur est efficace). 
Le robot reçoit la récompense $+10$ (resp. $-10$) 
pour chaque bonne (resp. mauvaise) pierre échantillonnée, % $-10$ for each bad rock sampled,
et $+10$ lorsqu'il atteint la sortie de la grille. 

Dans cadre possibiliste, 
la fonction d'observation
est approximée en utilisant
une distance critique au-delà 
de laquelle tester une pierre n'est pas informatif:
$\pi \paren{ o_i' \sachant s_i',a,s_v } = 1$ $ \forall o_i' \in \mathcal{O}_{i} $.
%If the
%rover is distant from the rock less than $d$, 
Le degré de possibilité d'une observation erronée
devient zéro lorsque le robot se trouve sur
la pierre testée, 
et le plus petit degré de possibilité 
non nul sinon. 
Enfin, puisque la sémantique possibiliste ne permet
pas de sommer les récompenses,
une variable additionnelle
$s^2_v \in
\set{ 1, \ldots, R }$
 est introduite:
elle compte le nombre de pierres testées.
L'aversion qualitative de l'échantillonage
est défini par $\Psi(s)=\frac{R+2-s_{v}^{2}}{R+2} \in \mathcal{L}$
si la position est la sortie,
et zéro sinon.
Enfin, la position du robot
est notée $s^1_v \in \mathcal{S}^1_v$ 
et donc la variable d'état visible 
est finalement
$s_v=(s^1_v,s^2_v) \in \mathcal{S}^1_v \times \mathcal{S}^2_v = \mathcal{S}_v$. 

Les observations $\set{ o_{good},o_{bad} }$
pour la pierre testée peuvent être modélisées de manière équivalente
comme le produit cartésien des observations 
$\set{ o_{good_1},o_{bad_1} } \times \cdots \times \set{ o_{good_R},o_{bad_R} }$ 
pour chaque pierre.
En utilisant cette modélisation,
les espaces d'états et d'observations
sont respectivement factorisés de la manière suivante,
$\mathcal{S}^1_v \times \ldots \times \mathcal{S}^m_v \times \mathcal{S}^1_h \times \ldots \times \mathcal{S}^l_h $,
et $\mathcal{O} = \mathcal{O}^1 \times \ldots \times \mathcal{O}^l$, 
et nous pouvons maintenant associer une variable d'observation 
$O^j \in \mathcal{O}^j$ à la variable d'état cachée correspondante $S^j_h \in \mathcal{S}^j_h$. 
Cela permet de raisonner
sur le DBN de la figure \ref{fig_piMOMDPFact}, 
qui exprime trois hypothèses importantes
qui nous permettrons de factoriser l'état de croyance: 
\begin{enumerate}
\item toutes les variables d'état $S^1_v,S^2_v,\ldots,S^m_v,S^1_h,S^2_h,\ldots,S^l_h$ 
sont indépendantes post-action, 
et la variable d'état suivante ne dépend par des variables d'état cachées courantes.
\item une variable cachée ne dépend pas d'autres variables d'état cachées précédentes: 
la nature d'une pierre est indépendante de la nature 
des autres pierres.
\item une variable d'observation est disponible pour chaque variable d'état cachée,
et dépend de cet état.
Il ne dépend pas d'autres
variables d'état cachées,
où des variables visibles courantes,
mais des variables visibles précédentes et de l'action choisie:
\end{enumerate}
Chaque variable d'observation
est en effet seulement associée
à la nature de la pierre correspondante.
La qualité de l'observation dépend de la position du robot 
\emph{i.e.} une variable d'état visible courante, 
ce qui n'est pas autorisé par le DBN: 
heureusement, puisque les déplacements sont déterministes,
nous contournons le problème en considérant que cette qualité
dépend de la position précédente et de l'action choisie.


\begin{figure}[b!]\centering
\begin{tikzpicture}
\tikzstyle{vertex}=[circle,fill=black!50,minimum size=32pt,inner sep=0pt,draw=black,thick]
\tikzstyle{vvertex}=[circle,fill=black!30,minimum size=32pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red!60,minimum size=25pt,inner sep=0pt,draw=black,thick]
\tikzstyle{rvertex}=[fill=yellow!60,decision=3,inner sep=-1pt,minimum size=35pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=32pt,inner sep=0pt,draw=black,thick]


%TIME
\node [font=\huge] (statet) at (3,7.3) {$t$};
\node [font=\huge] (statetplus1) at (10,7.3) {$t+1$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%vstates
%1
\node[vvertex] (vstate1) at (3,6) {$S^1_{v,t}$};
\node[vvertex] (vstate12) at (3,4.5) {$S^2_{v,t}$};
\node (vstate13) at (3,3.5) {$\vdots$};
%2
\node[vvertex] (vstate2) at (10,6) {$S^1_{v,t+1}$};
\node[vvertex] (vstate22) at (10,4.5) {$S^2_{v,t+1}$};
\node (vstate23) at (10,3.5) {$\vdots$};

%0
\node (vstate0) at (-1,6) {};
\node (vstate02) at (-1,4.5) {};
%3
\node (vstate3) at (14,6) {};
\node (vstate32) at (14,4.5) {};


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%hstates
%1
\node[vertex] (hstate1) at (3,2) {$S^1_{h,t}$};
\node[vertex] (hstate12) at (3,0.5) {$S^2_{h,t}$};
\node (hstate13) at (3,-0.5) {$\vdots$};
%2
\node[vertex] (hstate2) at (10,2) {$S^1_{h,t+1}$};
\node[vertex] (hstate22) at (10,0.5) {$S^2_{h,t+1}$};
\node (hstate23) at (10,-0.5) {$\vdots$};
%0
\node (hstate0) at (-1,2) {};
\node (hstate02) at (-1,0.5) {};
%3
\node (hstate3) at (14,2) {};
\node (hstate32) at (14,0.5) {};



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%action
\node[avertex] (action) at (6.2,-2.3) {$a_t$};
\node[avertex] (action0) at (0,-2.3) {$a_{t-1}$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% observations
%1
\node[overtex] (hobserv1) at (6,-4) {$O^1_t$};
\node[overtex] (hobserv12) at (4.6,-2.9) {$O^2_t$};
\node (doto1) at (3.9,-2.1) [rotate=36] {$\vdots$};
%\node at (3.7,-3.3) {$\ldots$};
%2
\node[overtex] (hobserv2) at (13,-4) {$O^1_{t+1}$};
\node[overtex] (hobserv22) at (11.6,-2.9) {$O^2_{t+1}$};
\node (doto2) at (10.9,-2.1) [rotate=36] {$\vdots$};
%\node at (7.9,-3.4) {$\ldots$};


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS

%SV
% arrows sv->sv
%1->2
\draw[->,>=latex] (vstate1) -- (vstate2);
\draw[->,>=latex] (vstate1) -- (vstate22);
\draw[->,>=latex] (vstate12) -- (vstate2);
\draw[->,>=latex] (vstate12) -- (vstate22);
%0->1
\draw[->,>=latex,dashed] (vstate0) -- (vstate1);
\draw[->,>=latex,dashed] (vstate0) -- (vstate12);
\draw[->,>=latex,dashed] (vstate02) -- (vstate1);
\draw[->,>=latex,dashed] (vstate02) -- (vstate12);
%2->3
\draw[->,>=latex,dashed] (vstate2) -- (vstate3);
\draw[->,>=latex,dashed] (vstate2) -- (vstate32);
\draw[->,>=latex,dashed] (vstate22) -- (vstate3);
\draw[->,>=latex,dashed] (vstate22) -- (vstate32);


% arrows sv->sh
%1->2
\draw[->,>=latex] (vstate1) -- (hstate2);
\draw[->,>=latex] (vstate1) -- (hstate22);
\draw[->,>=latex] (vstate12) -- (hstate2);
\draw[->,>=latex] (vstate12) -- (hstate22);
%0->1
\draw[->,>=latex,dashed] (vstate0) -- (hstate1);
\draw[->,>=latex,dashed] (vstate0) -- (hstate12);
\draw[->,>=latex,dashed] (vstate02) -- (hstate1);
\draw[->,>=latex,dashed] (vstate02) -- (hstate12);

%2->3
\draw[->,>=latex,dashed] (vstate2) -- (hstate3);
\draw[->,>=latex,dashed] (vstate2) -- (hstate32);
\draw[->,>=latex,dashed] (vstate22) -- (hstate3);
\draw[->,>=latex,dashed] (vstate22) -- (hstate32);


% arrows sv->oh

\draw[->,>=latex] (vstate1) to[bend right] (hobserv2);
\draw[->,>=latex] (vstate12) to[bend right] (hobserv2);
\draw[->,>=latex] (vstate1) to[bend right] (hobserv22);
\draw[->,>=latex] (vstate12) to[bend right] (hobserv22);


\node (fake0) at (-1,-1) {};
\node (fake1) at (-1,0) {};
\node (fake2) at (-1,-0.3) {};
\node (fake3) at (-1,-0.7) {};
\draw[->,>=latex,dashed] (fake0) to (hobserv1);
\draw[->,>=latex,dashed] (fake1) to (hobserv12);
\draw[->,>=latex,dashed] (fake2) to (hobserv1);
\draw[->,>=latex,dashed] (fake3) to (hobserv12);

%SH
% arrows sh->sh
%1->2
\draw[->,>=latex] (hstate1) -- (hstate2);
\draw[->,>=latex] (hstate12) -- (hstate22);
%0->1
\draw[->,>=latex,dashed] (hstate0) -- (hstate1);
\draw[->,>=latex,dashed] (hstate02) -- (hstate12);

%2->3
\draw[->,>=latex,dashed] (hstate2) -- (hstate3);
\draw[->,>=latex,dashed] (hstate22) -- (hstate32);

% arrows sh->oh
%1
\draw[->,>=latex] (hstate1) to (hobserv1);
\draw[->,>=latex] (hstate12) to (hobserv12);
%2
\draw[->,>=latex] (hstate2) to (hobserv2);
\draw[->,>=latex] (hstate22) to (hobserv22);

%A
% a->s
%1
\draw[->,>=latex] (action) -- (vstate2);
\draw[->,>=latex] (action) -- (vstate22);
\draw[->,>=latex] (action) -- (hstate2);
\draw[->,>=latex] (action) -- (hstate22);
%0
\draw[->,>=latex,dashed] (action0) -- (vstate1);
\draw[->,>=latex,dashed] (action0) -- (vstate12);
\draw[->,>=latex,dashed] (action0) -- (hstate1);
\draw[->,>=latex,dashed] (action0) -- (hstate12);

% a->oh
%1
\draw[->,>=latex] (action) to[bend right] (hobserv2);
\draw[->,>=latex] (action) to[bend right] (hobserv22);
%0
\draw[->,>=latex] (action0) to[bend right] (hobserv1);
\draw[->,>=latex] (action0) to[bend right] (hobserv12);

%\node (probao1) at (4,-2.2) [rotate=350] {$ \pi \paren{ o_{h,t} \sachant s_t, a_{t-1}}$};
%\node (probao2) at (11,-2.5) [rotate=350] {$ \pi \paren{ o_{h,t+1} \sachant s_{t+1}, a_{t}}$};
%\node (probas) at (8.9,0.5) [rotate=355] {$ \pi \paren{s_{t+1} \sachant s_t,a_{t}}$};

\end{tikzpicture}
\caption[DBN d'un $\pi$-PDMOM dont les variables de croyances peuvent être factorisées]{
DBN résumant les hypothèse d'indépendance d'un $\pi$-PDMOM
menant à des variables de croyances marginales 
et un $\pi$-PDM avec
une fonction de transition factorisée.
Les parents d'une variable d'état visible sont les variables d'état visibles précédentes.
Les parents d'une variable d'état cachée sont les variables d'état visibles précédentes
et la variable d'état cachée précédente correspondante. 
Enfin, les parents d'une variable d'observation
sont les variables d'état visible précédentes,
et la variables d'état cachée courante correspondante.
}
\label{fig_piMOMDPFact}
\end{figure}

\subsection{Conséquences des hypothèses de factorisation}
\label{section_factoAssumptions}
Dans cette section,
nous présentons le résultat formel
provenant des trois précédentes hypothèse:
ce résultat est la factorisation de $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$ 
en le produit cartésien $\displaystyle \bigtimes_{j=1}^{l} \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$.
En effet, l'état de croyance $\beta_h$
à propos de l'état caché du système $s_h \in \mathcal{S}_h$ 
peut être représenté des croyances marginales $\beta^j_h \in \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
sur les états cachés $s^j \in \mathcal{S}^j_h$, $\forall j \in \set{1,\ldots,l}$.

Le \textit{critère de d-Separation} \cite{pearl88}
qui permet de montrer graphiquement des résultats d'indépendance
à partir du DBN, se cache derrière les résultats fournis. 
Comme expliqué en section \ref{section_PPUDD}, 
un DBN peut être construit à partir de relations d'indépendances.
Notons $X \perp\!\!\!\perp Y \ \vert \ Z$
l'assertion ``$X$ est indépendant de $Y$ 
conditionnellement à $Z$'':
rappelons que pour une définition donnée de la relation d'indépendance 
par exemple probabiliste, ou possibiliste causale,
le DBN est construit de telle sorte que pour chaque variable $X$,
$X \perp\!\!\!\perp nondescend(X) \ \vert \ parents(X)$.
Si l'indépendance utilisée obéit aux axiomes des  \textit{semi-graphoids} 
\cite{Pearl1988PRI52121,DBLPjournals/corr/abs-1304-2379},
le critère graphique appelé d-Separation 
peut être utilisé pour identifier certaines indépendances sur le DBN.
Ce critère est utilisé, par exemple, dans le cadre des probabilités dans le travail \cite{Witwicki13icaps}.

Tout d'abord, 
le DBN de la figure \ref{fig_piMOMDPFact} 
représentant des hypothèses d'indépendance causale
la distribution de possibilité sur la variable d'état visible numéro $i$
$s^i_{v,t+1} \in \mathcal{S}^i_v$ peut s'écrire: 
\begin{equation}
\label{EQ_possV}
\pi \paren{ s^i_{v,t+1} \sachant s_{v,t}, a_t } = \Pi \paren{ S^i_{v,t+1} = s^i_{v,t+1} \sachant S_{v,t} = s_{v,t}, a_t   };
\end{equation}

Définissons l'information $i_t$
connue par l'agent à l'étape de temps $t \geqslant 1$
lorsque le modèle est un ($\pi$-)PDMOM:
 % of a $\pi$-MOMDP as: 
$i_0 = \set{ s_{v,0} }$, et
pour chaque étape de temps $t \geqslant 1$, 
$i_t = \set{ o_t,s_{v,t},a_{t-1},i_{t-1} }$:
la variable correspondante est notée $I_t$.
Le théorème suivant assure que l'état de croyance
courant peut être décomposé
en états de croyance marginaux
dépendant de l'information courante.
\begin{theorem}[Indépendance des variables d'état cachées sachant $i_t$]
\label{thmSHind} 
Considérons un $\pi$-PDMOM décrit par le DBN de la figure \ref{fig_piMOMDPFact}.
Si les variables d'état cachées initialesv$S^1_{h,0}, \ldots,S^l_{h,0}$ 
sont indépendantes, alors à chaque étape de temps $t>0$
l'état de croyance sur les états cachés
peut s'écrire 
\[ \beta_{h,t} = \displaystyle \min_{j=1}^{l} \beta^j_{h,t}\]
avec $\forall s \in \mathcal{S}^j_h$, $\beta^j_{h,t}(s) = \Pi \paren{ S^j_{h,t} = s \sachant I_t = i_t }$ l'état de croyance marginal
concernant les états cachés du système de l'ensemble $\mathcal{S}^j_h$.
\end{theorem}

Grâce au théorème précédent, 
l'espace d'état visible
par l'agent peut se réécrire 
$\mathcal{S}^1_v \times \ldots \times
\mathcal{S}^m_v \times \Pi^{\mathcal{S}^1_h}_{\mathcal{L}} \times \cdots \times \Pi^{\mathcal{S}^l_h}_{\mathcal{L}}$ 
avec $\Pi^{\mathcal{S}^j_h}_{\mathcal{L}}
\subsetneq \mathcal{L}^{\mathcal{S}^j_h}$. 
La taille de $\Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
est $\#\mathcal{L}^{\#\mathcal{S}^j_h} - (\#\mathcal{L}-1)^{\#\mathcal{S}^j_h}$
(s\textit{cf.} équation \ref{equation_numberOfPossDistrib}). 
Si toutes les variables d'état sont binaires,
$\# \Pi^{\mathcal{S}^j_h}_{\mathcal{L}} = 2 \#\mathcal{L} - 1$ pour tout $1 \leqslant j \leqslant l$, 
de telle sorte que 
$\# \mathcal{S}_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}} = 2^m (2
\#\mathcal{L} - 1)^l$: 
contrairement au cadre probabiliste,
{\bf les états cachés et les états visibles ont un impact
similaire sur la complexité de résolution}, 
\textit{i.e.} tous les deux simplement exponentiels 
end le nombre de variables d'état. 
Dans le cas général, en notant 
$\kappa = \max\{\max_{1 \leqslant i \leqslant m} \# S_{v,i} , \max_{1 \leqslant j \leqslant l} \# S_{h,j}\}$, 
il y a $\mathcal{O}(\kappa^m (\# \mathcal{L})^{(\kappa - 1) l})$ 
états de croyances, ce qui exponentiel en l'arité des variables d'état. 

La \textbf{fonction de mise à jour de l'état de croyance marginal} est $\nu^j$:
\[ \beta^j_{h,t+1} = \nu^j(s_{v,t},\beta^j_{h,t},a_t,o^j_{t+1}), \]
Cette mise à jour peut se noter 
\[ \beta^j_{h,t+1}(s^j_{h,t+1}) \propto^{\pi} \pi \paren{o^j_{t+1},s^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t} \]
puisqu'elle normalise au sens possibiliste la distribution de possibilité jointe
sur la variable d'état cachée et la variable d'observation qui ont le numéro $j$.

Ainsi, le degré de possibilité
que la variable de croyance marginale  
$B^{\pi,j}_{h,t+1}$ soit égale à $\beta^j_{h,t+1} \in \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
conditionnellement à $B^{\pi,j}_{h,t} = \beta^j_{h,t}$ 
et l'action $a_t \in \mathcal{A}$, 
peut se calculer:
\begin{equation}
\label{trans_marg_belief}
 \Pi \paren{ B^{\pi,j}_{h,t+1} = \beta^j_{h,t+1} \sachant S_{v,t} = s_{v,t}, B^{\pi,j}_{h,t} = \beta^j_{h,t}, a_t } = \max_{\substack{ o^j \in \mathcal{O}^j \mbox{ \tiny s.t. } \\ \nu^j(s_{v,t},\beta^j_{h,t},a_t,o^j) = \beta^j_{h,t+1}}} \pi \paren{ o^j \sachant s_{v,t}, \beta^j_{h,t}, a_t  }
\end{equation}
définissat la distribution de possibilité de transition
des états de croyance marginaux
$\pi \paren{ \beta^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t }$.

Enfin, le théorème \ref{thmVARind} nous assure que
les variables du $\pi$-PDM résultant d'un tel $\pi$-PDMOM %% TODO
indépendantes post-action conditionnellement à l'état courant du système:
cela permet alors d'écrire
la fonction de transition
de ce $\pi$-PDM sous forme factorisée:
\begin{theorem}[Expression factorisée de la distribution de possibilité de transition]
\label{thmVARind}
Si les hypothèses d'indépendance d'un $\pi$-PDMOM
sont décrites par par le DBN de la figure \ref{fig_piMOMDPFact},
alors
$\forall \beta_{h,t}=(\beta^1_{h,t},\ldots,\beta^l_{h,t}) \in \Pi^{\mathcal{S}_h}_{\mathcal{L}}, 
\beta_{h,t+1} = (\beta^1_{h,t+1},\ldots,\beta^l_{h,t+1}) \in  \Pi^{\mathcal{S}_h}_{\mathcal{L}}$, 
$\forall (s_{v,t},s_{v,t+1}) \in (\mathcal{S}_v)^2$, 
$\forall a_t \in \mathcal{A}$, \\
$\pi \paren{ s_{v,t+1}, \beta_{h,t+1} \sachant s_{v,t}, \beta_{h,t}, a }$ 
\[  = \displaystyle  \min \set{ \min_{i=1}^{m} \pi \paren{ s^i_{v,t+1} \sachant s_{v,t}, a_t } ,  \min_{j=1}^{l} \pi \paren{ \beta^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t  } }, \]
où la distribution de possibilité de transition
des variables d'état visibles
est donné par l'équation (\ref{EQ_possV})
et celle des variables de croyance marginales
par l'équation (\ref{trans_marg_belief}).
\end{theorem}
En utilisant ce résultat, 
une telle expression factorisée
de la distribution de possibilité de transition
permet le calcul d'une fonction valeur avec $n=m+l$ étapes,
comme décrit dans la section précédente:
le $\pi$-PDMPO est en effet un $\pi$-PDM factorisé
puisque les variables $(S^1_v,\ldots,S^m_v,B^{\pi,1}_h,\ldots,B^{\pi,l}_h)$,
peuvent jouer le rôle des variables $X_1,\ldots,X_n$
dans l'algorithme \ref{PPUDD}.	

Notons que la relation d'indépendance probabiliste 
satisfait les axiomes des semi-graphoids:
ainsi, les résultats d'indépendance
due à la d-Separation sont aussi vrai pour le cadre probabiliste.
Si les hypothèse d'indépendance d'un PDMOM probabiliste \cite{OngShaoHsuWee-IJRR10,AraThoBufCha-ICTAI10} 
sont décrit par le DBN de la figure \ref{fig_piMOMDPFact},
alors une factorisation similaire en est déduite.
Le PDM construit à partir d'un tel PDMOM probabiliste
est donc un PDM factorisé, 
dont l'espace d'état est infini.

Les théorèmes précédent permettent
d'écrire la fonction de transition
d'un ($\pi$-)PDM résultant d'un ($\pi$-)PDMOM
avec des distributions qui concernent
moins de variables.
La fonction valeur 
La mise à jour de la fonction valeur
durant la programmation dynamique
est alors divisée
en $n=m+l$ étapes dans le cas possibiliste,
comme décrit par la boucle \textit{for} de l'algorithme \ref{PPUDD}.
Cette boucle permet de manipuler des ADDs
qui ont moins de n½uds
ce qui rend les calculs plus rapides en général.
Ces résultats sont utilisés dans la section suivante
afin de calculer de manière plus efficace
les stratégies optimales d'un $\pi$-PDMOM structuré.

\section{Résultats expérimentaux}
\label{section_expe_PPUDD}
Les calculs sont beaucoup plus simples dans le cadre possibiliste,
mais il fait évidemment en payer le prix:
les modèles possibilistes peuvent être considérés
comme des approximations
de modèles probabilistes.
Comme montré dans cette section,
une approximation possibiliste en utilisant PPUDD
peut mener à de meilleures stratégies,
au sens probabiliste,
que celles calculées par certain 
algorithmes probabilistes
lorsque la dimension du problème considéré
rend les calculs insurmontables.

\subsection{Missions Robotiques}

Nous avons comparé PPUDD sur le problème \textit{Rocksample} (RS),
décrit en section \ref{section_RS_motivatingEx}, 
contre un récent planificateur probabiliste pour PDMOM, 
APPL \cite{OngShaoHsuWee-IJRR10}, 
et un planificateur pour PDMPO utilisant des ADDs, 
symbolic HSVI \cite{Sim2008SHS1620163.1620241}.
Ces deux algorithmes peuvent être arrêtés à n'importe quel moment, 
et plus les calculs durent, 
plus la stratégie calculée est performante:
ainsi nous avons décidé d'arrêter 
les calculs lorsque l'erreur d'approximation
passe en-dessous de $1$.
La figure \ref{figureRS1},
où l'instance du problème 
augmente avec la complexité du problème,
montre que APPL déborde en mémoire à l'instance numéro $8$, 
et symbolic HSVI à l'instance numéro $7$,
tandis que PPUDD
peut calculer une stratégie pour
des instances beaucoup plus compliquées. 
\begin{figure}\centering
\begin{subfigure}[c]{.48\linewidth}
\includegraphics[width=\linewidth]{RockSampleCompTime.pdf}
\caption{Computation time (sec)}
\label{figureRS1}
\end{subfigure}
\begin{subfigure}[c]{.48\linewidth}
\includegraphics[width=\linewidth]{courbePerfTime.pdf}
\caption{Expected total reward}
\label{figureRS2}
\end{subfigure}
\vspace{0.5cm}
\caption[PPUDD vs. APPL and symb-HSVI, RockSample problem]{PPUDD vs. APPL and symb HSVI sur le problème RockSample: 
l'axe des abscisse représente l'indice de l'instance du problème, 
croissant avec la complexité de instance du problème.}
\end{figure}
Nous pouvons aussi fixer une durée de calcul
aux algorithmes probabilistes utilisés:
le temps de calcul d'APPL est alors fixé au temps de calcul de PPUDD,
et les performances de leurs stratégies respectives
sont comparées
en terme d'espérance de la somme des récompenses 
(et en utilisant les récompenses du modèle probabiliste). 
%For PPUDD, we
%simply evaluated with the probabilistic model the policy that was computed
% under
%possibilistic settings.
\'Etonnamment, la figure \ref{figureRS2} 
montre que les récompenses récupérées 
sont en moyenne plus grandes avec PPUDD
qu'avec APPL.
La raison est que APPL est en fait un planificateur 
probabiliste qui cherche à raffiner une approximation durant le temps de calcul, 
ce qui montre qui notre approche consistant à résoudre exactement
un modèle approximé peut mieux fonctionner que de résoudre de manière approchée
un modèle exact.
Enfin, nous pouvons noter que les probabilités du modèle d'observation, 
qui représente les incertitudes concernant les réponses des capteurs, 
peut être difficile à connaître précisément en pratique,
auquel cas les modèles possibilistes
peuvent plus physiquement rigoureux.

Ces résultats nous ont permis de juger pertinent
la participation de PPUDD
à la compétition internationale de planification probabiliste 2014,
même si le calcul de stratégie pour les modèles probabilistes
n'est pas la vocation initiale de ce planificateur.
La section suivante discute des résultats des différents solvers.

\subsection{Compétition de planification probabiliste internationale 2014}
La session entièrement observable
de la compétition internationale de planification probabiliste
permet de comparer des planificateurs de PDM
en garantissant les même ressources en terme de puissance de calcul
et de temps de calcul
à chacun des algorithmes participants.
Les planificateurs compétiteurs doivent calculer
des stratégies pour certains problèmes
qui ne sont pas connus à l'avance. 
\'Etant donné un de ces problèmes,
les planificateurs ont un temps limité
pour envoyer des actions à un serveur de la compétition
qui simule l'évolution de l'état du système: %% TODO
successive states are sampled by the competition server
using the transition probability 
distributions of the MDP defining the problem, 
and sent to a given competitor's solver.
For each system state received, the solver 
has to send back the action
it has computed.
These data exchanges are conducted 
during few trials of finite horizon and
the score of the solver for the considered problem 
is the average (over trials) 
of the undiscounted and finite sum of 
rewards along the trajectory generated by the trial.

Materials about this competition are available at the official web page of the competition
\url{https://cs.uwaterloo.ca/~mgrzes/IPPC_2014/}.
Problems are grouped in \textit{domains}, 
which are MDPs whose a finite number of parameters are undefined:
the problem, or MDP, used in practice during the competition
is an \textit{instance} of a domain, \textit{i.e.}
a domain whose parameters have been set.
In this competition, $8$ domains have been proposed,
called respectively
\textit{Academic advising}, \textit{Crossing traffic}, \textit{Elevators}, 
\textit{Skill teaching}, \textit{Tamarisk}, \textit{Traffic}, \textit{Triangle tireworld}
and \textit{Wildfire}.
Three possible encodings of the instances of these domains are proposed,
\textit{i.e.} three different languages can be used to describe the instances:
the first is the \textit{Planning Domain Definition Language} (PPDDL, \cite{Younes_ppddl1.0});
the second is a \textit{LISP}-like language introduced with symbolic algorithms such as SPUDD 
which defines explicitely transition probability distributions and reward function as ADDs
(see \url{http://users.cecs.anu.edu.au/~ssanner/IPPC_2011/}); 
finally, the third is the \textit{Relational Dynamic Influence Diagram Language} (RDDL, \cite{Sanner_relationaldynamic})
which is simpler and more expressive than the previous ones.
The competition consists in evaluating the solver over 
$10$ instances per domain with $30$ runs per instance and
$18$ minutes per instance: it takes $24$ hours in total.

In order to ensure that everyone has the same computational power,
each competitor solver is set up in a remote server whose RAM is $7.5$Gb 
with $2$ cores.% (``m3.large'', see \url{https://aws.amazon.com/ec2/pricing/}).
The client and server for the competition are available in the open source \textit{RDDLSim} software, 
which is available online at \url{http://code.google.com/p/rddlsim/}.
Four solvers have been proposed for this competition:
\begin{itemize}
\item \textit{PROST} \cite{DBLP:conf/aips/KellerE12}, based on \textit{Upper Confidence bound applied to Trees} (UCT, \cite{Kocsis:2006:BBM:2091602.2091633})
and using directly RDDL encoding;
\item \textit{GOURMAND} \cite{DBLP:conf/aaai/KolobovMW12,Kolobov12reverseiterative}, based on \textit{Labeled Real Time Dynamic Programming} (LRTDP, \cite{Bonet03labeledrtdp})
using PPDDL encoding;
\item \textit{symbolic LRTDP}, using ADDs and LISP-like encoding \cite{symbLRTDP};
\item our algorithm PPUDD, using LISP-like encoding too.
\end{itemize}

As the score given to solvers only depends on the $40$ first stages of the process,
the presented version of PPUDD consists of the Algorithm \ref{PPUDD} 
with the ``while condition'' $\overline{U^*} \neq \overline{U^c}$ at line \ref{while_PPUDD}
replaced by the condition ``iteration $\leqslant 40$''.
It also incrementally augments the planning
horizon while maintaining a mask stored in form of 
a Binary Decision Diagram (BDD, \textit{i.e.} an ADD with leaves in $\set{0,1}$) 
representing the states reachable from the initial state:
the computation of the current value function is then restricted
to the reachable states only. While PPUDD is an offline algorithm,
we proposed also \textit{AnyTime PPUDD} (ATPPUDD) 
which is an anytime version which learns computation times of
Bellman backups while dispatching the computational effort accordingly
over the remaining planning horizon much like GOURMAND does in the
probabilistic world (see \cite{DBLP:conf/aaai/KolobovMW12}). 

When encoded with the LISP-like format, 
problems of the competition, \textit{i.e.} instances of each domains, 
are described as factored MDPs
with boolean system state variables: 
for each action $a \in \mathcal{A}$ and for each next boolean system state variable $X_i'$, 
one ADD representing the corresponding transition probability distribution
$\textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'), a \Big)$ is given.
In order to define the $\pi$-MDP which will be solved by PPUDD,
we simply normalize these distributions in the possibilistic sense:
we set to $1$ the possibility degree of an assignment of $X_i'$ when 
its probability value is maximal, and to the probability value otherwise.
For instance, for a given assignment of the previous variables $parents(X_i')$, 
if the probability value of the assignment (or event) $X_i'=1$ is $0.7$ 
(and thus probability $0.3$ that $X_i' = 0$), 
then the possibility degree of $X_i'=1$ is set to $1$,
and the one of $X_i' = 0$ is set to $0.3$.

In terms of ADDs, it can be computed as follows:
let us first recall the notation  $\textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)^{X_i'=0}$,
used to represent the subtree of the ADD
$\textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)$ setting $X_i'$ to false (\textit{i.e.} to $0$).
As well, $\textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)^{X_i'=1}$ is the subtree of the same ADD, setting $X_i'$ to true (\textit{i.e.} to $1$).
Let us denote by $\mathds{1}_{\textbf{p}_{\top}>\textbf{p}_{\bot}}$ 
the BDD equal to $1$ for each variable assignment such that %$X_i'$ is true ($X_i'=\top$),
%and $0$ otherwise. 
%for each assignment of the variables of $parents(X_i')$ such that 
%\ovalbox{$\min$}
\[ \textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)^{X_i'=0} < \textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)^{X_i'=1}, \]
and equal to $0$ for other assignments.
The BDD always equal to $1$ is denoted by $\mathds{1}$. 
The BDD $\mathds{1}_{\textbf{p}=0.5}$ is equal to $1$
for variable assignments such that the probability of the event $X_i'=1$ (or $X_i'=0$) 
is equal to $0.5$, and this BDD is equal to $0$ otherwise.
We can also denote by
$\mathds{1}_{\textbf{p}_{\top}<\textbf{p}_{\bot}} $
the BDD which is equal to $1$ for assignments of variables in $parents(X_i')$ 
such that the probability of event $X_i' = 1$
is lower than the probability of event $X_i'=0$:
this BDD can be computed from previous BDDs,
$ \mathds{1} \ \ovalbox{$-$} \ \mathds{1}_{\textbf{p}_{\top}>\textbf{p}_{\bot}} \ \ovalbox{$-$} \ \mathds{1}_{\textbf{p}=0.5}$,
where $\ovalbox{$-$}$ is the minus operator, applied to trees.
The possibility transition distribution for the $i^{th}$ variable is
\begin{eqnarray*}
\pi \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big) = \ovalbox{$\max$} \Bigg\{ 	& \mathds{1}_{\textbf{p}=0.5}, 														& \\
				& \ovalbox{$\min$} \bigg\{ \mathds{1}_{\textbf{p}_{\top}>\textbf{p}_{\bot}}, \textbf{p} \Big( X_i' \Big\vert parents(X_i'), a \Big) \bigg\} , 	& \\
				& \ovalbox{$\min$} \bigg\{ \mathds{1}_{\textbf{p}_{\top}<\textbf{p}_{\bot}}  , \textbf{p} \Big( X_i' \Big\vert parents(X_i'), a \Big) \bigg\} 	& 	\Bigg\} 
\end{eqnarray*}

As well, for each action $a \in \mathcal{A}$, 
an ADD representing the reward function for this action 
is provided and denoted by $r(X_1,\ldots,X_n,a)$. 
Let us define then for each $s \in \mathcal{S}$ and $a \in \mathcal{A}$,
\[ \Psi(s,a) = \frac{ r(s,a) - \min_{s,a} r(s,a) }{ \max_{s,a} r(s,a) - \min_{s,a} r(s,a)} \in [0,1]. \]
The terminal preference function is set to $\Psi(s) = \max_{a \in \mathcal{A}} \Psi(s,a)$,
and the strategy is initialized by $\delta^*(s) \in \operatorname*{argmax}_{a \in \mathcal{A}} \Psi(s,a)$
at the beginning of the algorithm.

Note that possibility and preference degrees are not in a scale $\mathcal{L}$ 
as previously defined (\textit{i.e.} $\set{ 0, \frac{1}{k}, \frac{2}{k} \ldots, 1 }$ for some $k \geqslant1$).
Indeed, possibility degrees comes from probability values, 
and preferences are normalized rewards.
However, only $\max$ and $\min$ operators are used, 
so it has no impact on the qualitative results of the computations. 

The library used to perform computations with ADDs
is the \textit{CU Decision Diagram Package} 
(CUDD, \url{http://vlsi.colorado.edu/~fabio/CUDD/}),
and the described versions of PPUDD are available 
at the adress \url{https://github.com/drougui/ppudd}.

Following figures illustrates the results of IPPC 2014:
the score is given in function of the instance index,
which generally increases with the difficulty (or the size) 
of the associated problem.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random strategy, ``noop'' strategy, ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Academic advising} problem},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/academic_advising.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/academic_advising.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/academic_advising.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/academic_advising.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/academic_advising.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/academic_advising.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/academic_advising.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Crossing traffic} problem},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Results of IPPC 2014: \textit{Academic advising} and \textit{Crossing traffic} problems]{
Results of the International Probabilistic Planning Competition -- Fully Observable track}
\label{figure_IPPC_ACA_CRO}
\end{figure}

Figure \ref{figure_IPPC_ACA_CRO} presents the scores obtained by each solver 
for each of the $10$ instances of the \textit{Academic advising} domains,
\textit{i.e.} the average over $30$ trials of the sum of the encountered rewards.
Performances of our algorithms are close to the best ones.
However, an unexplained and unwanted bug occurred with ATPPUDD for the $2^{nd}$ instance, 
as only $3$ runs have been performed by this solver.
For the other instances, PPUDD and ATPPUDD produce strategies with performances like 
PROST and GOURMAND, and better than Symbolic LRTDP.
This has been less true for the \textit{Crossing traffic} problem, 
whose results are also described by Figure \ref{figure_IPPC_ACA_CRO}.
This problem models a robot which has to reach a goal 
which is across an highway with a lot of cars.
These cars arrive randomly and move left.
As the possibility degree of the fact that no car arrive 
is set to $1$ by our naive MDP to $\pi$-MDP translation,
the optimistic criterion leads to decide to cross the street,
even if an unseen car may arrive (with a probability $<0.5$ but bit enough to be cautious). 
This explain the poor quality of the produced strategies 
for this domain. Note however that, for the $6$ last instances (most difficult problems)
our approach leads to better strategies than the probabilistic solver Symbolic LRTDP.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random strategy, ``noop'' strategy, ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm,
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Elevators} problem}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/elevators_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/elevators_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/elevators_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/elevators_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/elevators_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/elevators_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/elevators_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Skill teaching} problem},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Results of IPPC 2014: \textit{Elevators} and \textit{Skill teaching problems}]{
Results of the International Probabilistic Planning Competition -- Fully Observable track}
\label{figure_IPPC_ELE_SKI}
\end{figure}

In the \textit{Elevators} problem, people arrive randomly and have to be transported 
to the correct building stage: 
as the frequentist information is lost using the possibilistic approach
and seems important in this problem (people do not want to wait once arrived), 
scores of our algorithms are poorer than the ones of PROST and GOURMAND.
The toy example at the beginning of the introduction of this chapter
illustrates that the possibilistic approaches can select actions probabilistically clearly suboptimal
when the probability values are at the heart of the problem.
PPUDD and ATPPUDD are however better than Symbolic LRTDP, as shown by Figure \ref{figure_IPPC_ELE_SKI},
and than doing nothing (``noop strategy'') or choosing actions randomly (``random strategy'').
PPUDD and ATPPUDD have quite good behaviours with the \textit{Skill teaching} problem 
as illustrated by the same figure. Moreover, ATPPUDD leads to better results
for the last three instances: as these instances are the \textit{Skill teaching} problems 
with the largest system space,  
the anytime version, which manages the computation time, 
produces strategies with better performances
than PPUDD, which classically solve the associated $\pi$-MDP, 
but cannot complete computations and lead to a poorer strategy.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Tamarisk} problem},
compat=newest,
legend entries={GOURMAND, PROST, Symbolic LRTDP, random strategy, ``noop'' strategy, ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/tamarisk.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/tamarisk.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/tamarisk.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/tamarisk.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/tamarisk.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/tamarisk.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/tamarisk.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Traffic} problem},
width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/traffic_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/traffic_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/traffic_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/traffic_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/traffic_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/traffic_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/traffic_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Results of IPPC 2014: \textit{Tamarisk} and \textit{Traffic} problems]{
Results of the International Probabilistic Planning Competition -- Fully Observable track}
\label{figure_IPPC_TAM_TRA}
\end{figure}


With respect to other solvers, possibilistic solvers have good results 
with the \textit{Tamarisk} domain, as shown in Figure \ref{figure_IPPC_TAM_TRA}..
However, some instances (e.g. the $6^{th}$, the $8^{th}$ and the $10^{th}$) 
are not even run as the ADD instantiation takes too long. 
Symbolic LRTP faces the same issue
as it uses also the LISP-like encoding of the problem.
We think that this is an issue specific to the competition,
as each problem has to be equivalently translated into
three different languages (RDDL, PPDDL and LISP-like),
which produces sometimes artificially complex
encodings of the problems.
The \textit{Traffic} domain is really hard to solve by PPUDD and ATPPUDD (see Figure \ref{figure_IPPC_TAM_TRA}). 
Actually the least scores are obtained with this domain,
and even the random and the noop strategies are better strategies.
Note that we did not implement any ``watchdog''
returning random actions when the computed strategy is less effective than the random one.
However, this kind of gadget is essential to improve results for such large and risky problem. 
As mentioned above for the \textit{Crossing traffic} problem, 
the optimistic criterion may lead to dangerous actions, as it does here.
Moreover, as this problem involves frequentist information (car arrivals)
an high suboptimality of the strategy produced by the possibilistic approach
is confirmed for this kind of problems (see the \textit{Elevator} problems). 
Finally, the \textit{Traffic} problem is known to be one of the hardest domain,
so ADD instantiation takes long, as well as computations, 
which are then not proceeded enough to produce satisfying results. 

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random strategy, ``noop'' strategy, ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm,
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Triangle tireworld} problem}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%PPUDD
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10}, width=11cm,height=11cm,
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Wildfire} problem}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/wildfire_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/wildfire_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/wildfire_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/wildfire_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/wildfire_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/wildfire_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/wildfire_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Results of IPPC 2014: \textit{Triangle tireworld} and \textit{Wildfire} problems]{
Results of the International Probabilistic Planning Competition -- Fully Observable track}
\label{figure_IPPC_TRI_WIL}
\end{figure}

Finally, the two last domains, whose results are described in Figure \ref{figure_IPPC_TRI_WIL},
are called \textit{Triangle Tireworld} and \textit{Wildfire}.
Firt, ATPPUDD faces an unexplained bug for each instance of the \textit{Triangle Tireworld} domain:
no trial is performed from numéro $5$ instance, and maximum $2$ trials are performed for other instances
(which explains the poor score for each instance).
As already mentioned for the \textit{Tamarisk} domain, 
ADD instantiation takes too long for the last instances,
and no trial is performed for the last $4$ instances 
with PPUDD too: Symbolic LRTDP faces the same issue. 
The last domain, called \textit{Wildfire}, leads to highly frequentist problems:
it involves random fire starts. 
That is why PPUDD and ATPPUDD strategies are not really efficient, 
but not too distant from Symbolic LRTDP solver's results.


\section{Conclusion} 
We presented PPUDD, the first algorithm to the best of our knowledge that 
solves factored possibilistic qualitative (MO)MDPs with symbolic calculations. In our opinion,
possibilistic models are a good tradeoff between non-deterministic ones, whose
uncertainties are not at all quantified yielding a very approximate model, and
probabilistic ones, where uncertainties are fully specified,
sometimes arbitrarily in practice. 
The resolution of planning problems 
using the framework of non-determinism 
is called \textit{contingent/conformant planning} 
studied for instance in \cite{Albore_atranslation-based,bonet2014flexible}.
By the way, in the introduction of this thesis,
we might also add ``non-determinism''
as a particular case of Possibility Theory
in Figure \ref{uncertainty_theories},
as values of associated non additive measures
are $0$ or $1$ instead of a more flexible scale $\mathcal{L}$.

Moreover,
$\pi$-MOMDPs reason about finite values in a qualitative scale $\mathcal{L}$ whereas
probabilistic MOMDPs deal with values in $\mathbb{R}$, which implies larger ADDs
for symbolic algorithms. Also, the former reduce to finite-state belief
$\pi$-MDPs contrary to the latter that yield \emph{continuous}-state belief MDPs
of significantly higher complexity. Our experimental results highlight the point that using an
exact algorithm (PPUDD) for an approximate model ($\pi$-MDPs) can bring significantly faster computations
than reasoning about complex exact models, while providing better
strategies than approximate algorithms (APPL) for exact models. 
In the future, we would like to developp a probabilistic algorithm using 
the generalization of our possibilistic belief factorization theory to
probabilistic settings (see Theorem \ref{thm_factoredPROBbelief}): 
related but sightly different results have been proposed for
probabilistic POMDPs \cite{DBLP:conf/aips/ShaniPBS08,Poupart:2005:phd}. 
These results also does not concern 
the case of mixed observability.

This chapter finally presents the results of our possibilistic approach
during IPPC 2014: 
the highlighted bottleneck of our possibilistic algorithms
resides on the translation from probabilities to possibilities: 
the naive automated translation presented before the description of the results 
leads to poor policies in benchmarks with complex dynamics and reward structures.
Another issue is the size of the input LISP-like encoded domains whose ADD
instantiation before optimization takes a very long time or 
does not even fit into memory for many difficult benchmarks:
this difficulty is shared with the Symbolic LRTDP solver.
However, there is almost no discretization of the initial probability values defining the MDP
in order to produce the possibility degrees
during the instantiation of the ADD defining the $\pi$-MDP:
the maximal difference between two possibility degrees is set to $10^{-3}$.
Stronger discretizations have not been tested yet, 
and could improve scores of our solvers for problems with such memory issues.
Modeling issues have been also highlighted, 
namely the fact that some problems request 
a cautious behaviour, not provided by the use of
the optimistic criterion (see Definition \ref{probstylerewrittenMOMDPcrit}) 
used during the competition. 
Moreover, as illustrated in introduction, 
these experiments show that problems with high entropy events 
are outperformed by probabilistic approaches since the possibilistic 
approach does not take into account the frequentist information about the problem.
The use of \textit{lexi}-approaches, as used in the following chapter, 
may be a possibilistic stratagem to get around this issue.
Note finally that the partially observable version of PPUDD 
(with the generation of a mask of reachable belief states, avoiding useless computations
on unreachable beliefs) is also available on the repository \url{https://github.com/drougui/ppudd}.

The next chapter, Chapter \ref{chap_IHM}, deals with \textit{Human-Machine Interaction} (HMI)
problems: the uncertainty dynamics of the system are in this context typically not known in terms
of probability values, and the qualitative possibilistic approach is shown to be a natural approach
to produce efficient diagnosis of human errors.

Finally, the last chapter, Chapter \ref{chap_hybrid},
takes into account the remarks made using the results of IPPC14:
an approach using Probability and Possibility Theory
in order to benefit from both approaches in the resolution of factored POMDPs
is presented: quantitative information of the problem is kept to avoid the highlighted modeling issues,
and the belief state is handled in a possibilistic way, in order to get a smart discretization of it
and to benefit from a finite and factorized belief state spaces.
This approach leads to a factored probabilistic MDP 
which can be solved for instance by GOURMAND or PROST
(which does not use the memory constraining LISP-like encoding).
