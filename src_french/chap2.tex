Dans ce chapitre, nous proposons
l'étude des modèles $\pi$-PDMOM 
dans le but de résoudre 
de très grands problèmes de planification
lorsqu'ils sont structurés.
Inspirés par l'algorithme \textit{Stochastic Planning Using Decision Diagrams} (SPUDD)
construit pour résoudre les PDM probabilistes factorisés, 
nous avons mis en place un algorithme symbolique
appelé PPUDD conçu pour résoudre les $\pi$-PDMOM. 
Tandis que le nombre de feuilles
des arbres de décisions utilisés par SPUDD
peuvent devenir aussi grand que la taille de l'espace des états,
puisque leurs valeurs sont des nombres réels 
agrégés par des additions et des multiplications,
le nombre de feuilles de PPUDD 
est borné par le nombre d'éléments dans $\mathcal{L}$
car leurs valeurs restent dans l'échelle finie $\mathcal{L}$
via les opérations $\min$ et $\max$ seulement.
Enfin, nous présentons un $\pi$-PDMOM
satisfaisant certaines hypothèses d'indépendance 
sur ces variables visibles, cachées,
et d'observation.
Ce dernier résulte en un $\pi$-PDM factorisé,
sur lequel PPUDD peut être lancé.
Nos résultats expérimentaux montrent que 
le temps de calcul de PPUDD 
est beaucoup plus petit que 
Symbolic-HSVI et APPL
pour les versions possibilistes 
et probabilistes des mêmes benchmarks, 
tout en fournissant des stratégies de bonne qualité.
Les performances des stratégies
calculées par PPUDD ont été testées
pendant la compétition internationale de planification probabiliste (IPPC 2014)
dont les résultats sont exposés ici.

\section{Introduction}
Les travaux sur les $\pi$-PDM(MO)
présentés précédemment ne tirent pas totalement avantage
de la structure du problème, \textit{i.e.}
les parties visibles ou cachées de l'état peuvent être elles-même
factorisées en plusieurs variables d'états. 
Dans le cadre probabiliste, 
les PDM factorisés et les méthodes de calcul symboliques 
\cite{BoutilierDG00,Hoey99spuddstochastic} ont été étudiés
intensivement dans le but de raisonner directement au niveau des variables
plutôt que sur l'espace d'état.
Un travail récent sur ces problématiques est par exemple \cite{RadoszyckiPS14}. 
Le célèbre algorithme SPUDD
\cite{Hoey99spudd:stochastic} 
résout des PDM factorisés en utilisant
des représentation symboliques de la fonction valeur et des stratégies
sous la forme d'arbre de décision algébriques (ADDs) \cite{Bahar1997ADD}, 
qui représentent de manière compacte
les fonctions réelles de variables booléennes:
les ADDs sont des arbres 
dont les noeuds représentent les variables d'état
et les feuilles sont les valeurs de la fonction. 
Au lieu de mettre à jour les valeurs de la fonction pour chaque état
individuellement à chaque itération de l'algorithme,
ils sont aggrégés dans les ADDs
et les opérations sont effectués de manière symbolique
directement entre les ADDs sur plusieurs variables en même temps.
Cependant, SPUDD est limité par la potentielle manipulation
d'énormes ADDs dans le pire des cas: par exemple,
l'espérance implique des additions et des multiplications sur des valeurs réelles
(probabilités et récompenses), 
créant de nouvelles valeurs entres elles,
de manière à ce que le nombre de feuille des ADDs
peut devenir égal à l'espace d'état,
\textit{i.e.} exponentiel en le nombre de variables d'état.

Ainsi,
le travail présenté ici
est motivé par la simple observation
que les \textbf{opérations symboliques avec des PDM possibilistes
devraient nécessairement limiter la taille des ADDs}: 
en effet, ce formalisme opère sur une échelle \emph{finie}
$\mathcal{L}$ avec seulement les opération $\max$ et $\min$,
ce qui implique que les valeurs manipulée restent dans
l'échelle finie $\mathcal{L}$, 
qui est généralement beaucoup plus petite
que le nombre d'états.
\begin{figure} \centering
\begin{tikzpicture}
\begin{axis}[grid=major,xmax=100,
legend entries={ $8$ variables: cadre qualitatif, cadre quantitatif,  
$10$ variables: cadre qualitatif, cadre quantitatif},legend style={at={(2,0.6)}},
xlabel={Taille de l'échelle $\mathcal{L}$},
ylabel={Nombre maximal de noeuds},
title={Nombre maximal de noeud d'un ADD: feuilles dans $\mathcal{L}$ vs dans $\mathbb{R}$}]
\addplot table[x=scaleSize, y=MaxNbNodesPoss]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_8VARS.txt};
\addplot table[x=scaleSize, y=MaxNbNodes]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_8VARS.txt};
\addplot table[x=scaleSize,y=MaxNbNodesPoss]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_10VARS.txt};
\addplot table[x=scaleSize,y=MaxNbNodes]{ADDsizeFctLsize/src/MAXnbNodes_fctOf_Lsize_10VARS.txt};
\end{axis}
\end{tikzpicture}
\caption[Limitations de la taille maximale d'un ADD dans le cadre qualitatif]{
La taille maximale (nombre total de noeuds)
d'un ADD dont les valeurs sont dans $\mathcal{L}$ is limited:
cette taille maximale est représentée par les courbes avec les ronds bleus et marrons,
en fonction de la taille de $\mathcal{L}$.
Lorsque les feuilles de ADDs sont dans $\mathbb{R}$,
le nombre de ses noeuds est potentiellement exponentiel
en le nombre de variables:
la borne supérieure est représentée par les courbes
avec les carrés rouges et noirs.
(fonctions constantes de la taille de $\mathcal{L}$).}
\label{ADDsize}
\end{figure}

La figure \ref{ADDsize} montre que
les ADDs utilisés dans le cadre possibiliste
a un nombre limité de noeuds
puisque le nombre de feuilled
est au plus égal au cardinal de
l'échelle possibiliste qualitative $\mathcal{L}$:
%which is generally far smaller than the number of states.  
%Figure \ref{ADDsize} shows 
la taille maximale (
nombre maximal de noeud)
d'un ADD
dont les feuilles sont dans $\mathcal{L}$, 
est représenté comme une fonction
de $\# \mathcal{L}$,
dans le cas de $8$ et $10$ variables. 

Dans ce chapitre, nous présentons
un algorithme basé sur la programmation dynamique symbolique
pour résoudre les $\pi$-PDMMO factorisés
appelé Possibilistic Planning Using
Decision Diagram (PPUDD). 
Cette contribution seule
n'est pas suffisante
puisque les variables de croyance
ont un nombre de valeurs
exponentiel en la taille de l'espace des états cachés.
Donc, notre seconde contribution est un théorème 
pour factoriser l'état de croyance
en de nombreuses variables de croyance marginales
lorsque certaines hypothèses d'indépendance 
sur les variables d'état et d'observation
d'un $\pi$-PDMOM sont vérifiées:
cela permet de résoudre certains problèmes
dont les calculs sont inabordable tels quels. 
Notons que notre idée de factorisation
de l'état de croyance est assez général
pour être valable pour les modèles probabilistes.
Enfin, les performances de PPUDD sont comparées à celles de symbolic
HSVI \cite{Sim2008SHS1620163.1620241}
(une version symbolique de l'algorithme pour PDMPO called HSVI) 
et APPL \cite{Kurniawati-RSS08,OngShaoHsuWee-IJRR10}
(déjà utilisé dans le chapitre précédent, 
et basé sur SARSOP)
sous observabilité mixte.
Les résultats obtenus étant prometteurs,
nous avons participé à la compétition internationale de planification probabiliste (IPPC 2014)
et les résultats de PPUDD durant la session entièrement observable d'IPPC 2014
sont présentés et discutés.
Un algorithme dédié à la résolution des $\pi$-PDMOM en utilisant des ADDs
est disponible dans le dépôt \url{https://github.com/drougui/ppudd}).

\section{Résoudre des $\pi$-PDMOM en utilisant la programmation dynamique symbolique} 
\label{section_PPUDD}
Les PDM factorisés \cite{Hoey99spuddstochastic} 
ont été utilisés pour résoudre plus rapidement les
problèmes structurés de décision séquentielle, 
sous incertitude probabiliste, 
en raisonnant symboliquement sur les fonctions
des états du systèmes, à travers des arbres de décision algébriques. 
Inspiré par ce travail,
cette section présente la résolution symbolique
des $\pi$-PDMOM factorisés:
dans ce modèle, l'espace des états visibles $\mathcal{S}_v$, 
l'espace des états cachés $\mathcal{S}_h$
et l'ensemble des observations $\mathcal{O}_h$
sont tels que l'espace d'état du $\pi$-PDM résultant 
(basé sur l'espace des états de croyances et l'espace des variables visibles)
est sous la forme 
$\mathcal{S}^1_v \times \cdots \times \mathcal{S}^m_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$, 
où chacun de ces espaces sont finis.
Nous verrons dans la section suivante comment $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
peut être factorisé de cette manière
grâce à la factorisation de $\mathcal{S}_h$
et $\mathcal{O}_h$. 
La factorisation de la variable de la croyance probabiliste
dans \cite{DBLP:conf/aaai/BoyenK99,DBLP:conf/aips/ShaniPBS08} 
est approximative, tandis que celle présentée ici est exacte.
Puisque les espaces finis de taille $K$
peuvent être eux-même factorisés
en $\lceil \log_2 K \rceil$ espaces binaires \cite{Hoey99spudd:stochastic}, 
nous poubons faire l'hypothèse que nous raisonnons 
sur un $\pi$-PDM dont l'espace d'état est noté $\mathcal{X}$
est entièrement décrit par les variables $(X^1,\ldots,X^n)$, 
avec $n \in \mathbb{N}^{\ast}$ et $\forall i$, $X^i \in \set{\top,\bot}$:
$\mathcal{X} = \set{\top,\bot}^n$.

Rappelons que les réseaux bayésiens dynamiques (DBNs) \cite{Dean:1989:DBN}
déjà utilisés en section \ref{section_Markov2POMDP} 
(par exemple dans le diagramme d'influence figure \ref{POMDP})
et dans le chapitre précédent (figure \ref{piMOMDP} 
illustrant la structure d'observabilité mixte)
sont des représentations graphique très utiles pour les processus étudiés. 
Un DBN représentant la structure d'un $\pi$-PDM factorisé
est dessiné dans la figure \ref{fig_piMDPFact}:
les variables d'état à une étape de temps donnée $t \geqslant 0$
sont notées $X_t = (X_t^i)_{i=1}^n$ (variables courantes),
et $(X^i_{t+1})_{i=1}^n$ sont les variables d'état à l'étape de temps $t+1$ (variable suivante).
\begin{figure}\centering
\begin{tikzpicture}[scale=1.5,transform shape]
%% vertex shape and color
\tikzstyle{vertex}=[circle,fill=black!30,minimum size=27pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red!60,minimum size=22pt,inner sep=0pt,draw=black,thick]
\tikzstyle{rvertex}=[fill=yellow!60,decision=3,inner sep=-1pt,minimum size=35pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=35pt,inner sep=0pt,draw=black,thick]
%TIME
%\node [font=\huge] (statet) at (4,0.2) {$t$};
%\node [font=\huge] (statetplus1) at (9.2,0.2) {$t+1$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1
\node[vertex] (state1) at (3.3,3) {$X^1_t$};
\node[vertex] (state12) at (3.3,1) {$X^2_t$};
\node (state13) at (3.3,0.2) {\begin{Large} $\vdots$ \end{Large}};
%2
\node[vertex] (state2) at (8,3) {$X^1_{t+1}$};
\node[vertex] (state22) at (8,1) {$X^2_{t+1}$};
\node (state23) at (8,0.2) {\begin{Large} $\vdots$ \end{Large}};

%0
\node (state0) at (0.5,3) {};
\node (state02) at (0.5,1) {};
%3
\node (state3) at (11,3) {};
\node (state32) at (11,1) {};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%action
\node[avertex] (action) at (5.7,-1) {$a_t$};
\node[avertex] (action0) at (1.3,-1) {$a_{t-1}$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS

%SV
% arrows sv->sv
%1->2
\draw[->,>=latex] (state1) -- (state2);
\draw[->,>=latex] (state1) -- (state22);
\draw[->,>=latex] (state12) -- (state2);
\draw[->,>=latex] (state12) -- (state22);
%0->1
\draw[->,>=latex,dashed] (state0) -- (state1);
\draw[->,>=latex,dashed] (state0) -- (state12);
\draw[->,>=latex,dashed] (state02) -- (state1);
\draw[->,>=latex,dashed] (state02) -- (state12);
%2->3
\draw[->,>=latex,dashed] (state2) -- (state3);
\draw[->,>=latex,dashed] (state2) -- (state32);
\draw[->,>=latex,dashed] (state22) -- (state3);
\draw[->,>=latex,dashed] (state22) -- (state32);

%A
% a->s
%1
\draw[->,>=latex] (action) -- (state2);
\draw[->,>=latex] (action) -- (state22);
%0
\draw[->,>=latex,dashed] (action0) -- (state1);
\draw[->,>=latex,dashed] (action0) -- (state12);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRANSITION FUNCTIONS
\node (T1) at (6.4,2.7) {$T^1_a$};
\node (T2) at (6.4,1.3) {$T^2_a$};
\end{tikzpicture}
\caption[Réseau bayésien dynamique d'un ($\pi$-)PDM factorisé]
{Réseau dynamique bayésien pour des ($\pi$-)PDM:
dans le cadre possibiliste (resp. probabiliste) 
$T^i_a$ est la distribution de possibilité (resp. probabilité) de transition
sur la variable d'état $X^i_{t+1}$
conditionnellement à
l'action sélectionnée $a \in \mathcal{A}$
et à ses parents $parents(X^i_{t+1}) \subseteq \set{X^1_t,\ldots,X^n_t}$
(\textit{i.e.} $parents(X^i_{t+1})$ est un sous ensemble de l'ensemble des variables d'état courantes)
où $n\geqslant1$ est le nombre de variables décrivant l'espace d'état.
}
\label{fig_piMDPFact}
\end{figure}
Dans le cadre des DBNs, $parents(X^i_{t+1})$ 
est l'ensemble des variables d'état
desquelles la variable d'état suivante $X^i_{t+1}$ dépend,
\textit{i.e.} une variable $Y$, 
représenté par un noeud dans le DBN,
est dans $parents(X^i_{t+1})$ 
si et seulement si il y a une flèche de $Y$ à $X^j_{t+1}$.
Nous supposons que $parents(X^i_{t+1}) \subseteq \set{X^1_t,\ldots,X^n_t}$,
\textit{i.e.} les parents de la variable d'état suivante $X^i_{t+1}$
font partie des variables d'état courantes $\set{X^1_t,\ldots,X^n_t}$:
il ne peut y avoir de flèches entre
les variables d'état de la même étape de temps.

Avec les notations du $\pi$-PDMOM, 
les hypothèses du réseau bayésien
de la figure \ref{fig_piMDPFact}
nous permettent de calculer la distribution de possibilité jointe: 
$\pi \paren{s_v',\beta_h' \sachant s_v,\beta_h,a} = \pi \paren{ X' \sachant X,a } = \min_{i=1}^{n} \pi \paren{ X_i' \sachant parents(X_i'),a}$,
où, étant donné l'étape de temps $t$,
les variables primées sont les variables concernant l'étape de temps $t+1$
(next variables), and non-primed variables are current variables 
(at time step $t$): for instance, 
$X_i'$ is the notation for $X^i_{t+1}$,
and $X_i$ the one for $X_t^i$.
Thus, a factored $\pi$-MOMDP can be defined 
with transition functions $T^i_a= \pi \paren{X_i' \sachant parents(X_i'),a }$ 
for each action $a$ and variable $X_i'$
(if transitions are assumed stationary).
% such that $T^{a,i}(parents(X_i'),a,X_i') = \pi \paren{\mathbb{X}_i' \sachant parents(\mathbb{X}_i'),a }$.

Each transition function can be compactly encoded in an Algebraic Decision
Diagram (ADD) \cite{Bahar:1997:ADD}. 
An ADD, as illustrated in Figure \ref{fig_ADDtransition}, 
is a directed acyclic graph which compactly represents a
real-valued function of binary variables, whose identical sub-graphs are
merged and zero-valued leaves are not memorized. 
The following notations are used to make it 
explicit that we are working with symbolic functions encoded as ADDs:
\begin{itemize}
\item $\ovalbox{$\min$} \set{f,g}$ where $f$ and $g$ are 2 ADDs;
\item $\ovalbox{$\max$}_{X_i} f$ = $\ovalbox{$\max$} \set{ f^{X_i=0},f^{X_i=1} }$, 
\end{itemize}
which can be easily computed because ADDs are constructed on the basis of the 
Shannon expansion: $f = \overline{X_i} \cdot f^{X_i=0} + X_i \cdot f^{X_i=1}$ 
where $f^{X_i=1}$ and $f^{X_i=0}$ are sub-ADDs representing the positive and 
negative Shannon cofactors (see Fig. \ref{fig_ADDtransition}). 

The optimistic possibilistic update of dynamic programming, 
\emph{i.e.} line \ref{VIupdate_OptAlgo} 
of the Value Iteration Algorithm \ref{algorithmIVPIMDP} 
in the previous chapter,
(or line \ref{piMOMDP_VIupdate} of the VI Algorithm \ref{algorithmpiMOMDP} for $\pi$-MOMDPs)
%equation of Line \ref{alg_piMDP_DP} of Algorithm \ref{algorithmIVPIMDP}
can be rewritten in a symbolic form, so that states are
now globally updated at once instead of individually: 
denoting by $X=(X_1,\ldots,X_n)$ the current variable
and $X'=(X_1',\ldots,X_n')$ the next one,
the possibilistic Q-value of an action $a \in \mathcal{A}$
%in a state $x \in \mathcal{X}$,
is $\overline{q^a} = \overline{q^a}(X)= \ovalbox{$\max$}_{X'} \ovalbox{$\min$} \set{ \pi \paren{ X' \sachant X,a }, \overline{U^*}(X') }$.
The computation of this ADD ($\overline{q^a}$) can be decomposed into independent computations 
using the following proposition:
\begin{Property}[Possibilistic regression of the Value Function]
\label{propositionRegression}
Consider the current value function
 $\overline{U^*}: \set{\top,\bot}^n \rightarrow \mathcal{L}$. 
For a given action $a \in \mathcal{A}$, let us define:
\begin{itemize}
\item $\overline{q^a_0} = \overline{U^*}(X'_1,\cdots,X'_n)$, 
\item $\overline{q^a_i} = \max_{X'_i \in \set{\top,\bot}} \min \Big\{ \pi \paren{X_i' \sachant parents(X_i'),a} , \overline{q^a_{i-1}} \Big\}$.
\end{itemize}
Then, the possibilistic Q-value of action $a$ is: $\overline{q^a} = \overline{q^a_n}$,
which depends on variables $X_1,\ldots,X_n$,
and the next value function is $\overline{U^*}(X_1,\ldots,X_n) = \max_{a \in \mathcal{A}} \overline{q^a_n}(X_1,\ldots,X_n)$.
\end{Property}
The proof is given in Annex \ref{propositionRegression_RETURN}.
Note that the same trick can be used to compute 
pessimistic value functions,
using the equation (\ref{equationmaxmin4}) of Property \ref{property_minmax}: 
\begin{itemize}
\item $\underline{q^a_0} = \underline{U^*}(X'_1,\cdots,X'_n)$, 
\item $\underline{q^a_i} = \min_{X'_i \in \set{\top,\bot}} \max \Big\{ 1 - \pi \paren{X_i' \sachant parents(X_i'),a} , \underline{q^a_{i-1}} \Big\}$,
\end{itemize}
and next value function is $\underline{U^*}(X_1,\ldots,X_n) = \max_{a \in \mathcal{A}} \underline{q^a_n}(X_1,\ldots,X_n)$.

The Q-value of action $a$, represented as an ADD, can be then 
iteratively regressed over successive post-action state variables $X'_i ,
1 \leqslant i \leqslant n$. 
\begin{figure}
\centering
\begin{subfigure}[b]{0.35\linewidth}
\flushleft
\begin{tikzpicture}
\draw (-1.2,5.5) rectangle (2.8,7.3);
\draw (0.75,7) node (key) {KEY};
\draw (0.25,5.5) node (Lt) {};
\draw (1.25,5.5) node (Lf) {};
\draw (key) -- (Lt) node [left,midway] {$\top$ (true)};
\draw [dashed] (key) -- (Lf) node [right,midway] {$\bot$ (false)};

\draw (1,4.5) node (Xp1) {$X_1'$};
\draw (0,0.5) node (L3) {$1$};
\draw (1.5,3.2) node (X11) {$X_1$};
\draw (2,2) node (X2) {$X_2$};
\draw (1,0.5) node (L1) {$\frac{2}{3}$};
\draw (2.6,0.5) node (L2) {$\frac{1}{3}$};
\draw [dashed] (Xp1) -- (X11);
\draw (X11) -- (L1);
\draw [dashed] (X11) -- (X2);
\draw (X2) -- (L1);
\draw [dashed] (X2) -- (L2);
\draw (Xp1) -- (L3);
\end{tikzpicture}
\caption{ADD encoding $T^a_1$ of Fig. \ref{fig_piMDPFact}\\\\}
\label{fig_ADDtransition}
\end{subfigure}
\qquad
\begin{subfigure}[b]{0.58\linewidth}
\flushright
\begin{tikzpicture}

\draw (-2,3) node {\ovalbox{$\min$} $\Bigg\{$};

\draw (0,4.5) node (Xp1) {$X'_1$};
\draw (1,3) node (Xp2) {$X'_2$};
\draw (-1,1.5) node (Lp1) {$\frac{1}{3}$};
\draw (0.5,1.5) node (Lp2) {$\frac{2}{3}$};
\draw (2,1.5) node (Lp3) {$0$};
\draw (Xp1) -- (Lp1);
\draw [dashed] (Xp1) -- (Xp2);
\draw (Xp2) -- (Lp2);
\draw [dashed] (Xp2) -- (Lp3);

\draw (2.2,3) node {,};

\draw (4,4.5) node (Xp1) {$X_1'$};
\draw (3,1.5) node (L3) {$1$};
\draw  (Xp1) -- (L3);
\draw (4.6,3.5) node (X11) {$X_1$};
\draw (5.2,2.5) node (X2) {$X_2$};
\draw (4,1.5) node (L1) {$\frac{2}{3}$};
\draw (5.7,1.5) node (L2) {$\frac{1}{3}$};
\draw (X11) -- (L1);
\draw [dashed] (X11) -- (X2);
\draw (X2) -- (L1);
\draw [dashed] (X2) -- (L2);
\draw [dashed] (Xp1) -- (X11);


\draw (6.3,3	) node {$\Bigg\}$};



\draw (-2,-1.3) node {=};
%\draw (-0.5,-1.3) node {=};


\draw (-0.7,0.5) node (rXp1) {$X'_1$};
\draw (0.5,-0.2) node (rXp2) {$X'_2$};
\draw (-0.2,-1.2) node (rX1) {$X_1$};
\draw (0.7,-2) node (rX2) {$X_2$};
\draw (-0.5,-3.2) node (rL1) {$\frac{2}{3}$};
\draw (1.3,-3.2) node (rL2) {$\frac{1}{3}$};
\draw (-1.5,-3.2) node (rL3) {$\frac{1}{3}$};
\draw (2,-3.2) node (rL4) {$0$};

\draw [dashed] (rXp1) -- (rXp2);
\draw  (rXp2) -- (rX1);
\draw [dashed] (rXp2) -- (rL4);
%\draw (rXp2) -- (rX1);
%\draw [dashed] (rXp2) -- (rL3);
\draw (rXp1) -- (rL3);
\draw (rX1) -- (rL1);
\draw [dashed] (rX1) -- (rX2);
\draw (rX2) -- (rL1);
\draw [dashed] (rX2) -- (rL2);

%\draw (0.3,-2) node (dL1) {$\frac{1}{3}$};
%\draw (rXp1) -- (rL1);

\draw (2.5,-1.3) node {$\xrightarrow[\text{\ovalbox{$\max$}}_{X'_1}]{}$};

%\draw (4.5,-2) node (rrXp2) {$\mathbb{X}'_2$};
\draw (4.8,0.5) node (rrpX2) {$X_2'$};
\draw (4,-0.5) node (rrX1) {$X_1$};
\draw (4.5,-1.7) node (rrX2) {$X_2$};
\draw (3.5,-3.2) node (rrL1) {$\frac{2}{3}$};
\draw (5.7,-3.2) node (rrL2) {$\frac{1}{3}$};
\draw (rrpX2) -- (rrX1);
\draw (rrX1) -- (rrL1);
%\draw (rrXp2) -- (rrX1);
\draw [dashed] (rrX1) -- (rrX2);
\draw (rrX2) -- (rrL1);
\draw [dashed] (rrX2) -- (rrL2);
\draw [dashed] (rrpX2) -- (rrL2);

\end{tikzpicture}
\caption{Symbolic regression of the current Q-value ADD combined with the transition ADD of Figure \ref{fig_ADDtransition}\\}
\label{fig_ADDregression}
\end{subfigure}
\caption{Algebraic Decision Diagrams for PPUDD}
\label{fig_ADD}
\end{figure}	
Figure \ref{fig_ADDregression} illustrates the possibilistic regression of the
Q-value of an action for the first state variable $X_1$ and leads to the intuition that ADDs should be far 
smaller in practice under possibilistic settings, since their leaves lie in $\mathcal{L}$ instead
of $\mathbb{R}$, thus yielding more sub-graph simplifications.


\begin{algorithm} \caption{PPUDD (infinite horizon resolution)} \label{PPUDD} 
 $\overline{U^*} \gets 0$ ;
 $\overline{U^c} \gets \Psi$ ;
 $\overline{\delta} \gets \widehat{a}$ \;

\While {$\overline{U^*} \neq \overline{U^c}$ \label{while_PPUDD} }{
 $\overline{U^*} \gets \overline{U^c}$ \;
 \For {$a \in \mathcal{A}$ \label{ppuddBeginQ}}{
	$\overline{q^a}  \gets $ swap each $X_i$ variable in $\overline{U^*}$ with $X_i'$ \label{ppuddSwap} \;
	\For {$1 \leqslant i \leqslant n$}{
	 $\overline{q^a} \gets \ovalbox{$\min$} \ \bigg\{ \overline{q^a} , \pi \Big( X_i' \ \Big\vert parents(X_i'),a \Big) \bigg\}$ \;
	 $\overline{q^a} \gets \ovalbox{$\max$}_{X_i'} \overline{q^a}$ \;
	}
	$\overline{U^c} \gets \ovalbox{$\max$} \set{\overline{q^a},\overline{U^c}  } $ \label{ppuddEndQ} \;
	update $\overline{\delta}$ to $a$ where $\overline{q^a}=\overline{U^c}$ and $\overline{U^c} > \overline{U^*}$ \;
}
}
\Return $\overline{U^*}$, $\overline{\delta^*}$ \;
\end{algorithm}

Algorithm \ref{PPUDD} is a symbolic version of the $\pi$-MOMDP Value Iteration Algorithm (Algorithm \ref{algorithmpiMOMDP} in the previous chapter),
which relies on the regression scheme defined in Proposition
\ref{propositionRegression}. Inspired by SPUDD \cite{Hoey99spudd:stochastic},
PPUDD means \emph{Possibilistic Planning Using Decision Diagrams}. As for SPUDD,
it needs to swap unprimed state variables to primed ones in the ADD encoding the
current value function before computing the Q-value of an action $a$ (see Line
\ref{ppuddSwap} of Algorithm \ref{PPUDD} and Figure \ref{fig_ADDregression}). This operation is required to
differentiate the next state represented by primed variables from the
current one when operating on ADDs. Lines \ref{ppuddBeginQ}-\ref{ppuddEndQ} apply
Proposition \ref{propositionRegression} and correspond to Line
\ref{VIupdate_OptAlgo} of Algorithm \ref{algorithmIVPIMDP}.

We mentioned at the beginning of this section 
that belief state space $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
could be described by $\lceil \log_2 K \rceil$ binary variables where $K=\#
\mathcal{L}^{\#\mathcal{S}_h} - (\# \mathcal{L}-1)^{\# \mathcal{S}_h}$. 
However, this $K$ can be
very large so we propose 
in the next section a method to exploit the
factorization of $\mathcal{S}_h$ and $\mathcal{O}_h$ 
in order to factorize $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$
itself into small belief subvariables, which will decompose the possibilistic
transition ADD into an aggregation of smaller ADDs.
Note that PPUDD can solve $\pi$-MOMDPs even if this belief factorization is not 
feasible, but it will manipulate larger ADDs.

The previous chapter highlight that 
a pre-treatment is required to translate
a $\pi$-MOMDP into a $\pi$-MDP whose state space is $\mathcal{X}$.
We can then reason on the state space accessible to the agent 
$\mathcal{X} = S_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$ and
solve the $\pi$-MOMDP as a $\pi$-MDP.
Next section links the structured properties of a $\pi$-MOMDP,
concerning dependencies of original variables (visible, hidden and observation ones), 
to the factorization of the treated problem \textit{i.e.} of the resulting $\pi$-MDP 
on $S_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}}$ (concerning dependencies of visible and belief variables).

Finally, we note that we could have used \emph{complete action diagrams} (CADs)
introduced in \cite{St-aubin00apricodd:approximate}, 
which directly encode the
transition matrix of each action as a single ADD.
On one hand, CADs are simpler
to manipulate than a set of transition ADDs for each state variable, 
and can deal with correlated action effects. On the other hand,
they require operating larger ADDs while preventing intermediate simplifications that are yet
offered by reasoning about separate state variables as we do (Lines
\ref{ppuddBeginQ}-\ref{ppuddEndQ} of Algorithm \ref{PPUDD}) or SPUDD does
\cite{Hoey99spudd:stochastic}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Factorisation de la variable de croyance d'un $\pi$-PDMOM}
%\subsection{Possibilistic Bayesian Networks and Independences}
%\subsection{The $\pi$-MOMDP case}
Factorizing the belief variable requires three structural assumptions on the 
$\pi$-MOMDP's DBN, which are illustrated by the Rocksample benchmark 
\cite{Smith:2004:HSV:1036843.1036906}.
\subsection{Exemple de motivation}
\label{section_RS_motivatingEx}
A rover navigating in a $g \times g$ grid has to collect scientific samples from 
interesting (``good'') rocks among $R$ ones and then to reach the exit.
% It is 
%equipped with a noisy long-range sensor that can be used to determine if a rock 
%is ``good'' or not.
It knows the locations of the $R$ rocks $(x_i,y_i)_{i=1}^R$ 
but not which ones are actually of interest (called ``good'' rocks). 
However, sampling a rock is expensive: 
the rover is equipped with a noisy long-range sensor that can be used to determine 
if a rock is ``good'' or not (``bad''). 
When a rock is sampled, 
it becomes (or stays) ``bad'' (no more interesting). 
At the end of the mission, the rover has to reach the exit location at the right side of the grid:
\begin{itemize}
\item $\mathcal{S}_{v}$ consists of all the possible locations of the rover %$(x_r,y_r)$ 
in addition to the exit ($\# \mathcal{S}_v = g^2 +1$);
\item $\mathcal{S}_h$ consists of all the possible natures of the rocks:
$\mathcal{S}_h = \mathcal{S}^1_h \times \ldots \times \mathcal{S}^R_h$,
with $\forall 1 \leqslant i \leqslant R$, $\mathcal{S}^i_h=\set{ good,bad }$;
\item $\mathcal{A}$ contains the (deterministic) moves in the $4$ directions ($a_{north},a_{east},a_{south},a_{west}$)
, checking rock $i$, ($a_{check_i}$) 
$\forall 1 \leqslant i \leqslant R $, 
and sampling the current rock, ($a_{sample}$);
\item $\mathcal{O} = \set{ o_{good},o_{bad} }$ are the possible sensor's answers for the current rock. 
% $\mathcal{O}_{1} \times \ldots \times \mathcal{O}_{R}$ where $\forall 1 \leqslant i \leqslant R$, $\mathcal{O}_{i}=\set{ o_{good},o_{bad} }$ are observations concerning the $i^{th}$ rock. \\
\end{itemize}

The rationale behind observation dynamics is the following: 
the more the rover is close to
the checked rock, the better it observes its nature. 
In the original
probabilistic model, 
the probability of a correct observation equals
$\frac{1}{2}\paren{ 1 + e^{-c \sqrt{(x_r-x_i)^2 + (y_r-y_i)^2} }} $ with $c>0$.
 a constant (the smaller is $c$, the more effective is the sensor). 
The rover gets the reward $+10$ (resp. $-10$) for each good (resp. bad) sampled rock, % $-10$ for each bad rock sampled,
and $+10$ when it reaches the exit. 

In the possibilistic model, the observation function is approximated using a
critical distance $d>0$ beyond which checking a rock is uninformative: $\pi
\paren{ o_i' \sachant s_i',a,s_v } = 1$ $ \forall o_i' \in \mathcal{O}_{i} $.
%If the
%rover is distant from the rock less than $d$, 
The possibility degree of
erroneous observation becomes zero if the robot stands at
the checked rock, and least non zero possibility degree otherwise. Finally, 
as possibilistic semantics does not allow sums of
rewards, an additional visible state variable $s^2_v \in
\set{ 1, \ldots, R }$ which counts the number of checked rocks is introduced. 
The qualitative dislike of sampling is modeled as $\Psi(s)=\frac{R+2-s_{v}^{2}}{R+2}$
if the location is terminal and zero otherwise. 
%Preference $\Psi(s)$ equals qualitative dislike of sampling $\frac{R+2-s_{v,2}}{R+2}$ 
%if all rocks are bad and location is terminal, zero otherwise.
The location of the rover is finally denoted by $s^1_v \in \mathcal{S}^1_v$ and the visible state is then
$s_v=(s^1_v,s^2_v) \in \mathcal{S}^1_v \times \mathcal{S}^2_v = \mathcal{S}_v$. 

Observations $\set{ o_{good},o_{bad} }$ for the current rock can be equivalently 
modeled as a Cartesian product of observations $\set{ o_{good_1},o_{bad_1} } \times \cdots \times \set{ o_{good_R},o_{bad_R} }$ 
for each rock. By using this equivalent modeling, state and observation spaces 
are both respectively factorized as 
$\mathcal{S}^1_v \times \ldots \times \mathcal{S}^m_v \times \mathcal{S}^1_h \times \ldots \times \mathcal{S}^l_h $
 and $\mathcal{O} = \mathcal{O}^1 \times \ldots \times \mathcal{O}^l$, and we can now map each observation variable 
$O^j \in \mathcal{O}^j$ to its hidden state variable $S^j_h \in \mathcal{S}^j_h$. 
It allows us to reason 
about the DBN of Figure \ref{fig_piMOMDPFact}, 
which expresses three important assumptions 
that will help us factorize the belief state itself:
\begin{enumerate}
\item all state variables $S^1_v,S^2_v,\ldots,S^m_v,S^1_h,S^2_h,\ldots,S^l_h$ are post-action independent 
variables, and the next visible variables do not depend on current hidden ones.
Thus, there is no arrow between two state variables at the same time step, as $S^2_{v,t}$ and $S^1_{h,t}$,
nor arrow from a current hidden variable to a next visible one, as $S^1_{h,t}$ and $S^1_{v,t+1}$;
\item a hidden variable does not depend on previous other hidden variables: the nature of a rock is 
independent from the previous nature of other rocks. 
For instance, there is no arrow from $S^1_{h,t}$ to $S^2_{h,t+1}$;
\item an observation variable is available for each hidden state variable,
and depends on it. 
It does not depend on other 
hidden state variables 
nor current visible ones, 
but on previous visible state variables and action:
for instance, there is no arrow between $S^1_{h,t+1}$ and $O^2_{t+1}$, 
nor between $S^1_{v,t+1}$ and $O^1_{t+1}$. 
\end{enumerate}
Each observation 
variable is indeed only related 
to the nature of the corresponding rock. 
The observation quality yet depends on the rover's location 
\emph{i.e.} a current visible state variable, 
not allowed by the DBN: 
fortunately, as moves are deterministic, 
we avoid this issue considering observations depend on previous location and action.


\begin{figure}[b!]\centering
\begin{tikzpicture}
\tikzstyle{vertex}=[circle,fill=black!50,minimum size=32pt,inner sep=0pt,draw=black,thick]
\tikzstyle{vvertex}=[circle,fill=black!30,minimum size=32pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red!60,minimum size=25pt,inner sep=0pt,draw=black,thick]
\tikzstyle{rvertex}=[fill=yellow!60,decision=3,inner sep=-1pt,minimum size=35pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=32pt,inner sep=0pt,draw=black,thick]


%TIME
\node [font=\huge] (statet) at (3,7.3) {$t$};
\node [font=\huge] (statetplus1) at (10,7.3) {$t+1$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%vstates
%1
\node[vvertex] (vstate1) at (3,6) {$S^1_{v,t}$};
\node[vvertex] (vstate12) at (3,4.5) {$S^2_{v,t}$};
\node (vstate13) at (3,3.5) {$\vdots$};
%2
\node[vvertex] (vstate2) at (10,6) {$S^1_{v,t+1}$};
\node[vvertex] (vstate22) at (10,4.5) {$S^2_{v,t+1}$};
\node (vstate23) at (10,3.5) {$\vdots$};

%0
\node (vstate0) at (-1,6) {};
\node (vstate02) at (-1,4.5) {};
%3
\node (vstate3) at (14,6) {};
\node (vstate32) at (14,4.5) {};


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%hstates
%1
\node[vertex] (hstate1) at (3,2) {$S^1_{h,t}$};
\node[vertex] (hstate12) at (3,0.5) {$S^2_{h,t}$};
\node (hstate13) at (3,-0.5) {$\vdots$};
%2
\node[vertex] (hstate2) at (10,2) {$S^1_{h,t+1}$};
\node[vertex] (hstate22) at (10,0.5) {$S^2_{h,t+1}$};
\node (hstate23) at (10,-0.5) {$\vdots$};
%0
\node (hstate0) at (-1,2) {};
\node (hstate02) at (-1,0.5) {};
%3
\node (hstate3) at (14,2) {};
\node (hstate32) at (14,0.5) {};



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%action
\node[avertex] (action) at (6.2,-2.3) {$a_t$};
\node[avertex] (action0) at (0,-2.3) {$a_{t-1}$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% observations
%1
\node[overtex] (hobserv1) at (6,-4) {$O^1_t$};
\node[overtex] (hobserv12) at (4.6,-2.9) {$O^2_t$};
\node (doto1) at (3.9,-2.1) [rotate=36] {$\vdots$};
%\node at (3.7,-3.3) {$\ldots$};
%2
\node[overtex] (hobserv2) at (13,-4) {$O^1_{t+1}$};
\node[overtex] (hobserv22) at (11.6,-2.9) {$O^2_{t+1}$};
\node (doto2) at (10.9,-2.1) [rotate=36] {$\vdots$};
%\node at (7.9,-3.4) {$\ldots$};


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS

%SV
% arrows sv->sv
%1->2
\draw[->,>=latex] (vstate1) -- (vstate2);
\draw[->,>=latex] (vstate1) -- (vstate22);
\draw[->,>=latex] (vstate12) -- (vstate2);
\draw[->,>=latex] (vstate12) -- (vstate22);
%0->1
\draw[->,>=latex,dashed] (vstate0) -- (vstate1);
\draw[->,>=latex,dashed] (vstate0) -- (vstate12);
\draw[->,>=latex,dashed] (vstate02) -- (vstate1);
\draw[->,>=latex,dashed] (vstate02) -- (vstate12);
%2->3
\draw[->,>=latex,dashed] (vstate2) -- (vstate3);
\draw[->,>=latex,dashed] (vstate2) -- (vstate32);
\draw[->,>=latex,dashed] (vstate22) -- (vstate3);
\draw[->,>=latex,dashed] (vstate22) -- (vstate32);


% arrows sv->sh
%1->2
\draw[->,>=latex] (vstate1) -- (hstate2);
\draw[->,>=latex] (vstate1) -- (hstate22);
\draw[->,>=latex] (vstate12) -- (hstate2);
\draw[->,>=latex] (vstate12) -- (hstate22);
%0->1
\draw[->,>=latex,dashed] (vstate0) -- (hstate1);
\draw[->,>=latex,dashed] (vstate0) -- (hstate12);
\draw[->,>=latex,dashed] (vstate02) -- (hstate1);
\draw[->,>=latex,dashed] (vstate02) -- (hstate12);

%2->3
\draw[->,>=latex,dashed] (vstate2) -- (hstate3);
\draw[->,>=latex,dashed] (vstate2) -- (hstate32);
\draw[->,>=latex,dashed] (vstate22) -- (hstate3);
\draw[->,>=latex,dashed] (vstate22) -- (hstate32);


% arrows sv->oh

\draw[->,>=latex] (vstate1) to[bend right] (hobserv2);
\draw[->,>=latex] (vstate12) to[bend right] (hobserv2);
\draw[->,>=latex] (vstate1) to[bend right] (hobserv22);
\draw[->,>=latex] (vstate12) to[bend right] (hobserv22);


\node (fake0) at (-1,-1) {};
\node (fake1) at (-1,0) {};
\node (fake2) at (-1,-0.3) {};
\node (fake3) at (-1,-0.7) {};
\draw[->,>=latex,dashed] (fake0) to (hobserv1);
\draw[->,>=latex,dashed] (fake1) to (hobserv12);
\draw[->,>=latex,dashed] (fake2) to (hobserv1);
\draw[->,>=latex,dashed] (fake3) to (hobserv12);

%SH
% arrows sh->sh
%1->2
\draw[->,>=latex] (hstate1) -- (hstate2);
\draw[->,>=latex] (hstate12) -- (hstate22);
%0->1
\draw[->,>=latex,dashed] (hstate0) -- (hstate1);
\draw[->,>=latex,dashed] (hstate02) -- (hstate12);

%2->3
\draw[->,>=latex,dashed] (hstate2) -- (hstate3);
\draw[->,>=latex,dashed] (hstate22) -- (hstate32);

% arrows sh->oh
%1
\draw[->,>=latex] (hstate1) to (hobserv1);
\draw[->,>=latex] (hstate12) to (hobserv12);
%2
\draw[->,>=latex] (hstate2) to (hobserv2);
\draw[->,>=latex] (hstate22) to (hobserv22);

%A
% a->s
%1
\draw[->,>=latex] (action) -- (vstate2);
\draw[->,>=latex] (action) -- (vstate22);
\draw[->,>=latex] (action) -- (hstate2);
\draw[->,>=latex] (action) -- (hstate22);
%0
\draw[->,>=latex,dashed] (action0) -- (vstate1);
\draw[->,>=latex,dashed] (action0) -- (vstate12);
\draw[->,>=latex,dashed] (action0) -- (hstate1);
\draw[->,>=latex,dashed] (action0) -- (hstate12);

% a->oh
%1
\draw[->,>=latex] (action) to[bend right] (hobserv2);
\draw[->,>=latex] (action) to[bend right] (hobserv22);
%0
\draw[->,>=latex] (action0) to[bend right] (hobserv1);
\draw[->,>=latex] (action0) to[bend right] (hobserv12);

%\node (probao1) at (4,-2.2) [rotate=350] {$ \pi \paren{ o_{h,t} \sachant s_t, a_{t-1}}$};
%\node (probao2) at (11,-2.5) [rotate=350] {$ \pi \paren{ o_{h,t+1} \sachant s_{t+1}, a_{t}}$};
%\node (probas) at (8.9,0.5) [rotate=355] {$ \pi \paren{s_{t+1} \sachant s_t,a_{t}}$};

\end{tikzpicture}
\caption[DBN of a factored belief-independent $\pi$-MOMDP]{
DBN summing up independence assumptions of a $\pi$-MOMDP
leading to marginal beliefs and a $\pi$-MDP with 
a factored transition function \textit{i.e.} a factored belief $\pi$-MDP.
Parents of a visible state variable are the previous visible state variables.
Parents of a hidden state variable are the previous visible state variables 
and the corresponding previous hidden state variable. 
Finally,
parents of an observation variable are the previous visible state variables,
and the corresponding current hidden state variable.
}
\label{fig_piMOMDPFact}
\end{figure}

\subsection{Conséquences des hypothèses de factorisation}
\label{section_factoAssumptions}
In this section, we formally demonstrate how the three previous independence assumptions can be
used to factorize $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$ 
as the Cartesian product $\displaystyle \bigtimes_{j=1}^{l} \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$:
indeed, the belief state $\beta_h$ about the hidden states $s_h \in \mathcal{S}_h$ 
can be represented with marginal belief states $\beta^j_h \in \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
about hidden states $s^j \in \mathcal{S}^j_h$, $\forall j \in \set{1,\ldots,l}$.

To this end, we will use the \textit{d-Separation criterion} \cite{pearl88} 
in order to show some independence between variables from the DBN.
As explained in Section \ref{section_PPUDD}, 
a DBN can be drawn from independence relations.
Let us denote by $X \perp\!\!\!\perp Y \ \vert \ Z$
the assertion ``$X$ is independent from $Y$ conditional on $Z$'':
recall that for a given definition of the used independence relation,
e.g. probabilistic, non interactivity (NI), or minimum based (M, causal) independence,
the DBN is drawn such that for each node (variable) $X$,
$X \perp\!\!\!\perp nondescend(X) \ \vert \ parents(X)$.
If the used independence relation obeys the \textit{semi-graphoids} axioms \cite{Pearl:1988:PRI:52121,DBLP:journals/corr/abs-1304-2379},
the graphical criterion called d-Separation 
can be used to identify some independences between variables of the DBN.

This criterion is for instance used in probabilistic settings in \cite{Witwicki13icaps}.
Recall that the M-independence is not symetric (see Section \ref{qualitative_indep}),
and thus does not obey the axioms of semi-graphoids.
However, the NI-independence leads to a semi-graphoid,
as proved in \cite{delafonk}.

Let us recall that M-independence implies NI-independence
(see Theorem \ref{theorem_NIequivalence} of Section \ref{qualitative_indep}).
The DBN of Figure \ref{fig_piMOMDPFact}
represents M-independences between variables:
thus the DBN representing NI-independences (which is not drawn in this work) 
has potentially less arrows,
\textit{i.e.} assumes potentially more independences 
than the DBN representing the M-independences.
Assuming that the DBN of Figure \ref{fig_piMOMDPFact} 
represents NI-independences is a relaxation
\textit{i.e.} we potentially forget some NI-independence assumptions
by doing this assumption.
However, all NI-independences proved using d-Separation criterion
on the DBN are true.

First of all, the DBN of Figure \ref{fig_piMOMDPFact} representing the M-independence assumptions,
some probability distributions can be defined from the fact that each node is M-independent from its non-descendants
conditional on its parents: given a time step $t \geqslant0$, an action $a_t \in \mathcal{A}$,
and a current state $s = (s_{v,t},s_{h,t}) = (s^1_{v,t},\ldots, s^m_{v,t}, s^1_{h,t}, \ldots, s^l_{h,t}) \in \mathcal{S}$,
\begin{itemize}
\item $\forall i \in \set{ 1,\ldots,m }$, 
the transition possibility distribution over the $i^{th}$ visible state variable $s^i_{v,t+1} \in \mathcal{S}^i_v$: 
\begin{equation}
\label{EQ_possV}
\pi \paren{ s^i_{v,t+1} \sachant s_{v,t}, a_t } = \Pi \paren{ S^i_{v,t+1} = s^i_{v,t+1} \sachant S_{v,t} = s_{v,t}, a_t   };
\end{equation}
\item $\forall j \in \set{ 1,\ldots,l }$, 
the transition possibility distribution over the $j^{th}$ hidden state variable $s^j_{h,t+1} \in \mathcal{S}^i_h$: 
\begin{equation}
\label{EQ_possH}
\pi \paren{ s^j_{h,t+1} \sachant s_{v,t}, s_{h,t}, a_t } = \Pi \paren{ S^i_{h,t+1} = s^i_{h,t+1} \sachant S_{h,t} = s_{h,t}, a_t   } ;
\end{equation}
\item $\forall j \in \set{ 1,\ldots,l }$, 
the observation possibility distribution over the $j^{th}$ observation variable $o^j \in \mathcal{O}^i$: 
\begin{equation}
\label{EQ_possO}
\pi \paren{ o^j_{t+1} \sachant s_{v,t}, s_{h,t+1}, a_t } = \Pi \paren{ O^j_{t+1} = o^j_{t+1} \sachant S_{v,t} = s_{v,t}, S_{h,t+1} = s_{h,t+1}, a_t }.
\end{equation}
\end{itemize}
With these distributions, the dynamics of the process of a $\pi$-MOMDP
respecting the assumptions of Figure \ref{fig_piMOMDPFact}
is entirely defined.


Let us define the information $i_t$ known by the agent at time step $t \geqslant 1$
when the model is a ($\pi$-)MOMDP:
 % of a $\pi$-MOMDP as: 
$i_0 = \set{ s_{v,0} }$, and
for each time step $t \geqslant 1$, 
$i_t = \set{ o_t,s_{v,t},a_{t-1},i_{t-1} }$:
the corresponding variable is denoted by $I_t$.
The next theorem shows that the current belief can be decomposed into 
marginal belief states dependent on the current information $i_t$.
\begin{theorem}[Independence of hidden state variables and marginal belief states]
\label{thmSHind} 
Consider a $\pi$-MOMDP described by the DBN of Figure \ref{fig_piMOMDPFact}.
If initial hidden variables $S^1_{h,0}, \ldots,S^l_{h,0}$ are NI-independent, then at each time step $t>0$ 
the belief over hidden states can be written as 
\[ \beta_{h,t} = \displaystyle \min_{j=1}^{l} \beta^j_{h,t}\]
with $\forall s \in \mathcal{S}^j_h$, $\beta^j_{h,t}(s) = \Pi \paren{ S^j_{h,t} = s \sachant I_t = i_t }$ the belief state 
concerning hidden states of the set $\mathcal{S}^j_h$.
\end{theorem}
The proof is given in Annex \ref{thmSHind_RETURN}.

Thanks to the previous theorem, the state space accessible to the agent can now be rewritten as 
$\mathcal{S}^1_v \times \ldots \times
\mathcal{S}^m_v \times \Pi^{\mathcal{S}^1_h}_{\mathcal{L}} \times \cdots \times \Pi^{\mathcal{S}^l_h}_{\mathcal{L}}$ 
with $\Pi^{\mathcal{S}^j_h}_{\mathcal{L}}
\subsetneq \mathcal{L}^{\mathcal{S}^j_h}$. 
The size of $\Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
is $\#\mathcal{L}^{\#\mathcal{S}^j_h} - (\#\mathcal{L}-1)^{\#\mathcal{S}^j_h}$
(see Equation \ref{equation_numberOfPossDistrib}). 
If all state variables are binary,
$\# \Pi^{\mathcal{S}^j_h}_{\mathcal{L}} = 2 \#\mathcal{L} - 1$ for all $1 \leqslant j \leqslant l$, so that 
$\# \mathcal{S}_v \times \Pi^{\mathcal{S}_h}_{\mathcal{L}} = 2^m (2
\#\mathcal{L} - 1)^l$: contrary to probabilistic settings, {\bf hidden state variables
and visible ones have a similar impact on the solving complexity}, i.e. both
singly-exponential in the number of state variables. 
In the general case, by noting 
$\kappa = \max\{\max_{1 \leqslant i \leqslant m} \# S_{v,i} , \max_{1 \leqslant j \leqslant l} \# S_{h,j}\}$, 
there are $\mathcal{O}(\kappa^m (\# \mathcal{L})^{(\kappa - 1) l})$ 
flattened belief states, which is indeed exponential in the
arity of state variables too. 

In Section \ref{section_piPOMDP} about $\pi$-POMDP, 
the belief state variable 
at time step $t \geqslant 0$ 
is denoted by $B^{\pi}_{t}$,
and its possible values are $\beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$.
Now that $\Pi^{\mathcal{S}_h}_{\mathcal{L}}$ has been factorized,
we can consider the \textit{marginal belief state variables}
$B^{\pi,j}_{h,t}$, $\forall j \in \set{1,\ldots,l}$, 
whose possible values are $\beta_h^j \in \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$,
\textit{i.e.} belief states concerning hidden states $s^j \in \mathcal{S}^j_h$.
We now want to show that successive variables 
$S^1_{v,t},\ldots,S^m_{v,t},B^{\pi,1}_{h,t},\ldots,B^{\pi,l}_{h,t}$
respect the assumptions of the DBN of
Figure \ref{fig_piMDPFact},
\textit{i.e.} are independent post-action variables,
as successive variables $X^1_t, \ldots, X^n_t$.
This result is based on Lemma
\ref{lemBEL}, which shows how marginal belief state are actually updated. 

\begin{Lemma}[Update of the marginal belief states]
\label{lemBEL}
At time $t \geqslant 0$, 
if the system is in the visible state $s_{v,t} = (s^1_{v,t},\ldots,s^m_{v,t}) \in \mathcal{S}_v$,
in the belief state over the $j^{th}$ hidden state $\beta^j_{h,t} \in \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$, 
and if the agent selects action $a_t \in \mathcal{A}$ 
and then gets observation $o^j_{t+1} \in \mathcal{O}^j$, 
the update of the belief state about hidden system states $s^j \in \mathcal{S}^j_h$ is:
\begin{equation} 
\label{beliefUpdateJ} 
\beta^j_{h,t+1}(s^j_{t+1}) = \left \{ 
\begin{array}{cc} 
\displaystyle 1 & \hspace{-1cm}  \mbox{ if }  \pi \paren{o^j_{t+1},s^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t},a_t } = \pi \paren{o^j_{t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t } \\
 \pi \paren{o^j_{t+1},s^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t} & \mbox{ otherwise}. 
\end{array} \right. 
\end{equation}
where $\pi \paren{ o^j_{t+1}, s^j_{h,t+1} \sachant s_v, \beta^j_h, a } $ is the notation for
\[ \max_{s^j \in \mathcal{S}^j_h} \min \set{ \pi \paren{ o^j_{t+1} \sachant s_v, s^j_{t+1}, a} , \pi \paren{ s^j_{t+1} \sachant s_v, s^j, a}, \beta^j_h(s^j) }, \]
using distributions (\ref{EQ_possH}) and (\ref{EQ_possO}),
and \[ \pi \paren{ o^j_{t+1} \sachant s_v, \beta^j_h, a  } = \max_{s^j_{t+1} \in \mathcal{S}^j_h} \pi \paren{ o^j_{t+1}, s^j_{t+1} \sachant s_v, \beta^j_h, a }.\]
\end{Lemma}
The proof is given in Annex \ref{lemBEL_RETURN}.
The associated \textbf{belief update function} is $\nu^j$:
\[ \beta^j_{h,t+1} = \nu^j(s_{v,t},\beta^j_{h,t},a_t,o^j_{t+1}), \]
which can be denoted by 
\[ \beta^j_{h,t+1}(s^j_{h,t+1}) \propto^{\pi} \pi \paren{o^j_{t+1},s^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t} \]
as it consists of the possibilistic normalization 
of the joint possibility distribution over the $j^{th}$ hidden state variable
and the $j^{th}$ observation.

Hence, the possibility degree that the marginal belief state variables $B^{\pi,j}_{h,t+1}$ is $\beta^j_{h,t+1} \in \Pi^{\mathcal{S}^j_h}_{\mathcal{L}}$
conditional on $B^{\pi,j}_{h,t} = \beta^j_{h,t}$ and the action $a_t \in \mathcal{A}$, 
can be computed:
\begin{equation}
\label{trans_marg_belief}
 \Pi \paren{ B^{\pi,j}_{h,t+1} = \beta^j_{h,t+1} \sachant S_{v,t} = s_{v,t}, B^{\pi,j}_{h,t} = \beta^j_{h,t}, a_t } = \max_{\substack{ o^j \in \mathcal{O}^j \mbox{ \tiny s.t. } \\ \nu^j(s_{v,t},\beta^j_{h,t},a_t,o^j) = \beta^j_{h,t+1}}} \pi \paren{ o^j \sachant s_{v,t}, \beta^j_{h,t}, a_t  }
\end{equation}
defining the transition possibility distribution of marginal belief states
$\pi \paren{ \beta^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t }$.

Finally, Theorem \ref{thmVARind} relies on Lemma \ref{lemBEL} to ensure
independence of all post-action variables of the belief $\pi$-MDP resulting from the factorization,
conditional on the current state:
this allows us to write the possibilistic transition function of the 
belief-state $\pi$-MDP in a factorized form:
\begin{theorem}[Factored expression of the transition possibility distribution]
\label{thmVARind}
If independence assumptions of a $\pi$-MOMDP are described by the DBN of Figure \ref{fig_piMOMDPFact},
then
$\forall \beta_{h,t}=(\beta^1_{h,t},\ldots,\beta^l_{h,t}) \in \Pi^{\mathcal{S}_h}_{\mathcal{L}}, 
\beta_{h,t+1} = (\beta^1_{h,t+1},\ldots,\beta^l_{h,t+1}) \in  \Pi^{\mathcal{S}_h}_{\mathcal{L}}$, 
$\forall (s_{v,t},s_{v,t+1}) \in (\mathcal{S}_v)^2$, 
$\forall a_t \in \mathcal{A}$, \\
$\pi \paren{ s_{v,t+1}, \beta_{h,t+1} \sachant s_{v,t}, \beta_{h,t}, a }$ 
\[  = \displaystyle  \min \set{ \min_{i=1}^{m} \pi \paren{ s^i_{v,t+1} \sachant s_{v,t}, a_t } ,  \min_{j=1}^{l} \pi \paren{ \beta^j_{h,t+1} \sachant s_{v,t}, \beta^j_{h,t}, a_t  } }, \]
where the transition possibility distributions of visible state variables is given in the equation (\ref{EQ_possV})
and the one of marginal belief state variables in the equation (\ref{trans_marg_belief}).
\end{theorem}
The proof is given in Annex \ref{thmVARind_RETURN}.
Using this result, such a factorized expression of the transition possibility distribution
allows to compute the value function with $n=m+l$ stages, 
as described in the previous section:
the $\pi$-MOMDP is indeed a factored $\pi$-MDP
since variables $(S^1_v,\ldots,S^m_v,B^{\pi,1}_h,\ldots,B^{\pi,l}_h)$,
can play the role of variables $X_1,\ldots,X_n$
in Algorithm \ref{PPUDD}.	

Note that Probability Theory does not make any difference between 
causal independence (M-independence in Possibility Theory, see Definition \ref{def_mcond}) 
and decompositional independence (NI-independence in Possibility Theory, see Definition \ref{def_NIindep}).
Moreover, probabilistic independence relation obey the axioms of semi-graphoids:
so previous independence results due to d-Separation are also true in probabilistic settings. 
If independence assumptions between variables of a probabilistic MOMDP \cite{OngShaoHsuWee-IJRR10,AraThoBufCha-ICTAI10} are described 
by the DBN of Figure \ref{fig_piMOMDPFact},
then a similar factorization result can be deduced.
The MDP built from such a probabilistic MOMDP is thus a factored MDP,
with infinite state space.

Previous theorems allow to express the transition distribution
of the ($\pi$-)MDP resulting from a ($\pi$-)MOMDP
with distributions which concern less variables.
The value function update is then divided
into $n=m+l$ stages in the possibilistic case, 
as depicted by the \textit{for} loop of Algorithm \ref{PPUDD}.
Qualitative possibilistic MOMDPs
can however also be solved using ADDs
even if the independence assumptions do not hold:
in this case, one global transition distribution, 
encoded as a big ADD concerning all variables $(S^1_v,\ldots,S^m_v,B^{\pi}_h)$ 
is used, and the number of potential values $\beta_h \in \Pi^{\mathcal{S}_h}_{\mathcal{L}}$
of the global belief state variable $B^{\pi}_h$
increases exponentially with the number of hidden states: 
$\# \Pi^{\mathcal{S}_h}_{\mathcal{L}} = \paren{\# \mathcal{L}}^{\# \mathcal{S}_h} - \paren{\# \mathcal{L} -1}^{\# \mathcal{S}_h}$
(see Equation \ref{equation_numberOfPossDistrib}).
Nevertheless, if the factorization of the transition distribution
is possible,
handled ADDs have less nodes
and computations should be faster.
These results are used in the next section to compute more efficiently
optimal strategies of $\pi$-MOMDPs.

\section{Résultats expérimentaux}
\label{section_expe_PPUDD}
The main expected advantages of using factored
$\pi$-(MO)MDPs over their probabilistic counterparts are: 
\begin{enumerate}
\item values of ADDs are
in the finite scale $\mathcal{L}$ rather than $\mathbb{R}$, so that the number of
their leaves is at most $\#\mathcal{L} \ll 2^n$ (probabilistic models' ADDs can have
up to $2^n$ leaves, where $n$ is the number of variables involved in the ADD);
%available to the agent,
%\textit{i.e.} visible state variables and variables representing belief states concerning the hidden ones; 
\item $\pi$-MOMDPs boil down to factored \emph{finite}-state
belief $\pi$-MDPs that can be solved by PPUDD assuming some independence
assumptions on the underlying DBNs; 
\item $\pi$-MOMDPs are in the same
complexity class as $\pi$-MDPs \textit{if all hidden state variables are binary}
(in probabilistic models, partially-observable problems are always in a higher
complexity class). 
\end{enumerate}
Of course, we have to pay a price: namely, possibilistic models can be seen as approximations 
of probabilistic ones 
(except if probabilities in the model are not precisely known
and uncertainty of the problem is better described in a qualitative form). 
Yet, many state-of-the-art probabilistic
algorithms are approximate, 
e.g. MDP solver PROST \cite{DBLP:conf/aips/KellerE12} (based on UCT algorithm \cite{Kocsis:2006:BBM:2091602.2091633})
and POMDP solvers described in Section \ref{section_SAalgo}.
Our PPUDD algorithm, however, is exact.



\subsection{Missions Robotiques}

We compared PPUDD on the \textit{Rocksample problem} (RS),
described in Section \ref{section_RS_motivatingEx}, 
against a recent probabilistic MOMDP planner, 
APPL \cite{OngShaoHsuWee-IJRR10}, and a POMDP
planner using ADDs, symbolic HSVI \cite{Sim2008SHS1620163.1620241}. Both
algorithms are approximate and anytime, so we  decided to stop them when they
reached a precision of $1$. 
Figure \ref{figureRS1}, where problem instances
increase with grid size and number of rocks, shows that APPL runs out of memory
at the $8^{th}$ problem instance, symbolic HSVI at the $7^{th}$ one, while PPUDD
outperforms them by many orders of magnitude. 
\begin{figure}\centering
\begin{subfigure}[c]{.48\linewidth}
\includegraphics[width=\linewidth]{RockSampleCompTime.pdf}
\caption{Computation time (sec)}
\label{figureRS1}
\end{subfigure}
\begin{subfigure}[c]{.48\linewidth}
\includegraphics[width=\linewidth]{courbePerfTime.pdf}
\caption{Expected total reward}
\label{figureRS2}
\end{subfigure}
\vspace{0.5cm}
\caption[PPUDD vs. APPL and symb-HSVI, RockSample problem]{PPUDD vs. APPL and symb HSVI on the RockSample problem: the $x$-axis represents indexes of problem instances, increasing with the problem sizes.}
\end{figure}
Instead of precision, computation
time of APPL can be fixed at PPUDD's computation time in order to compare their
expected total rewards (using probabilistic model's rewards) 
after they consumed the same CPU time. 
%For PPUDD, we
%simply evaluated with the probabilistic model the policy that was computed
% under
%possibilistic settings.
Surprisingly, Figure \ref{figureRS2} shows that rewards gathered are higher with
PPUDD than with APPL. The reason is that APPL is in fact an approximate
probabilistic planner, which shows that our approach consisting in exactly
solving an approximate model can outperform algorithms that approximately solve
an exact model. 
Moreover, exact POMDP planners are
unable to scale to problems of the size of the RockSample ones.
Finally, it is worth noting that probabilities of the
observation model, which represent uncertainties of sensor outputs, may be
difficult to precisely know in practice, in which case possibilistic models may
be more physically accurate. 
In fact for this example the policy produced by
PPUDD is the best to get all possible rewards: this is essentially because the
rover can be sure of a rock's nature checking 
when it is on it.

These results assured us that it was not unreasonable to present PPUDD
in the International Probabilistic Planning Competition 2014,
even though the computation of strategies for probabilistic problems 
is not the initial vocation of this solver.
The next section describes the competition context
as well as the presented versions of PPUDD,
and discusses the results of the different competitor solvers.

\subsection{Compétition de planification probabiliste internationale 2014}
% TODO TODO TODO TODO 
% presentation de la compet 1
% presentation des competiteurs
% presentation de nos versions
% MASK, ATPPUDD, description 
% description des domains et en parallele, les resultats et commentaires
% + citer depot \\

%puis les 2 chaps suivants \\
%puis intro/concl \\


The fully observable track 
of the International Probabilistic Planning Competition (IPPC) 
allows to fairly compare performances of MDP solvers.
The competitors' solvers have to compute 
strategies for some problems 
which are not known in advance.
Given one of these problems, 
solvers have a limited amount of time 
to send actions to the competition server 
which simulates the evolution of the system state:
successive states are sampled by the competition server
using the transition probability 
distributions of the MDP defining the problem, 
and sent to a given competitor's solver.
For each system state received, the solver 
has to send back the action
it has computed.
These data exchanges are conducted 
during few trials of finite horizon and
the score of the solver for the considered problem 
is the average (over trials) 
of the undiscounted and finite sum of 
rewards along the trajectory generated by the trial.

Materials about this competition are available at the official web page of the competition
\url{https://cs.uwaterloo.ca/~mgrzes/IPPC_2014/}.
Problems are grouped in \textit{domains}, 
which are MDPs whose a finite number of parameters are undefined:
the problem, or MDP, used in practice during the competition
is an \textit{instance} of a domain, \textit{i.e.}
a domain whose parameters have been set.
In this competition, $8$ domains have been proposed,
called respectively
\textit{Academic advising}, \textit{Crossing traffic}, \textit{Elevators}, 
\textit{Skill teaching}, \textit{Tamarisk}, \textit{Traffic}, \textit{Triangle tireworld}
and \textit{Wildfire}.
Three possible encodings of the instances of these domains are proposed,
\textit{i.e.} three different languages can be used to describe the instances:
the first is the \textit{Planning Domain Definition Language} (PPDDL, \cite{Younes_ppddl1.0});
the second is a \textit{LISP}-like language introduced with symbolic algorithms such as SPUDD 
which defines explicitely transition probability distributions and reward function as ADDs
(see \url{http://users.cecs.anu.edu.au/~ssanner/IPPC_2011/}); 
finally, the third is the \textit{Relational Dynamic Influence Diagram Language} (RDDL, \cite{Sanner_relationaldynamic})
which is simpler and more expressive than the previous ones.
The competition consists in evaluating the solver over 
$10$ instances per domain with $30$ runs per instance and
$18$ minutes per instance: it takes $24$ hours in total.

In order to ensure that everyone has the same computational power,
each competitor solver is set up in a remote server whose RAM is $7.5$Gb 
with $2$ cores.% (``m3.large'', see \url{https://aws.amazon.com/ec2/pricing/}).
The client and server for the competition are available in the open source \textit{RDDLSim} software, 
which is available online at \url{http://code.google.com/p/rddlsim/}.
Four solvers have been proposed for this competition:
\begin{itemize}
\item \textit{PROST} \cite{DBLP:conf/aips/KellerE12}, based on \textit{Upper Confidence bound applied to Trees} (UCT, \cite{Kocsis:2006:BBM:2091602.2091633})
and using directly RDDL encoding;
\item \textit{GOURMAND} \cite{DBLP:conf/aaai/KolobovMW12,Kolobov12reverseiterative}, based on \textit{Labeled Real Time Dynamic Programming} (LRTDP, \cite{Bonet03labeledrtdp})
using PPDDL encoding;
\item \textit{symbolic LRTDP}, using ADDs and LISP-like encoding \cite{symbLRTDP};
\item our algorithm PPUDD, using LISP-like encoding too.
\end{itemize}

As the score given to solvers only depends on the $40$ first stages of the process,
the presented version of PPUDD consists of the Algorithm \ref{PPUDD} 
with the ``while condition'' $\overline{U^*} \neq \overline{U^c}$ at line \ref{while_PPUDD}
replaced by the condition ``iteration $\leqslant 40$''.
It also incrementally augments the planning
horizon while maintaining a mask stored in form of 
a Binary Decision Diagram (BDD, \textit{i.e.} an ADD with leaves in $\set{0,1}$) 
representing the states reachable from the initial state:
the computation of the current value function is then restricted
to the reachable states only. While PPUDD is an offline algorithm,
we proposed also \textit{AnyTime PPUDD} (ATPPUDD) 
which is an anytime version which learns computation times of
Bellman backups while dispatching the computational effort accordingly
over the remaining planning horizon much like GOURMAND does in the
probabilistic world (see \cite{DBLP:conf/aaai/KolobovMW12}). 

When encoded with the LISP-like format, 
problems of the competition, \textit{i.e.} instances of each domains, 
are described as factored MDPs
with boolean system state variables: 
for each action $a \in \mathcal{A}$ and for each next boolean system state variable $X_i'$, 
one ADD representing the corresponding transition probability distribution
$\textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'), a \Big)$ is given.
In order to define the $\pi$-MDP which will be solved by PPUDD,
we simply normalize these distributions in the possibilistic sense:
we set to $1$ the possibility degree of an assignment of $X_i'$ when 
its probability value is maximal, and to the probability value otherwise.
For instance, for a given assignment of the previous variables $parents(X_i')$, 
if the probability value of the assignment (or event) $X_i'=1$ is $0.7$ 
(and thus probability $0.3$ that $X_i' = 0$), 
then the possibility degree of $X_i'=1$ is set to $1$,
and the one of $X_i' = 0$ is set to $0.3$.

In terms of ADDs, it can be computed as follows:
let us first recall the notation  $\textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)^{X_i'=0}$,
used to represent the subtree of the ADD
$\textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)$ setting $X_i'$ to false (\textit{i.e.} to $0$).
As well, $\textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)^{X_i'=1}$ is the subtree of the same ADD, setting $X_i'$ to true (\textit{i.e.} to $1$).
Let us denote by $\mathds{1}_{\textbf{p}_{\top}>\textbf{p}_{\bot}}$ 
the BDD equal to $1$ for each variable assignment such that %$X_i'$ is true ($X_i'=\top$),
%and $0$ otherwise. 
%for each assignment of the variables of $parents(X_i')$ such that 
%\ovalbox{$\min$}
\[ \textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)^{X_i'=0} < \textbf{p} \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big)^{X_i'=1}, \]
and equal to $0$ for other assignments.
The BDD always equal to $1$ is denoted by $\mathds{1}$. 
The BDD $\mathds{1}_{\textbf{p}=0.5}$ is equal to $1$
for variable assignments such that the probability of the event $X_i'=1$ (or $X_i'=0$) 
is equal to $0.5$, and this BDD is equal to $0$ otherwise.
We can also denote by
$\mathds{1}_{\textbf{p}_{\top}<\textbf{p}_{\bot}} $
the BDD which is equal to $1$ for assignments of variables in $parents(X_i')$ 
such that the probability of event $X_i' = 1$
is lower than the probability of event $X_i'=0$:
this BDD can be computed from previous BDDs,
$ \mathds{1} \ \ovalbox{$-$} \ \mathds{1}_{\textbf{p}_{\top}>\textbf{p}_{\bot}} \ \ovalbox{$-$} \ \mathds{1}_{\textbf{p}=0.5}$,
where $\ovalbox{$-$}$ is the minus operator, applied to trees.
The possibility transition distribution for the $i^{th}$ variable is
\begin{eqnarray*}
\pi \Big( X_i' \ \Big\vert \ parents(X_i'),a \Big) = \ovalbox{$\max$} \Bigg\{ 	& \mathds{1}_{\textbf{p}=0.5}, 														& \\
				& \ovalbox{$\min$} \bigg\{ \mathds{1}_{\textbf{p}_{\top}>\textbf{p}_{\bot}}, \textbf{p} \Big( X_i' \Big\vert parents(X_i'), a \Big) \bigg\} , 	& \\
				& \ovalbox{$\min$} \bigg\{ \mathds{1}_{\textbf{p}_{\top}<\textbf{p}_{\bot}}  , \textbf{p} \Big( X_i' \Big\vert parents(X_i'), a \Big) \bigg\} 	& 	\Bigg\} 
\end{eqnarray*}

As well, for each action $a \in \mathcal{A}$, 
an ADD representing the reward function for this action 
is provided and denoted by $r(X_1,\ldots,X_n,a)$. 
Let us define then for each $s \in \mathcal{S}$ and $a \in \mathcal{A}$,
\[ \Psi(s,a) = \frac{ r(s,a) - \min_{s,a} r(s,a) }{ \max_{s,a} r(s,a) - \min_{s,a} r(s,a)} \in [0,1]. \]
The terminal preference function is set to $\Psi(s) = \max_{a \in \mathcal{A}} \Psi(s,a)$,
and the strategy is initialized by $\delta^*(s) \in \operatorname*{argmax}_{a \in \mathcal{A}} \Psi(s,a)$
at the beginning of the algorithm.

Note that possibility and preference degrees are not in a scale $\mathcal{L}$ 
as previously defined (\textit{i.e.} $\set{ 0, \frac{1}{k}, \frac{2}{k} \ldots, 1 }$ for some $k \geqslant1$).
Indeed, possibility degrees comes from probability values, 
and preferences are normalized rewards.
However, only $\max$ and $\min$ operators are used, 
so it has no impact on the qualitative results of the computations. 

The library used to perform computations with ADDs
is the \textit{CU Decision Diagram Package} 
(CUDD, \url{http://vlsi.colorado.edu/~fabio/CUDD/}),
and the described versions of PPUDD are available 
at the adress \url{https://github.com/drougui/ppudd}.

Following figures illustrates the results of IPPC 2014:
the score is given in function of the instance index,
which generally increases with the difficulty (or the size) 
of the associated problem.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random strategy, ``noop'' strategy, ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Academic advising} problem},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/academic_advising.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/academic_advising.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/academic_advising.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/academic_advising.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/academic_advising.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/academic_advising.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/academic_advising.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Crossing traffic} problem},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/crossing_traffic_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Results of IPPC 2014: \textit{Academic advising} and \textit{Crossing traffic} problems]{
Results of the International Probabilistic Planning Competition -- Fully Observable track}
\label{figure_IPPC_ACA_CRO}
\end{figure}

Figure \ref{figure_IPPC_ACA_CRO} presents the scores obtained by each solver 
for each of the $10$ instances of the \textit{Academic advising} domains,
\textit{i.e.} the average over $30$ trials of the sum of the encountered rewards.
Performances of our algorithms are close to the best ones.
However, an unexplained and unwanted bug occurred with ATPPUDD for the $2^{nd}$ instance, 
as only $3$ runs have been performed by this solver.
For the other instances, PPUDD and ATPPUDD produce strategies with performances like 
PROST and GOURMAND, and better than Symbolic LRTDP.
This has been less true for the \textit{Crossing traffic} problem, 
whose results are also described by Figure \ref{figure_IPPC_ACA_CRO}.
This problem models a robot which has to reach a goal 
which is across an highway with a lot of cars.
These cars arrive randomly and move left.
As the possibility degree of the fact that no car arrive 
is set to $1$ by our naive MDP to $\pi$-MDP translation,
the optimistic criterion leads to decide to cross the street,
even if an unseen car may arrive (with a probability $<0.5$ but bit enough to be cautious). 
This explain the poor quality of the produced strategies 
for this domain. Note however that, for the $6$ last instances (most difficult problems)
our approach leads to better strategies than the probabilistic solver Symbolic LRTDP.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random strategy, ``noop'' strategy, ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm,
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Elevators} problem}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/elevators_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/elevators_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/elevators_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/elevators_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/elevators_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/elevators_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/elevators_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Skill teaching} problem},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/skill_teaching_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Results of IPPC 2014: \textit{Elevators} and \textit{Skill teaching problems}]{
Results of the International Probabilistic Planning Competition -- Fully Observable track}
\label{figure_IPPC_ELE_SKI}
\end{figure}

In the \textit{Elevators} problem, people arrive randomly and have to be transported 
to the correct building stage: 
as the frequentist information is lost using the possibilistic approach
and seems important in this problem (people do not want to wait once arrived), 
scores of our algorithms are poorer than the ones of PROST and GOURMAND.
The toy example at the beginning of the introduction of this chapter
illustrates that the possibilistic approaches can select actions probabilistically clearly suboptimal
when the probability values are at the heart of the problem.
PPUDD and ATPPUDD are however better than Symbolic LRTDP, as shown by Figure \ref{figure_IPPC_ELE_SKI},
and than doing nothing (``noop strategy'') or choosing actions randomly (``random strategy'').
PPUDD and ATPPUDD have quite good behaviours with the \textit{Skill teaching} problem 
as illustrated by the same figure. Moreover, ATPPUDD leads to better results
for the last three instances: as these instances are the \textit{Skill teaching} problems 
with the largest system space,  
the anytime version, which manages the computation time, 
produces strategies with better performances
than PPUDD, which classically solve the associated $\pi$-MDP, 
but cannot complete computations and lead to a poorer strategy.

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Tamarisk} problem},
compat=newest,
legend entries={GOURMAND, PROST, Symbolic LRTDP, random strategy, ``noop'' strategy, ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/tamarisk.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/tamarisk.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/tamarisk.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/tamarisk.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/tamarisk.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/tamarisk.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/tamarisk.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Traffic} problem},
width=11cm,height=11cm]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/traffic_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/traffic_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/traffic_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/traffic_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/traffic_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/traffic_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/traffic_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Results of IPPC 2014: \textit{Tamarisk} and \textit{Traffic} problems]{
Results of the International Probabilistic Planning Competition -- Fully Observable track}
\label{figure_IPPC_TAM_TRA}
\end{figure}


With respect to other solvers, possibilistic solvers have good results 
with the \textit{Tamarisk} domain, as shown in Figure \ref{figure_IPPC_TAM_TRA}..
However, some instances (e.g. the $6^{th}$, the $8^{th}$ and the $10^{th}$) 
are not even run as the ADD instantiation takes too long. 
Symbolic LRTP faces the same issue
as it uses also the LISP-like encoding of the problem.
We think that this is an issue specific to the competition,
as each problem has to be equivalently translated into
three different languages (RDDL, PPDDL and LISP-like),
which produces sometimes artificially complex
encodings of the problems.
The \textit{Traffic} domain is really hard to solve by PPUDD and ATPPUDD (see Figure \ref{figure_IPPC_TAM_TRA}). 
Actually the least scores are obtained with this domain,
and even the random and the noop strategies are better strategies.
Note that we did not implement any ``watchdog''
returning random actions when the computed strategy is less effective than the random one.
However, this kind of gadget is essential to improve results for such large and risky problem. 
As mentioned above for the \textit{Crossing traffic} problem, 
the optimistic criterion may lead to dangerous actions, as it does here.
Moreover, as this problem involves frequentist information (car arrivals)
an high suboptimality of the strategy produced by the possibilistic approach
is confirmed for this kind of problems (see the \textit{Elevator} problems). 
Finally, the \textit{Traffic} problem is known to be one of the hardest domain,
so ADD instantiation takes long, as well as computations, 
which are then not proceeded enough to produce satisfying results. 

\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10},
legend entries={GOURMAND, PROST, Symbolic LRTDP, random strategy, ``noop'' strategy, ATPPUDD, PPUDD},legend style={at={(1.52,1.1)}},width=11cm,height=11cm,
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Triangle tireworld} problem}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/triangle_tireworld_inst_mdp.txt};%PPUDD
\end{axis}
\end{tikzpicture}
\\
\vspace{0.5cm}
\begin{tikzpicture}
\begin{axis}[grid=major,xmin=1,xmax=10,xtick={1,2,3,4,5,6,7,8,9,10}, width=11cm,height=11cm,
xlabel={Problem instances},
ylabel={Raw average score},
title={\textit{Wildfire} problem}]
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=5, y error index=6]{IPPC14_resultats/wildfire_inst_mdp.txt};%GOURMAND
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=26, y error index=27]{IPPC14_resultats/wildfire_inst_mdp.txt};%PROST14
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=40, y error index=41]{IPPC14_resultats/wildfire_inst_mdp.txt};%FACTOREDLRTDP
\addplot+[line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=33, y error index=34]{IPPC14_resultats/wildfire_inst_mdp.txt};%RANDOM
\addplot+[color=black,line width=1pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=12, y error index=13]{IPPC14_resultats/wildfire_inst_mdp.txt};%NOOP
\addplot[color=ggreen,mark=diamond,line width=3.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=54, y error index=55]{IPPC14_resultats/wildfire_inst_mdp.txt};%ATPPUDD 
\addplot[color=orange,mark=diamond,line width=2.2pt] plot[error bars/.cd,y dir=both,y explicit] table[x index=1, y index=47, y error index=48]{IPPC14_resultats/wildfire_inst_mdp.txt};%PPUDD 
\end{axis}
\end{tikzpicture}
\caption[Results of IPPC 2014: \textit{Triangle tireworld} and \textit{Wildfire} problems]{
Results of the International Probabilistic Planning Competition -- Fully Observable track}
\label{figure_IPPC_TRI_WIL}
\end{figure}

Finally, the two last domains, whose results are described in Figure \ref{figure_IPPC_TRI_WIL},
are called \textit{Triangle Tireworld} and \textit{Wildfire}.
Firt, ATPPUDD faces an unexplained bug for each instance of the \textit{Triangle Tireworld} domain:
no trial is performed from the $5^{th}$ instance, and maximum $2$ trials are performed for other instances
(which explains the poor score for each instance).
As already mentioned for the \textit{Tamarisk} domain, 
ADD instantiation takes too long for the last instances,
and no trial is performed for the last $4$ instances 
with PPUDD too: Symbolic LRTDP faces the same issue. 
The last domain, called \textit{Wildfire}, leads to highly frequentist problems:
it involves random fire starts. 
That is why PPUDD and ATPPUDD strategies are not really efficient, 
but not too distant from Symbolic LRTDP solver's results.

%%%% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
%%%% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
%%%% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
%%%% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
%\section{Practical Implementation of the Algorithms using ADDs}
%CUDD library
%%%% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
%%%% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
%%%% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
%%%% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO


\section{Conclusion} 
We presented PPUDD, the first algorithm to the best of our knowledge that 
solves factored possibilistic qualitative (MO)MDPs with symbolic calculations. In our opinion,
possibilistic models are a good tradeoff between non-deterministic ones, whose
uncertainties are not at all quantified yielding a very approximate model, and
probabilistic ones, where uncertainties are fully specified,
sometimes arbitrarily in practice. 
The resolution of planning problems 
using the framework of non-determinism 
is called \textit{contingent/conformant planning} 
studied for instance in \cite{Albore_atranslation-based,bonet2014flexible}.
By the way, in the introduction of this thesis,
we might also add ``non-determinism''
as a particular case of Possibility Theory
in Figure \ref{uncertainty_theories},
as values of associated non additive measures
are $0$ or $1$ instead of a more flexible scale $\mathcal{L}$.

Moreover,
$\pi$-MOMDPs reason about finite values in a qualitative scale $\mathcal{L}$ whereas
probabilistic MOMDPs deal with values in $\mathbb{R}$, which implies larger ADDs
for symbolic algorithms. Also, the former reduce to finite-state belief
$\pi$-MDPs contrary to the latter that yield \emph{continuous}-state belief MDPs
of significantly higher complexity. Our experimental results highlight the point that using an
exact algorithm (PPUDD) for an approximate model ($\pi$-MDPs) can bring significantly faster computations
than reasoning about complex exact models, while providing better
strategies than approximate algorithms (APPL) for exact models. 
In the future, we would like to developp a probabilistic algorithm using 
the generalization of our possibilistic belief factorization theory to
probabilistic settings (see Theorem \ref{thm_factoredPROBbelief}): 
related but sightly different results have been proposed for
probabilistic POMDPs \cite{DBLP:conf/aips/ShaniPBS08,Poupart:2005:phd}. 
These results also does not concern 
the case of mixed observability.

This chapter finally presents the results of our possibilistic approach
during IPPC 2014: 
the highlighted bottleneck of our possibilistic algorithms
resides on the translation from probabilities to possibilities: 
the naive automated translation presented before the description of the results 
leads to poor policies in benchmarks with complex dynamics and reward structures.
Another issue is the size of the input LISP-like encoded domains whose ADD
instantiation before optimization takes a very long time or 
does not even fit into memory for many difficult benchmarks:
this difficulty is shared with the Symbolic LRTDP solver.
However, there is almost no discretization of the initial probability values defining the MDP
in order to produce the possibility degrees
during the instantiation of the ADD defining the $\pi$-MDP:
the maximal difference between two possibility degrees is set to $10^{-3}$.
Stronger discretizations have not been tested yet, 
and could improve scores of our solvers for problems with such memory issues.
Modeling issues have been also highlighted, 
namely the fact that some problems request 
a cautious behaviour, not provided by the use of
the optimistic criterion (see Definition \ref{probstylerewrittenMOMDPcrit}) 
used during the competition. 
Moreover, as illustrated in introduction, 
these experiments show that problems with high entropy events 
are outperformed by probabilistic approaches since the possibilistic 
approach does not take into account the frequentist information about the problem.
The use of \textit{lexi}-approaches, as used in the following chapter, 
may be a possibilistic stratagem to get around this issue.
Note finally that the partially observable version of PPUDD 
(with the generation of a mask of reachable belief states, avoiding useless computations
on unreachable beliefs) is also available on the repository \url{https://github.com/drougui/ppudd}.

The next chapter, Chapter \ref{chap_IHM}, deals with \textit{Human-Machine Interaction} (HMI)
problems: the uncertainty dynamics of the system are in this context typically not known in terms
of probability values, and the qualitative possibilistic approach is shown to be a natural approach
to produce efficient diagnosis of human errors.

Finally, the last chapter, Chapter \ref{chap_hybrid},
takes into account the remarks made using the results of IPPC14:
an approach using Probability and Possibility Theory
in order to benefit from both approaches in the resolution of factored POMDPs
is presented: quantitative information of the problem is kept to avoid the highlighted modeling issues,
and the belief state is handled in a possibilistic way, in order to get a smart discretization of it
and to benefit from a finite and factorized belief state spaces.
This approach leads to a factored probabilistic MDP 
which can be solved for instance by GOURMAND or PROST
(which does not use the memory constraining LISP-like encoding).
