%%%fisherman's wharf hostel + bike
\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
Les contributions de cette thèse
sont principalement centrées
autour des travaux préliminaire de Régis Sabbadin \cite{Sabbadin1999pipomdp}.
Ce dernier propose
un homologue possibiliste qualitatif aux PDMPO
modélisant l'incertitude 
avec des distributions de possibilité qualitatives. 
%%% CONCL INTRO
Ainsi, dans notre travail, 
des modèles possibiliste qualitatifs
pour la planification sous incertitude
ont été développés et étudiés.
Cette étude a été motivée 
par différents problèmes posés par les PDMPOs  
détaillés en introduction.

La motivation majeure de cette étude est
la réduction de la complexité des calculs
offerte par les $\pi$-PDMPO: 
son espace d'états de croyance est fini,
tandis que l'espace des états de croyance probabilistes est infini.
De plus, comme ses états de croyance
sont des distributions de possibilité
sur l'espace des états,
l'ignorance totale peut être définie
par une distribution de possibilité 
égale à $1$ sur tous les états.
Si un état donné du système 
est parfaitement connu
comme étant l'état courant,
l'état de croyance associé attribue $1$ à cet état du système
et $0$ à tous les autres puisqu'ils sont impossibles:
c'est une représentation plus appropriée que 
la probabilité uniforme.
L'imprécision des distributions de probabilités
est aussi naturellement gérée par le formalisme possibiliste.

Plus généralement,
les modèles possibilistes qualitatifs
ont besoin de moins d'information à propos du système 
que le modèle probabiliste:
les plausibilité des événements
sont ``seulement'' classifiées
dans l'échelle possibiliste $\mathcal{L}$ plutôt que quantifiée.
Cette thèse propose finalement
des contribution théoriques et pratiques
à propos de ces processus possibilistes qualitatifs:
d'une part, les contributions théoriques
sont par exemple l'observabilité mixte,
la gestion des horizons indéfinis, 
ou les résultats d'indépendance.
D'autre part, la principale contribution pratique
est la démonstration que ces modèles 
possibilistes qualitatifs ont un intérêt
dans la simplification des calculs ou la modélisation
via des résultats expérimentaux (par exemple IPPC 2014)
ou l'étude du comportement de la croyance possibiliste.

Ces contributions sont amenées à travers des applications robotiques
afin de faire le lien avec les problématiques de départ:
des missions de reconnaissance de cible sont étudiées 
(par exemple le problème \textit{Rocksample}, 
ou même la mission décrite par la figure \ref{robotgridfig});
IPPC 2014 contient aussi des problèmes comparables à des systèmes robotiques 
(par exemple le problème \textit{Elevator}, ou le problème \textit{Tamarisk}, 
aussi utilisé lors de la compétition d'apprentissage par renforcement 2014). 
\nico{
Un aperçu plus détaillé des contributions suit.

\subsection*{New features for the $\pi$-POMDPs
and first strategy executions}
%%%%%%%%%%%%%
%%% chap2 %%%
%%%%%%%%%%%%%
%%THEORETICAL CONTRIBUTIONS
We then adapted $\pi$-POMDPs 
to mixed-observability \cite{OngShaoHsuWee-IJRR10},
which is part of the theoretical contributions of our work:
this contribution, called $\pi$-MOMDP, 
dramatically reduces the size of the belief space 
and thus allows the first computations of strategies from $\pi$-POMDPs. 
Finally, another theoretical contribution 
is the qualitative counterpart 
to the value iteration algorithm,
with the associated criterion 
for time-unbounded executions. 
As proved in Annex \ref{theorem_DPpiMOMDP_RETURN}
(and our publication \cite{Drougard13}), 
there exists an optimal strategy, 
which is stationary 
\textit{i.e.} which does not depend on the stage of the process $t$. 
This strategy can be computed 
by the proposed dynamic programming scheme: 
it is also shown that 
the number of iterations 
to make the value function converge 
is less than the size of the state space.
The assumption of the existence of a ``stay'' action
is not a constraint in practice
as it is only selected 
in some goal states.
Note also that the target recognition missions 
presented in this thesis 
have typically unbounded durations.

%%%PRACTICAL ONES
As already pointed out,
the experiments in the second chapter 
use both previous theoretical contributions.
Indeed, the mixed-observability property of the problem,
makes computations feasible for our robotic example.
The proposed criterion is also really useful: 
it is convenient to allow 
the computation of strategies 
for robotic missions 
with unbounded durations.
For instance, in our example, 
it allows to define the mission properly: 
if the robot has not figured out 
which target is right one,
we want the mission to be continued.
The first practical contributions of this thesis 
is thus the computation of a strategy
for this robotic mission and its execution.
Indeed, we have shown 
that the qualitative possibilistic approach 
can outperform probabilistic POMDP ones 
for a target recognition problem 
where the agent's observations dynamics 
is not accurately defined.
Note that the only information used 
to define system observation of the system,
is that the more the robot is far from a target, 
the more the observation is noisy:
it already shows that the possibilistic framework
may be useful in case of restricted knowledge 
about the problem.


%%%OBSERVATIONS
Finally, the second chapter highlights 
a very interesting behavior 
of qualitative possibilistic belief states:
under some conditions, 
the possibilistic belief update, 
which is defined from the counterpart of Bayes rule \cite{Dubois199023},
increases the knowledge associated to the belief state.
Indeed, in our example, 
the belief state is responsible of the imprecision 
as it only takes into account 
more reliable observations,
and may also change to the opposite belief
if an observation contradicts the current one.
%the next belief is either more skeptic 
%about a state if its confirms the prior belief; 
%or changes to the opposite belief if it contradicts it.
On the contrary a probabilistic belief 
is modified in most cases
(there is a finite number of normalized eigenvectors for a given matrix). 
Some conditions leading to this behavior 
are then presented,
and associated proofs are given 
(see Annex \ref{theorem_specificity_belief_RETURN}).

\subsection*{Graphical work on independence and factorization (PPUDD)}
%%%%%%%%%%%%%
%%% chap3 %%%
%%%%%%%%%%%%%
%%THEORETICAL CONTRIBUTIONS
%%CHAP3
The next theoretical contribution is 
the introduction of the factored $\pi$-MOMDPs:
indeed, we have considered processes
whose state space is represented by $n$ 
post-action independent variables,
\textit{i.e.} processes whose 
state variables of the same time step
are independent (conditional on the past).
The algorithm proposed to exactly solve 
these factorized processes,
called PPUDD, is another contribution:
it is the symbolic version of 
the dynamic programming scheme 
proposed in the previous chapter.
Inspired by SPUDD \cite{Hoey99spudd:stochastic}, 
PPUDD means \emph{Possibilistic Planning Using Decision Diagrams}. 
As SPUDD, it operates on ADDs 
encoding value, preference and transition functions.
Finally, the last theoretical contributions of the third chapter
are the proofs of independence results,
Theorem \ref{thmSHind} and Theorem \ref{thmVARind}:
they relate to a proposed graphical model
representing a particular factorization of the process
\textit{i.e.} particular independence assumptions
on the variables.
Thus, we have shown that 
these independence assumptions
lead to a natural factorization 
of the space of the belief states.
The belief state variables of a $\pi$-MOMDP 
satisfying these assumptions
are post-action independent,
and the associated problem 
can be more easily solved by PPUDD. 
The independence of sensors 
and of corresponding hidden state variables 
suffices to fulfill these conditions:
for instance, the \textit{RockSample} problem, 
well illustrates these conditions.

%%%% PRACTICAL CONTRIB
%% WEE!
The motivation leading to the design of PPUDD,
\textit{i.e.} the guess that the use of operators $\min$ and $\max$ 
leads to smaller ADDs, and thus to faster computations,
has been verified in practice:
our experiments and the results 
of the International Probabilistic Planning Competition (IPPC 2014\footnote{\url{https://cs.uwaterloo.ca/~mgrzes/IPPC_2014/}})
show that this possibilistic approach
can involve less computation time
and produce better policies
than its probabilistic counterparts
when computation time is limited,
or for high dimensional problems.
For instance, 
PPUDD performances have been compared 
to the probabilistic MOMDP solver APPL \cite{Kurniawati-RSS08,OngShaoHsuWee-IJRR10}
and the symbolic HSVI solver \cite{Sim:2008:SHS:1620163.1620241},
in terms of computation time, 
and using the average of 
the total reward at execution. 
PPUDD computes a strategy maximizing 
exactly the possibilistic criterion 
while APPL and symb. HSVI compute strategies
which approximately maximize the expected sum of rewards.
The experimental results on the Rocksample problem 
show that using an exact algorithm 
(PPUDD) for an approximate model ($\pi$-MOMDPs) 
can run significantly faster 
than reasoning about exact models, 
while providing better policies 
than approximate algorithms (APPL) 
for exact models.
Most of the cited contributions of this chapter
have been published in \cite{DBLP:conf/aaai/DrougardTFD14},
and an implementation of PPUDD 
to reproduce experiments 
can be found 
at the repository \url{www.github.com/drougui/ppudd}.

%% IPPC14
Finally, in order to focus on the behavior 
of the possibilistic qualitative approach 
in a wide panel of probabilistic problems, 
the next practical contribution 
is the participation 
in the fully observable track of IPPC 2014
with adapted versions of PPUDD.
The implementation of our solver for this competition 
was performed with the \textit{CU Decision Diagram Package}
for the ADDs computations. 
Comparing only solvers using ADDs, 
PPUDD and a probabilistic solver 
called symbolic LRTDP \cite{symbLRTDP}
(as a variant of \cite{Bonet03labeledrtdp}),
our solver produces better strategies 
than the probabilistic approach.

\subsection*{Discussion about the current results}
%% fin 3 lien CHAP4-5
%% ??  after? critique
Experiments concerning the Navigation problem
is a good illustration of the optimistic and pessimistic criteria
in the qualitative possibilistic framework: 
a robot has to reach a goal as soon as possible,
avoiding unsafe locations 
(for instance, locations where there is a risk that it falls down and break).
The strategy maximizing 
the optimistic criterion 
makes the robot 
unfrequently reach the goal,
but more quickly
than the one from the pessimistic criterion
which makes it however often reach the goal.  
Proposed approaches involve then 
the choice of a criterion.
This feature of the possibilistic approach
explains also the difficulties experienced
with the ``Traffic'' domain
of the competition: 
the optimistic criterion can be too optimistic
for the given problem, 
although strategies from it are generally 
more efficient than those from the pessimistic criterion
(that is why the optimistic criterion has been used for the competition).

Presented models 
assume also that preference
and uncertainty degrees
share the same scale
which makes it hard to
set in practice.
The fourth chapter about human-machine interaction
is meant to show that,
although qualitative possibilistic approach
may raise questions concerning modeling,
this approach can be very appropriate
in some practical situations,
as in our case, to produce diagnosis.

We got really good results 
with PPUDD in previous experiments 
among symbolic algorithms (SPUDD, symbolic LRTDP):
for instance, SPUDD provides good strategies, but 
cannot solve big instances of the tested problem. 
However, state space search algorithms
(PROST \cite{DBLP:conf/aips/KellerE12} and GOURMAND \cite{DBLP:conf/aaai/KolobovMW12}) won IPPC 2014,
and are yet far more efficient than ADD-based methods:
MDP instances with a complex structure
are encoded with ADDs big enough
to disqualify these approaches.
These observations and modeling considerations
are the motivations of the last chapter,
presenting the hybrid POMDP.
}


\section*{Perspectives}
%%%
%%% PERSPECTIVES
%%%
La première perspective vient d'une observation:
les problèmes de mémoire atteints avec le domaine \textit{Triangle tireworld} 
d'IPPC 2014  pourrait être contourné à l'aide d'une discrétisation plus important:
les version de PPUDD pour IPPC 2014 
utilisaient une précision de $10^{-3}$
sur le problème probabiliste initial,
menant à une importante échelle possibiliste $\mathcal{L}$.
Il serait instructif d'avoir une idée de l'impact de ce traitement
sur les performances de PPUDD: 
plus généralement, un travail plus important
sur la traduction d'un PDM probabiliste
en approximation possibiliste 
pourrait améliorer de manière significative
l'approche proposée pour IPPC 2014.

En terme de méthode de calculs,
nous nous sommes concentrés sur des résolutions symboliques
des $\pi$-PDM (PPUDD).
Cependant, des méthodes de résolutions alternatives
pourraient être étudiées:
par exemple, des méthodes heuristiques,
qui sont efficaces pour les problèmes probabilistes \cite{Teichteil-KonigsbuchVI11},
pourraient être adaptés
au contexte possibiliste.
En ce qui concerne l'apprentissage par renforcement
dans le cadre des possibilités qualitatives,
nous pouvons citer le travail suivant \cite{Sabbadin01}.

Enfin, les avancées de la théorie des possibilités
peuvent permettre de raffiner 
les PDM possibiliste qualitatifs
afin d'améliorer la modélisation lorsque c'est nécessaire.
Nous pouvons par exemple mentionner l'opérateur \textit{leximin} \cite{lexirefin}:
il permet d'éviter l'effet de noyade
des opérateurs $\min$ et $\max$.
Il peut être utilisé pour l'agrégation de préférences
ou pour calculer un degré de possibilité plus fin d'une trajectoire.
Notons que cela peut être une amélioration utilise, 
mais qu'elle complexifie inévitablement le problème.
Des critères plus discriminant sont aussi étudiés dans la littérature
\cite{LIP61723,GiangS01}.
