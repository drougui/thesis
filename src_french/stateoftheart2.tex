Le principal sujet de cette thèse est le processus décisionnel markovien partiellement observable (PDMPO),
ou plus précisément son homologue possibiliste qualitatif.
L'utilisation pratique du modèle probabiliste a été critiquée en introduction,
cependant il résume de manière appropriée les principales caractéristiques d'un système robotique:
son homologue possibiliste étant très similaire, son étude semble prometteuse.
Tout d'abord, la théorie des possibilités qualitative est présentée dans le but
d'introduire les \textit{processus décisionnels markoviens possibilistes qualitatifs} ($\pi$-PDM)
et enfin les \textit{processus décisionnels markoviens partiellement observables} ($\pi$-PDMPO)
qui constituent le point de départ de notre travail.\\

\subsection{Possibility Theory}
\label{posspres}
Les \textit{ensembles flous} ont été introduits par Lotfi Zadeh \cite{Zadeh1965338},
et étudiés par Didier Dubois \cite{didiersgroundhogday} et Henri Prade:
leur contributions ont mené à la fondation de la théorie des possibilités \cite{DuPr1988.4}.

Comme dans la théorie des probabilités,
cette théorie est basée sur une mesure d'incertitude,
appelée \textit{mesure de possibilité}.
Contrairement à la mesure de probabilité mesure $\mathbb{P}$ 
qui est une mesure classique,
la mesure de possibilité, notée $\Pi$,
est une \textit{mesure floue}, ou \textit{capacité}.
Pour faire simple,
une mesure floue n'est pas supposée additive,
mais juste \textit{monotone},
\textit{i.e.} si $A \subset B$
alors la mesure de $A$ est plus petite que la mesure de $B$.
Dans cette thèse, les mesures de possibilité vont concerner uniquement
des ensembles finis comme l'ensemble des états $\mathcal{S}$
et l'ensemble des observations $\mathcal{O}$.

Formellement, une mesure de possibilité est définie comme suit:
\begin{Def}[Possibility Measure]
\label{poss_measure}
Une mesure de possibilité sur l'ensemble fini $\Omega$ 
est une fonction $2^{\Omega} \rightarrow [0,1]$
telle que
\begin{itemize}
\item $\Pi(\Omega) = 1$ (\textit{normalization});
\item $\forall A,B \subset \Omega$, $\Pi \set{ A \cup B } = \max \big\{ \Pi(A), \Pi(B) \big\}$ (\textit{maxitivité}).
\end{itemize}
\end{Def}

La théorie des probabilité modélise l'incertitude 
due à la variabilité des événements:
en pratique, les probabilités sont les fréquences estimées des événements,
définis comme le vrai modèle de variabilité des événements.
Une autre interprétation de cette théorie est celle des probabilités subjectives 
définies par De Finetti \cite{de1974theory}:
la valeur de probabilité d'un événement 
est un pari échangeable, 
relatif aux connaissances d'une personne donnée.

La théorie des possibilités est dédiée à l'incertitude
due à un manque de connaissance, ou une imprécision à propos d'un événement.
La théorie des possibilités quantitatives est un cas particulier de probabilités imprécises
\textit{i.e.} une mesure de possibilité $\Pi$ représente un ensemble particulier
de mesures de probabilité définies sur $\Omega$.

Contrairement à la théorie des possibilités quantitatives,
la théorie des possibilités qualitatives utilise des mesures de possibilité
dont les valeurs sont définies sur n'importe quelle échelle ordonnée.
Cette théorie nous permet de raisonner, même lors d'un manque d'informations quantitatives:
la seule information donnée par une mesure de possibilité qualitative
est l'ordre de plausibilité entre les événements
\textit{i.e.} pour $A,B \subseteq \Omega$,
l'information ``l'événement $A$ est moins plausible que l'événement $B$'', 
qui est s'écrit $\Pi(A) \leqslant \Pi(B)$.
Ainsi les mesures de possibilité qualitatives $\Pi$
sont souvent définies 
comme des fonctions $2^{\Omega} \rightarrow \mathcal{L}$,
où $\mathcal{L}$ est un ensemble fini 
appelé \textit{échelle possibiliste}
associé à un ordre total. 
Dans ce travail, et plus spécifiquement dans les trois premiers chapitres de cette thèse,
l'échelle possibiliste est définie par $\mathcal{L} = \set{ 0, \frac{1}{k}, \ldots, 1 }$
pour simplifier les notations.

Nous pouvons définir la \textit{distribution de possibilité} 
$\pi$: $\pi(\omega) = \Pi(\set{\omega})$.
D'après la définition \ref{poss_measure},
la mesure de possibilité est entièrement 
définie par la distribution $\pi$.
Pour chaque distribution (ou mesure) de possibilité,
une mesure duale, appelée mesure de nécessité, peut être définie:
le degré de nécessité d'un événement augmente si le degré de possibilité de l'événement contraire décroît.
\vbox{
\begin{Def}[Mesure de nécessité associée à $\Pi$]
\label{DEF_necessity}
La mesure de nécessité associée à $\Pi$ est la mesure floue $\mathcal{N}:2^\Omega \rightarrow \mathcal{L}$ 
telle que $\forall A \subset \Omega$, \[ \mathcal{N}(A)=1-\Pi(\overline{A}), \]
où $\overline{A}$ est l'événement complémentaire $A$ dans $\Omega$. 
\end{Def}
}
\begin{figure} 
\center
\begin{tikzpicture}
\definecolor{ggreen}{rgb}{0.3,0.7,0.4};
\begin{axis}[xtick=\empty,ymin=-0.19]
\foreach \xx in {-2,-1.5,-1,-0.5,0,0.5,1,1.5,2} {
\addplot[black,dotted] coordinates {(\xx,0) (\xx,1)};
}
\addplot[mark=none,blue, ultra thick] coordinates { (-2, 0) (-1, 0) (-1, 0.6) (-0.5, 0.6) (-0.5, 1) (0, 1) (0,0.8) (1,0.8) (1,0.2) (1.5,0.2) (1.5,0) (2,0)  };
\addplot[mark=none,ggreen, very thick, densely dashed] coordinates { (-2, 0) (-1, 0) (-1, 0 )  (-0.5, 0)   (-0.5, 1) (0, 1) (0,0.6) (0.5, 0.6) (0.5,0.4) (1,0.4) (1,0.2) (1.5,0.2) (1.5,0) (2,0)  };
\end{axis}
\node (pi1) at (4.6,4.6) { {\color{blue} $\pi_1$} };
\node (pi1) at (3.9,3.7) { {\color{ggreen} $\pi_2$} };

\node (a) at (0.9,0.4) {$a$};
\node (b) at (1.63,0.45) {$b$};
\node (c) at (2.36,0.4) {$c$};
\node (d) at (3.09,0.45) {$d$};
\node (e) at (3.82,0.4) {$e$};
\node (f) at (4.55,0.45) {$f$};
\node (g) at (5.28,0.4) {$g$};
\node (h) at (6,0.45) {$h$};

\draw[red, thick] (2.85,0.7) -- (4.8,0.7) -- (4.8,0.2) -- (2.85,0.2) -- (2.85,0.7);
\draw[<->,>=latex,red, thick] (2.4,3.5) -- (2.4,5.2);
\draw[<->,>=latex,orange, thick, dashed] (5.1,1.75) -- (5.1,5.2);

\node (nec) at (1.2,4.5) {{\color{red} $\mathcal{N}_1(\set{ d, e, f })$}};
\node (nec) at (6,3.5) [rotate=300] {{\color{orange} $\mathcal{N}_2(\set{ d, e, f })$}};

\end{tikzpicture}
\caption[Distribution de possibilité, mesure de necessité et spécificité]{
Exemple de deux distributions de possibilité sur $\Omega=\set{a,b,c,d,e,f,g,h}$:
$\pi_1$ (ligne bleue continue) et $\pi_2$ (ligne verte pointillée), 
avec $\pi_2$ qui est plus spécifique que $\pi_1$.
La mesure de nécessité $\mathcal{N}_1$ 
associée à $\pi_1$
est évaluée sur l'événement $\set{d,e,f} \subset \Omega$: 
le degré de nécessité est égal à $0.4 = 1-0.6$, 
comme illustré par les flèches rouges continues.
La mesure de nécessité $\mathcal{N}_2$
associée à $\pi_2$ 
est évaluée sur le même événement:
le degré de nécessité est égal à $0.8 = 1 - 0.2$, 
comme illustré
par les flèches orange en pointillées. }  
\label{figure_necspec}
\end{figure}

L'ignorance totale est modélisée par une distribution de possibilité $\pi$ 
telle que $\forall \omega \in \Omega$,
$\pi(\omega)=1$ \textit{i.e.} 
n'importe quel événement élémentaire est possible.
Dans ce cas, 
$\forall A \subseteq \Omega$, 
$A \neq \Omega$, 
$\mathcal{N}(A)=1-\Pi(\overline{A})=1-\max_{\omega \in \overline{A}} \Pi(\omega)=0$:
contrairement à l'univers $\Omega$,
aucun événement n'est nécessaire. 

Au contraire, la connaissance parfaite que l'événement élémentaire $\omega_A \in \Omega$
est modélisé par un distribution de possibilité $\pi$
telle que $\pi(\omega_A) = 1$
et $\pi(\omega)=0$, $\forall \omega \neq \omega_A$. 
La nécessité du singleton $\set{\omega_A}$
est aussi égal à un: $\mathcal{N}(\set{\omega_A})=1-\Pi(\overline{\set{\omega_A }}) = 1$. 
L'événement élémentaire $\omega_A$
est nécessaire, et tous les autres ont un degré de nécessité nul:
si $\omega_B \neq \omega_A$, 
$\mathcal{N}(\set{\omega_B})=1-\Pi(\overline{\set{\omega_B }}) = 1 - \pi(\omega_A) = 0$.

Généralement, une distribution de possibilité est plus informative, ou plus \textit{spécifique}, que l'ignorance totale
et moins \textit{spécifique} que la connaissance parfaite:
\begin{Def}[Spécificité]
\label{def_specificity}
Une distribution de possibilité $\pi_2$
est plus spécifique que la distribution de possibilité $\pi_1$, 
si $\forall \omega \in \Omega$,
\[ \pi_2(\omega) \leqslant \pi_1(\omega).\]
\end{Def}
Les deux notions de nécessité et de spécificité sont illustrée en figure \ref{figure_necspec}.

Les principaux concepts de la théorie des possibilité ont été présentés.
L'homologue du critère basé sur espérance dans les modèles probabilistes,
nous présentons maintenant le critère qualitatif utilisé dans les modèles possibilistes qualitatifs.

\subsection{Critères Qualitatifs}
\label{subsection_qualcrit}

\begin{figure}[b!]
\center
\begin{tikzpicture}
\definecolor{ggreen}{rgb}{0.3,0.7,0.4};
\begin{axis}[xtick=\empty,ymin=-0.19]
\foreach \xx in {-2,-1.5,-1,-0.5,0,0.5,1,1.5,2} {
\addplot[black,dotted] coordinates {(\xx,0) (\xx,1)};
}
\addplot[mark=none, red, ultra thick] coordinates { (-2, 0) (-1, 0) (-1, 0.2) (-0.5, 0.2) (-0.5, 0.4) (0, 0.4) (0,0.6) (1,0.6) (1,0.8) (1.5,0.8) (1.5,1) (2,1)  };
\addplot[mark=none, blue, ultra thick, densely dashed] coordinates { (-2, 1) (-1, 1) (-1, 1)  (-0.5, 1)   (-0.5, 0.8) (0, 0.8) (0,0.6) (0.5, 0.6) (0.5,0.4) (1,0.4) (1,0.2) (1.5,0.2) (1.5,0) (2,0)  };
\addplot[mark=none, black, ultra thick, dotted]%densely dashed] 
coordinates { (-2, 0.6) (2,0.6)  };
\end{axis}
\node (fff) at (4.8,4.8) { {\color{red} $f(\omega_i)$} };
\node (mumu) at (1.5,4.7) { {\color{blue} $\mu(A_i)$} };
\node (sug) at (1.5,3) {$\mathbb{S}_{\mu}[f]$};

\node (a) at (0.9,0.45) {$\omega_1$};
\node (b) at (1.63,0.45) {$\omega_2$};
\node (c) at (2.36,0.45) {$\omega_3$};
\node (d) at (3.09,0.45) {$\omega_4$};
\node (e) at (3.82,0.45) {$\omega_5$};
\node (f) at (4.55,0.45) {$\omega_6$};
\node (g) at (5.28,0.45) {$\omega_7$};
\node (h) at (6,0.45) {$\omega_8$};
\draw[ggreen, thick] (3.5,0.7) -- (6.3,0.7) -- (6.3,0.2) -- (3.5,0.2) -- (3.5,0.7);
\node (h) at (4,1) {{\color{ggreen} $A_5$}};

\end{tikzpicture}
\caption[Résultat de l'intégrale de Sugeno]{ 
illustration du résultat de l'intégrale de Sugeno: 
l'axe des abscisses représente l'ensemble $\Omega = \set{ \omega_1, \ldots, \omega_{\# \Omega} }$,
où $\forall i \in \set{1,\ldots,\# \Omega-1}$, $f(\omega_i) \leqslant f(\omega_{i+1})$. 
L'axe des ordonnées est $\mathcal{L}$. 
La courbe rouge représente les degrés $f(\omega_i)$, 
la bleue en pointillées représente les degrés $\mu(A_i)$ 
avec $A_i = \set{ \omega_i, \ldots, \omega_{\# \Omega} }$,
et la noire est le résultat de l'intégrale de Sugeno. }  
\label{figure_sugeno}
\end{figure}

L'espérance utilisée comme critère dans les PDMPO probabilistes,
est finalement l'integrale de la la fonction de récompense
contre la mesure de probabilité.
Le concept de l'intégrale a été étendue aux mesure floues:
Quand la mesure est quantitative, l'extension est appelée \textit{intégrale de Choquet} \cite{Choquet1954}.
Dans le cas des mesure qualitatives, l'objet résultant est l'\textit{intégrale de Sugeno} \cite{Sugeno74}.
Nous définissons donc ici l'intégrale de Sugeno:
\begin{Def}[Integrale de Sugeno]
\label{sugeno_integral}
L'intégrale de Sugeno d'une fonction $f:\Omega \rightarrow \mathcal{L}$ 
contre la capacité (mesure floue) $\mu:2^{\Omega} \rightarrow \mathcal{L}$ est
\begin{align} 
\label{equation_sugeno1} \mathbb{S}_{\mu} \croch{ f } &= \max_{i=1}^{\# \Omega} \min \set{ f(\omega_i), \mu(A_i) } \\
\label{equation_sugeno2} &=  \min_{i=1}^{\# \Omega} \max \set{ f(\omega_i), \mu(A_{i+1}) }
\end{align}
où $f(\omega_1) \leqslant \ldots \leqslant f(\omega_{\# \Omega})$, 
 $A_i = \set{ \omega_i, \omega_{i+1}, \ldots, \omega_{\# \Omega} }$
et $A_{\#\Omega+1} = \emptyset$.
\end{Def}

Comme illustré dans la figure \ref{figure_sugeno},
l'intégrale de Sugeno de $f: \Omega \rightarrow \mathcal{L}$ 
contre la mesure floue $\mu$
est le plus grand degré $\lambda \in \mathcal{L}$ 
tel que la mesure $\mu$ of $\set{ \omega \sachant f(\omega) \geqslant \lambda}$ 
est plus grande ou égale à $\lambda$. 
Par exemple, le $h$-indice
(ou indice de Hirsh)
est l'intégrale de Sugeno de la fonction $paper \mapsto \# citations$
contre la mesure de comptage. 

L'intégrale de Sugeno contre une mesure de possibilité
et celle contre la nécessité,
mènent à deux critères pour la planification.
Ces intégrales se réécrivent comme suit:
\begin{theorem}[L'intégrale de Sugeno contre les mesures de possibilité et de nécessité]
\label{sugenoPossNec}
\begin{align}
\label{equation_sugenoposs} \mathbb{S}_{\Pi}[f] &= \max_{i=1}^{\# \Omega} \min \set{ f(\omega_i), \pi(\omega_i) },\\
\label{equation_sugenonec} \mathbb{S}_{\mathcal{N}}[f] &= \min_{i=1}^{\# \Omega} \max \set{ f(\omega_i), 1-\pi(\omega_i) } .
\end{align}
sont les réécritures des intégrales de Sugeno contre les mesures de possibilité et de nécessité.
\end{theorem}

Les critères possibilistes qualitatifs,
\textit{i.e.} les fonctions $\mathcal{A} \rightarrow \mathcal{L}$ 
mesurant la validité des actions étant donné un modèle possibiliste et de préférence,
a été proposé dans \cite{DBLPjournals/ijar/SabbadinFL98,DBLPjournals/eor/DuboisPS01,Dubois95possibilitytheory},
basé sur les intégrales de Sugeno (\ref{equation_sugenoposs}) and (\ref{equation_sugenonec}).
Rappelons que l'ensemble $\mathcal{S}$ (resp. $\mathcal{A}$) 
est comme précédemment l'ensemble fini des états du système $s$ (resp. des actions $a$).
La variable représentant l'état du système est $S \in \mathcal{S}$.
Soit $\big(\pi_a\big)_{a \in \mathcal{A}}$ 
une famille de distributions de possibilité sur $\mathcal{S}$,
\textit{i.e.} $\forall a \in \mathcal{A}$, $\pi_a(s) = \Pi_a( \set{S = s} )$
est le degré de possibilité de la situation $\set{S=s} \subset \Omega$ 
lorsque l'action $a \in \mathcal{A}$ est sélectionnée. 
Soit $\rho: \mathcal{S} \rightarrow \mathcal{L}$ la fonction de préférence,
définissant le degré de préférence de chaque état du système $s \in \mathcal{S}$.

\begin{Def}[Critère de décision qualitatif]
Soit $\pi_a$ la distribution de possibilité
décrivant l'incertitude à propos de l'état du système
étant donné que l'action $a \in \mathcal{A}$ a été sélectionnée, 
et $\rho(s)$ la préférence de l'état du système $s \in \mathcal{S}$.
En utilisant la formule (\ref{equation_sugenoposs}) avec $f=\rho(S)$, 
l'intégrale de Sugeno integral de la préférence contre la mesure de possibilité $\Pi_a$
mène à un critère optimiste pour $a \in \mathcal{A}$:
\label{def_qualcrit}
\begin{align}
\label{equation_critopt} \mathbb{S}_{\Pi_a}[\rho(S)] &= \max_{s \in \mathcal{S}} \min \set{ \rho(s), \pi_a(s) }.
\end{align}
De même, en utilisant la formule (\ref{equation_sugenonec}) avec $f=\rho(S)$, 
l'intégrale de Sugeno de la préférence contre la mesure de nécessité associée à $\Pi_a$, $\mathcal{N}_a$, 
mène au critère pessimiste pour $a \in \mathcal{A}$:
\begin{align}
\label{equation_critpess} \mathbb{S}_{\mathcal{N}_a}[\rho(S)] &= \min_{s \in \mathcal{S}} \max \set{ \rho(s), 1-\pi_a(s) }.
\end{align}
\end{Def}

Le critère pessimiste (\ref{equation_critpess}) a tendance à éviter les états non désirés,
tandis que le critère optimiste (\ref{equation_critopt}) souhaite rendre possible le fait d'atteindre les états préférés.
La figure (\ref{figure_criteria}) illustre le résultat du critère 
pour une action donnée $a\in\mathcal{A}$.

L'exemple suivant, illustré en figure \ref{figure_example},
montre bien la différence entre ces deux critères:
soit $\mathcal{S} = \set{ s_A , s_B , s_C }$ l'ensemble des états
et l'ensemble des actions $\mathcal{A} = \set{ a_1 , a_2 }$. 
Le modèle de préférence et le modèle d'incertitude
sont décrits respectivement par $\rho$ et $\paren{\pi_a}_{a \in \mathcal{A}}$:
\begin{itemize}
\item[$\bullet$] $1 = \rho(s_A) > \rho(s_B) > \rho(s_C) = 0$;
\item[$\bullet$] si l'action $a_1$ est sélectionnée, $\pi_{a_1}(s_A) = \pi_{a_1}(s_C) = 1$, et $\pi(s_B) = 0$;
\item[$\bullet$] si l'action $a_2$ est sélectionnée, $\pi_{a_2}(s_A) = \pi(s_C) = 0$, et $\pi(s_B) = 1$, \textit{i.e.}
le système est dans l'état $s_B$ de manière déterministe.
\end{itemize}

\begin{figure}
\center
\begin{tikzpicture} 
\tikzstyle{vertex}=[circle,fill=black!30,minimum size=20pt,inner sep=0pt]
\node[vertex] (G1) at (0,1) {$s_A$};
\node[vertex] (G2) at (0,0) {$s_B$};
\node[vertex] (G3) at (0,-1) {$s_C$};
\node (G1b) at (-3,1) {};
\node (G2b) at (-3,0) {};
\node (G3b) at (-3,-1) {};

\node (poss1) at (-1.7,1.3) {$\pi_{a_1}(s_A)=1$};
\node (poss2) at (-1.7,0.3) {$\pi_{a_1}(s_B)=0$};
\node (poss3) at (-1.7,-0.7) {$\pi_{a_1}(s_C)=1$};
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (G1b) to (G1);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (G2b) to (G2);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (G3b) to (G3);

\node[vertex] (GG1) at (5,1) {$s_A$};
\node[vertex] (GG2) at (5,0) {$s_B$};
\node[vertex] (GG3) at (5,-1) {$s_C$};
\node (GG1b) at (2,1) {};
\node (GG2b) at (2,0) {};
\node (GG3b) at (2,-1) {};

\node (poss1) at (3.3,1.3) {$\pi_{a_2}(s_A)=0$};
\node (poss2) at (3.3,0.3) {$\pi_{a_2}(s_B)=1$};
\node (poss3) at (3.3,-0.7) {$\pi_{a_2}(s_C)=0$};
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (GG1b) to (GG1);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (GG2b) to (GG2);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (GG3b) to (GG3);

\node (mu1) at (8,1.2) {$\rho(s_A)=1$};
\node (mu2) at (8.15,0.2) {$\rho(s_B)=0.5$};
\node (mu3) at (8,-0.8) {$\rho(s_C)=0$};
\end{tikzpicture}
\caption[Exemple d'une situation pour illustrer les critères qualitatifs]{Illustration de l'example de la section \ref{subsection_qualcrit}
à propos des critères qualitatifs. L'action $a_1$ maxmise le critère optimiste (\ref{equation_critopt}),
qui peut mener au meilleur état ($s_A$), 
mais aussi au pire ($s_C$).
Au contraire, l'action $a_2$ maximise
le critère pessimiste (\ref{equation_critpess})
puisque le pire cas n'est pas atteignable avec cette action.}  \label{figure_example}
\end{figure}

Le critère optimiste est maximisé par l'action $a_1$,
puisqu'avec cette action, le meilleur état du système, $s_A$, 
est entièrement possible.
Cependant, cette action rend le pire état du système, $s_C$,
entièrement possible, et l'état $s_A$ n'est pas du tout nécessaire: 
une action plus prudente est $a_2$, avec laquelle l'état devient $s_B$
avec certitude, mais avec une plus petite préférence.
On peut vérifier facilement que l'action $a_2$ maximises bien le critère pessimiste (\ref{equation_critpess}):

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{$\pi$-MDPs}
\label{subsection_piMDPs}
Ce modèle, présenté dans \cite{Sabbadin2001287,Sabbadi00,Sabbadin1999pipomdp}, 
est la version possibiliste qualitative des PDM probabilistes décrits en introduction,
basée sur les critères (optimistes et pessimistes) présentés en section \ref{subsection_qualcrit}:
cette version est appelée \textit{processus décisionnel markovien possibiliste qualitatif}, ou $\pi$-PDM.

L'ensemble fini des états du système
décrivant l'agent et son environnement, 
reste noté $\mathcal{S}$, 
comme vu en introduction avec les modèles probabilistes.
L'ensemble fini des actions est toujours $\mathcal{A}$ 
et $\mathcal{L}$ est l'échelle possibiliste $\set{ 0, \frac{1}{k}, \ldots, 1 }$,
avec $k\geqslant2$.

Comme dans le cas probabiliste, 
ce modèle considère que
les états successifs du système,
représentés par la séquence de variables $(S_t)_{t \in \mathbb{N}}$
avec $S_t \in \mathcal{S}$ $\forall t \geqslant 0$, is markovien.
Dans ce cadre possibiliste qualitatif,
cela signifie que la séquence $(S_t)_{t \in \mathbb{N}}$ est telle que
$\forall t \geqslant 0,  \forall (s_0,s_1,\ldots,s_{t+1}) \in \mathcal{S}^{t+2}$
et pour chaque séquence d'actions $(a_t)_{t\geqslant 0} \in \mathcal{A}^{\mathbb{N}}$, 
$S_{t+1}$ est independent from variables $\set{ S_0, \ldots, S_{t-1} }$,
conditioned on $\set{S_t = s}$ and $a_t$: 
\begin{equation}
\label{equation_possmarkov}
  \Pi \Big( \ S_{t+1}=s_{t+1} \ \Big\vert \ S_t=s_t, a_t \ \Big) = \Pi \Big( \ S_{t+1}=s_{t+1} \ \Big\vert \ S_t=s_t, S_{t-1}=s_{t-1}, \ldots, S_0=s_0, (a_t)_{t \geqslant 0} \ \Big).
\end{equation}
Cette indépendance possibiliste n'est pas la seule existante:
une présentation générale des indépendances et des conditionnements,
ainsi que de leurs conséquences dans les modèles graphiques
est disponible dans la thèse de N.Ben Amor \cite{Be2002.7}.

En utilisant cette propriété markovienne, 
la dynamique du système est entièrement décrite
avec les transitions possibilistes $\pi_t \paren{ s' \sachant s, a } = \Pi \Big( \ S_{t+1}=s' \ \Big\vert \ S_t=s, a \ \Big)  \in \mathcal{L}$:
$\forall t \geqslant 0$, $(s,s') \in \mathcal{S}^2$ et $a \in \mathcal{A}$,
$\pi_t \paren{ s' \sachant s, a }$ est le degré de possibilité, qu'à l'étape de temps $t$, 
le système atteigne l'état $s'$
lorsque l'agent choisit l'action $a$,
conditionné au fait que l'état courant est $s$.

Enfin, un $\pi$-PDM est entièrement défini avec la séquence de fonctions de préférence $(\rho_t)_{t=0}^{H-1}$,
où $\forall s \in \mathcal{S}$, $\forall a\in \mathcal{A}$, 
$\rho_t(s,a)$ est le degré de préférence lorsque l'état du système est $s$
et l'agent sélectionne l'action $a$
au temps $t$.
La fonction de préférence terminale, $\Psi$, 
donne pour chaque état du système $s \in \mathcal{S}$, 
le degré de préférence
si $S_H = s$: $\Psi(s)$.

Afin de définir les critères des $\pi$-PDM
à partir des critères possibilistes (\ref{equation_critopt}) and (\ref{equation_critpess}), 
nous introduisons, pour un horizon $H\geqslant0$, 
les \textit{trajectoires de longueur $H$} $\mathcal{T} = (s_1,\ldots,s_{H})$,
et $\mathcal{T}_H = \mathcal{S}^{H}$ l'ensemble de telles trajectoires.
Une règle de décision est notée $\delta: \mathcal{S} \rightarrow \mathcal{L}$, 
et une stratégie de longueur $H$ est une séquence de règles de décision $\delta_t$: $(\delta_t)_{t=0}^{H-1}$.
L'ensemble de toutes les stratégies de taille $H$ est noté $\Delta_H$.
Dans \cite{Sa1998.16}, 
pour une stratégie donnée $(\delta) \in \Delta_H$,
une séquence d'états du système $\mathcal{T} = (s_1,\ldots,s_H) \in \mathcal{T}_H$, 
et un état initial donné $s_0 \in \mathcal{S}$,
la \textit{préference d'une stratégie de taille $H$ commençant par $s_0$} 
est définie comme le degré de possibilité le plus petit de la trajectoire et de $s_0$:
\[ \rho \Big( \mathcal{T}, (\delta) \Big) = \min \set{ \min_{t=0}^{H-1} \rho\Big(s_t,\delta_t(s_t)\Big), \Psi(s_H)}.\]

En utilisant la propriété de Markov de ce processus d'états du système,
pour un état initial donné $s_0 \in \mathcal{S}$, 
un horizon $H \in \mathbb{N}$,
et une stratégie $(\delta_t)_{t=0}^{H-1}$, 
le degré de possibilité de la trajectoire $\mathcal{T} = (s_1, \ldots, s_H)$ est
\begin{equation}
\label{equation_possdistribtraj}
\Pi \paren{ S_H=s_h, S_{H-1}=s_{h-1}, \ldots, S_1 = s_1 \Big\vert S_0 = s_0, \big(\delta_t\big)_{t=0}^{H-1} } = \min_{t=0}^{H-1} \pi_{t+1} \Big( s_{t+1} \Big\vert s_{t}, \delta_{t}(s_{t}) \Big)
\end{equation}
noté $\pi \Big( \mathcal{T} \Big\vert s_0, (\delta) \Big)$.

L'intégrale de Sugeno de la préférence de la trajectoire
contre cette distribution est notée 
\[\mathbb{S}_{\Pi} \croch{ \rho \Big( \mathcal{T}, (\delta) \Big) \sachant S_0 = s_0, (\delta) } = \mathbb{S}_{\Pi} \croch{ \displaystyle \min \set{ \min_{t=0}^{H-1} \rho \Big(S_t,\delta_t(S_t) \Big), \Psi(S_H) } \sachant S_0 = s_0, (\delta) }\]
et correspond du critère optimiste definissant la stratégie optimale,
\textit{i.e.} une fonction valeur optimiste:
\begin{align} 
\overline{U_H} \Big( s_0,(\delta_t)_{t=0}^{H-1} \Big) &= \max_{\mathcal{T} \in \mathcal{T}_H} \min \set{ \rho \Big( \mathcal{T}, (\delta) \Big), \pi \Big( \mathcal{T} \Big\vert s_0, (\delta) \Big) }. \label{equation_optqualvalue} 
\end{align}
C'est équivalant du critère optimiste (\ref{equation_critopt}),
cependant, l'intégrale est sur les trajectoires $\mathcal{T}_H$, 
et la préférence dépend de la stratégie.
La \textit{stratégie optimale optimiste} $\overline{\delta^*}$ est la stratégie
maximisant la fonction valeur optimiste (\ref{equation_optqualvalue}),
et la \textit{fonction valeur optimiste optimale}
est la fonction valeur optimiste la plus grande en faisant varier $(\delta) \in \Delta_H$:
\begin{Def}[Fonction valeur et stratégie optimiste optimale]
$\forall s \in \mathcal{S}$,
\begin{align} 
\label{equation_optqualvaluestar} \overline{U_H^*}(s) &= \max_{(\delta) \in \Delta_H} \set{ \overline{U_H} \Big(s,(\delta)\Big) } & \mbox{ (fonction valeur optimale optimiste), } \\
\label{equation_optqualstratstar} \overline{\delta^*}(s) &\in \operatorname*{argmax}_{(\delta) \in \Delta_H} \set{ \overline{U_H} \Big(s,(\delta)\Big) } & \mbox{ (stratégie optimale optimiste). } 
\end{align}
\end{Def}

De même, le critère possibiliste qualitatif optimal (\ref{equation_critpess})
mène à un critère prudent pour les stratégies:
la fonction valeur pessimiste est
l'intégrale de Sugeno de la préférence de la trajectoire
contre la mesure de nécessité
qui vient de la distribution de possibilité
sur les trajectoires $\mathcal{T}_H$ (\ref{equation_possdistribtraj}) 
avec la stratégie $(\delta) \in \Delta_H$:
\begin{align} 
 \underline{U_H} \Big( s_0,(\delta_t)_{t=0}^{H-1} \Big) &= \min_{\mathcal{T} \in \mathcal{T}_H} \max \set{ \rho \Big( \mathcal{T},  (\delta) \Big), 1 - \pi \Big( \mathcal{T} \Big\vert s_0, (\delta) \Big) }. \label{equation_pessqualvalue}
\end{align}
notée $\mathbb{S}_{\mathcal{N}} \croch{ \rho \Big( \mathcal{T},  (\delta) \Big) \sachant S_0 = s, (\delta) }$.
Comme précédemment pour le cas optimiste,
la \textit{stratégie optimale optimiste} $\underline{\delta^*}$ 
est la stratégie maximisant
la fonction valeur pessimiste (\ref{equation_pessqualvalue}),
et la \textit{fonction valeur pessimiste optimale}
 est la fonction valeur maximale en faisant varier les stratégies $(\delta) \in \Delta_H$:
\\
\\
\vbox{
\begin{Def}[Fonction valeur et stratégie pessimiste optimale]
$\forall s \in \mathcal{S}$,
\begin{align} 
\label{equation_pessqualvaluestar} \underline{U_H^*}(s) &= \max_{(\delta) \in \Delta_H} \set{ \underline{U_H} \Big(s,(\delta)\Big) } & \mbox{ (fonction valeur optimale optimiste pessimiste), } \\
\label{equation_pessqualstratstar} \underline{\delta^*}(s) &\in \operatorname*{argmax}_{(\delta) \in \Delta_H} \set{ \underline{U_H} \Big(s,(\delta)\Big) } & \mbox{ (stratégie optimale pessimistic). } 
\end{align}
\end{Def}
}



As for the probabilistic MDPs (see Section \ref{subsectionDP} Theorem \ref{thm_mdp_finiteH}),
optimal value functions and strategies can be computed with Dynamic Programming:
\begin{theorem}[Dynamic Programming for $\pi$-MDPs]
\label{DPpiMDP}
The optimal optimistic criterion and an associated optimal strategy 
can be computed as follows: $\forall s \in \mathcal{S}$,
\begin{align}
\nonumber 
\overline{U_0^*}(s) & = \Psi(s), \mbox{\hspace{1cm} and, $\forall 1 \leqslant i \leqslant H$,} \\
\label{equation_recursiveopt} 
\overline{U_{i}^*}(s)& = \max_{a \in \mathcal{A}} \min \set{ \rho_{H-i}(s,a), \max_{s' \in \mathcal{S}} \min \set{ \pi_{H-i} \paren{ s' \sachant s, a  } , \overline{U^*_{i-1}}(s') }  }.
\end{align}
\begin{eqnarray}
\label{equation_recursiveoptstrat} 
\overline{\delta^*_{H-i}}(s) \in \operatorname*{argmax}_{a \in \mathcal{A}} \min \set{ \rho_{H-i}(s,a), \max_{s' \in \mathcal{S}} \min \set{ \pi_{H-i} \paren{ s' \sachant s, a  } , \overline{U^*_{i-1}}(s') }  }.
\end{eqnarray}
As well, the optimal pessimistic criterion and an associated optimal strategy 
can be computed as follows: $\forall s \in \mathcal{S}$,
\begin{align}
\nonumber 
\underline{U_0^*}(s) & = \Psi(s), \mbox{\hspace{1cm} and, $\forall 1 \leqslant i \leqslant H$,} \\
\label{equation_recursivepess} 
\underline{U_{i}^*}(s)& = \max_{a \in \mathcal{A}} \min \set{ \rho_{H-i}(s,a), \min_{s' \in \mathcal{S}} \max \set{ 1 - \pi_{H-i} \paren{ s' \sachant s, a  } , \underline{U^*_{i-1}}(s') }  }.
\end{align}
\begin{eqnarray}
\label{equation_recursivepessstrat} 
\underline{\delta^*_{H-i}}(s) \in \operatorname*{argmax}_{a \in \mathcal{A}} \min \set{ \rho_{H-i}(s,a), \min_{s' \in \mathcal{S}} \max \set{ 1 - \pi_{H-i} \paren{ s' \sachant s, a  } , \underline{U^*_{i-1}}(s') }  }.
\end{eqnarray}
\end{theorem}

In this theorem, the horizon $i$ is the opposite modulo $H$ 
of the stage of the process $t$: during execution,
$\delta_{t} = \delta_{H-i}$ is used at time step $t$,
\textit{i.e.} when it remains $i$ steps. 

Note that a broader class of MDP models, 
including both probabilistic and qualitative possibilistic MDPs presented above,
is called \textit{Algebraic MDPs} \cite{LIP67955}. 

The next section is devoted to the presentation of the qualitative possibilistic counterpart of the POMDPs denoted by $\pi$-POMDPs: 
the $\pi$-POMDP model is the partially observable version of the $\pi$-MDP one.
This model has been presented first in \cite{Sabbadin:1999:pipomdp} in pessimistic settings. 
The algorithm to solve it has been 
also presented in case no intermediate preference degree is involved \textit{i.e.}
in case where preference functions $\rho_t$ are not used:
in these settings only the terminal preference function $\Psi$
models the goal of the mission: an optimistic strategy maximizes 
the plausibility of strategies which end with a good preference,
and a cautious strategy minimizes the plausibility of strategies 
ending in unwanted states. 
As the preference of a system state trajectory $\mathcal{T} = (s_1,\ldots,s_H)$ 
is simply $\rho(\mathcal{T}) = \Psi(s_H)$, 
while the preference of a $\pi$-MDP trajectory is $\set{ \min_{t=0}^{H-1} \rho(s_t,a_t), \Psi(s_H) }$,
it it sufficient to consider a classical $\pi$-MDP such that $\rho_t(s,a) = 1$, $\forall s \in \mathcal{S}$, $\forall a \in \mathcal{A}$ and $\forall t \in \set{0,\ldots,H-1}$. 

\subsection{$\pi$-POMDPs}
\label{section_piPOMDP}
The qualitative possibilistic POMDP ($\pi$-POMDP) model has been first presented in \cite{Sabbadin:1999:pipomdp}.
As explained in Section \ref{section_POMDP} which presents the classical probabilistic POMDP model, 
in partially observable settings the system state is not given anymore as input to the agent:
the agent has to infer it using the observations $o \in \mathcal{O}$ 
received at each time step, represented by the observation process $(O_t)_{t \in \mathbb{N}}$.
The uncertainty about successive observation variables $O_t$
only depends on the current action and the reached state: 
if the agent selected action $a \in \mathcal{A}$ at time step $t$,
and the system has reached state $s' \in \mathcal{S}$ at time step $t+1$,
the observation $o' \in \mathcal{O}$ is received with possibility degree 
$\pi_t \paren{ o' \sachant s', a} = \Pi \paren{ O_{t+1} = o' \sachant S_{t+1}=s', a }$:
conditional to the next system state $s'$ and the current action $a$,
the next observation variable is M-independent (see Definition \ref{def_mbindep}) 
from all other variables. Figure \ref{POMDP} of Section \ref{section_POMDP}
illustrates just as well the dynamic and the structure of a $\pi$-POMDP:
however, the rewards $r$ and $R$ have to be replaced by preferences $\rho$ and $\Psi$, 
and transition (resp. observation) probability distributions $\textbf{p}$ 
have to be replaced by the possibility distribution $\pi_t \paren{s' \sachant s,a}$ 
(resp. $\pi_t \paren{o' \sachant s',a}$).

\begin{figure}[!t]
\input{src/fig-pomdp}
\caption[Diagramme d'Influence d'un $\pi$-POMDP et de son processus d'états de croyance]{
Diagramme d'Influence d'un $\pi$-POMDP et de son processus d'états de croyance:
les ronds noirs représentent les états successifs du système $S_t$,
les bleus représentent les observations successives $O_t$,
les carrés rouges sont les actions sélectionnées $a_t$,
et les losanges jaunes sont les préférences associées.
Les cercles verts en haut de la figure sont les états de croyance successifs
$B_t$ constituant le processus d'états de croyance,
calculé avec la mise à jour $B_{t+1} = \nu(B_t,a_t,O_{t+1})$.
Tout comme les lignes ondulée menant aux préférences,
les lignes vertes en pointillé représentent une influence déterministe.}
\label{POMDP} 
\end{figure}



As with the probabilistic model, the computation of strategies
is performed by translating of the $\pi$-POMDP
into a fully observable $\pi$-MDP.
The state space of the later is the set of 
possible \textit{qualitative possibilistic belief states} 
$\beta: \mathcal{S} \rightarrow \mathcal{L}$ describing the knowledge about the actual system state,
\textit{i.e.} the set of all the possibility distributions over $\mathcal{S}$.
This set is denoted by $\Pi^{\mathcal{S}}_{\mathcal{L}} = \set{ \pi: \mathcal{S} \rightarrow \mathcal{L} \sachant \max_{s \in \mathcal{S}} \pi(s) = 1 }$. 
Note first that the number of possible possibilistic beliefs about the actual system state is
\begin{equation}
\label{equation_numberOfPossDistrib}
\paren{\# \mathcal{L}}^{\# \mathcal{S}} - \paren{\# \mathcal{L} -1}^{\# \mathcal{S}}.
\end{equation}
Indeed, there are $\# \mathcal{L}^{\# \mathcal{S}}$ 
different functions from $\mathcal{S}$ to $\mathcal{L}$,
and $\paren{ \# \mathcal{L} - 1 }^{\# \mathcal{S}}$ non-normalized ones
\textit{i.e.} functions $f: \mathcal{S} \rightarrow \mathcal{L}$ 
such that $\max_{s \in \mathcal{S}} f(s) <1$.
The number of possibility distributions over $\mathcal{S}$
is the number of normalized functions from $\mathcal{S}$ to $\mathcal{L}$,
that is the total number of functions minus the number of non-normalized ones.

First of all, let us formally defined a $\pi$-POMDP as the 
$7$-uple $<\mathcal{S},\mathcal{A},\mathcal{O},T^{\pi},O^{\pi},\Psi,\beta_0>$:
\begin{itemize}
\item $\mathcal{S}$, a finite set of hidden system states;
\item $\mathcal{A}$ a finite set of actions;
\item $\mathcal{O}$ a finite set of observations;
\item $T^{\pi}$ the set of transition possibility distributions
containing for each time step $t \in \mathbb{N}$, each current system state $s \in \mathcal{S}$
and each current action $a \in \mathcal{A}$, the possibility distributions over the next system state $s' \in \mathcal{S}$, 
$\pi_t \paren{s' \sachant s, a}$;
\item $O^{\pi}$, the set of observation possibility distributions: 
for each time step $t \in \mathbb{N}$, each current action $a \in \mathcal{A}$, 
each next state $s'$, the possibility distribution over the next observation $o' \in \mathcal{O}$,
$\pi_t \paren{ o' \sachant s',a }$ is part of $O^{\pi}$.
\item $\Psi$ the preference function, defining for each state $s \in \mathcal{S}$, 
the preference assigned to the situations where the system terminates in state $s$.
\item $\beta_0$, the possibilistic \textit{initial belief state},
is the possibility distribution defining the uncertainty about the initial state: 
$\forall s \in \mathcal{S}$, $\beta_0(s) = \Pi(S_0 = s)$. 
\end{itemize}

At each time step the current qualitative possibilistic belief state 
is computed from these objects:
the possibilistic counterpart of the probabilistic belief defined in Definition \ref{def_belief} of Section \ref{section_POMDP}.
The initial belief state $\beta_0 \in \Pi^{\mathcal{S}}_{\mathcal{L}}$ is part of the definition of a $\pi$-POMDP.
At a time step $t \geqslant 1$, the belief state is the possibility distribution
over the current system state, conditional to all the data available to the agent. 
\begin{Def}[Qualitative Possibilistic Belief state]
\label{def_QualPossBel}
\begin{equation}
\beta_{t}(s) = \Pi \paren{ S_t = s \sachant O_1=o_1, \ldots, O_t=o_t, a_0, \ldots, a_{t-1} } = \Pi \paren{ S_t = s \sachant I_t = i_t }
\end{equation}
\end{Def}
where $i_t = \set{o_1,\ldots,o_t,a_0,\ldots,a_{t-1}}$ 
is the information available to the agent at time $t$ ( $i_0 = \set{} = \emptyset$ ),
and $I_t$ the variable version (as in the probabilistic POMDP presentation).

The possibilistic belief updating process consists in the sequence of belief states,
which can be computed recursively: 
\begin{theorem}[Qualitative Possibilistic Belief Update]
\label{belief_process_recursif_poss}
If the belief state at time step $t$ is $b_t$,
the selected action is $a_t \in \mathcal{A}$,
and the next observation is $o_{t+1}$, 
the next belief state $b_{t+1}$ is computed as follows:
\begin{equation}
\label{possbeliefupdate}
\beta_{t+1}(s') = \left \{ \begin{array}{ccc}
1 & \mbox{ if } \pi_t \paren{ s', o_{t+1} \sachant \beta_t, a_t}= \pi_t \paren{ o_{t+1} \sachant \beta_t,a_t }, \\
\pi_t \paren{ s', o_{t+1} \sachant \beta_t, a_t} & \mbox{ otherwise. } 
\end{array} \right.
\end{equation}
where the joint distribution over system state variable $S_{t+1}$
and observation variable $O_{t+1}$ conditional on the current information, 
is denoted by $\pi_t \paren{ s', o' \sachant \beta_t, a_t} 
= \min \bigg\{ \pi_{t} \paren{o' \sachant s', a_t},  \max_{s \in \mathcal{S}} \min \Big\{ \pi_t \paren{ s' \sachant s, a_t }, \beta_t(s) \Big\} \bigg\}$. 
The notation $\pi \paren{ o' \sachant \beta_t, a_t}$ is also used for $\max_{s' \in \mathcal{S}} \pi_t \paren{ s', o' \sachant \beta_t, a_t}$. 

This formula is called the \textbf{possibilistic belief update},
and since the belief state $\beta_{t+1}$ is shown to be a function of $\beta_t$, $a_t$ and $o_{t+1}$,
we denote it by \[ \beta_{t+1} = \nu(\beta_t,a_t,o_{t+1}),\]
with $\nu$ called the belief update function.
\end{theorem}
The proof is given in Annex \ref{belief_process_recursif_poss_RETURN}.

The possibilistic belief update (\ref{possbeliefupdate}) is denoted by
\[ \beta_{t+1}(s') \propto^{\pi} \pi_t \paren{ s', o_{t+1} \sachant \beta_t, a_t} \]
as it only consists in normalizing the function 
$s' \mapsto \pi \paren{ s',o_{t+1} \sachant \beta_t,a_t }$
in a possibilistic sense ($\max_{s} \pi(s) = 1$).

We denote by $B^{\pi}_t$ the belief state when considered as a variable,
\textit{i.e.} $B^{\pi}_0$ is deterministic equal to $\beta_0$
(but $S_0$ is uncertain with possibility distribution $\beta_0$)
and $B^{\pi}_{t+1} = \nu(B^{\pi}_t,a_t,O_{t+1})$
where $O_{t+1}$ is the observation variable at time step $t+1$.

To make things clear,
the $\pi$-POMDP model is defined here 
only with a terminal preference function $\Psi$,
and no intermediate ones $\rho_t$.
The next chapter will address formally the model with intermediate preference degree:
the pessimistic one presented in \cite{Sabbadin:1999:pipomdp} and 
an optimistic one. 
The criteria, or optimistic and pessimistic value functions of the $\pi$-POMDP model with terminal preference only,
are thus similar to criteria (\ref{MDPtermprefcritopt}) and (\ref{MDPtermprefcritpess}).
Note that the optimistic criterion has not been presented yet to the best of our knowledge,
and is proposed now in parallel with the pessimistic one \cite{Sabbadin:1999:pipomdp}.
\begin{Def}[$\pi$-POMDP Criteria with Terminal Preference Only]
\label{def_piPOMDPvaluefunction}
This is the same criteria as in the fully observable case (terminal preference case, Definition \ref{def_piMDPcriteria_TPO}):
however, these criteria depends here on the initial belief state. 

The optimistic $\pi$-POMDP criterion, or \textbf{optimistic value function}, 
is the Sugeno integral of the terminal preference 
with respect to the possibility measure of the system process
for a given strategy $(\delta_t)_{t=0}^{H-1}$:
the strategy which is looked for is a sequence of function of the available information $i_t$,
\textit{i.e.} $(\delta) = (\delta_t)_{t=0}^{H-1}$ with $\delta_t: i_t \mapsto \delta(i_t) \in \mathcal{A}$.
\begin{equation}
\label{equation_piPOMDPcriterionopt}
\overline{U_H}\Big(\beta_0,(\delta)_{t=0}^{H-1}\Big) = \max_{ s_H \in \mathcal{S}} \min \set{ \Psi(s_H), \pi \Big( s_H \Big\vert \beta_0, (\delta) \Big) }.
\end{equation}

As well, the $\pi$-POMDP \textbf{pessimistic value function} is the Sugeno integral of the terminal preference with respect to the necessity measure of the system process
given such a strategy:
\begin{equation}
\label{equation_piPOMDPcriterionpess}
\underline{U_H}\Big(\beta_0,(\delta)_{t=0}^{H-1}\Big) = \min_{ s_H \in \mathcal{S}} \max \set{ \Psi(s_H), 1 - \pi \Big( s_H \Big\vert \beta_0, (\delta) \Big) }.
\end{equation}

where
\begin{align*} 
\pi \Big( s_H \Big\vert \beta_0, (\delta) \Big) &= \Pi \Big( S_H = s_H \Big\vert (\delta) \Big) \\
&= \max_{(s_0,\ldots,s_{H-1}) \in \mathcal{S}^H} \min \set{ \min_{t=0}^{H-1} \pi_t \Big( s_{t+1} \Big\vert s_t, \delta(i_t) \Big), \beta_0(s_0) } 
\end{align*}
is the possibility distribution over the last system state given the strategy.
Thus the optimistic criterion may be denoted by $\mathbb{S}_{\Pi} \Big[ \Psi(S_H) \Big\vert \beta_0, (\delta) \Big] $ and the pessimistic one $\mathbb{S}_{\mathcal{N}} \Big[ \Psi(S_H) \Big\vert \beta_0, (\delta) \Big]$, as they are optimistic and pessimistic Sugeno integrals based on the distribution $\pi \Big( s_H \Big\vert \beta_0, (\delta) \Big)$.
\end{Def}

%Consider now the random variables representing successive actions 
%$(A_t)_{t \in \mathbb{N}}$:
%it includes the particular case $A_t = \delta(I_t)$ 
%where $I_t=\set{ A_0,O_1, \ldots A_{t-1}, O_t }$.
%The qualitative possibilistic value functions introduced in Definition \ref{equation_piPOMDPcriterionopt} 
%can be rewritten as follows:
As in the probabilistic framework,
Section \ref{section_abeldepvalfunc},
these criteria can be rewritten 
based on a belief-dependent preference.
Consider as previously, a strategy $(\delta_t)_{t=0}^{H-1}$ 
based on the current information $i_0 = \emptyset$, $i_1 = \set{ a_0,o_1 }$, $i_2 = \set{ a_0,o_1,a_1,o_2 }$, \textit{etc}:
for each time step $t \geqslant 0$, $\delta_t: i_t \mapsto \delta_t(i_t) \in \mathcal{A}$.
Recall that $\nu$ is the belief update function defined in Theorem \ref{belief_process_recursif_poss}.
We denote by $\widehat{O}_t = \Big(O_i\Big)_{i=1}^{t}$ the successive observations until time step $t$ seen as variables, 
and $I_t$ the associated information. 
The belief state at time step $t+1$, seen as a variable, 
can be written $B_{t+1}^{\pi} = \nu^{\delta_{t},\widehat{O}_{t+1}}(B_{t}^{\pi})$, $\forall t\geqslant0$,
with $(\delta_t)_{t=0}^{H-1}$ such a strategy, and the notation $\nu^{\delta_{t},\widehat{O}_{t+1}}: \beta \mapsto \nu\Big( \beta, \delta_{t}(I_{t}), O_{t+1} \Big)$. Thus,
\[ B^{\pi}_{H} = \paren{ \Circle_{t=0}^{H-1} \nu^{\delta_t,\widehat{O}_{t+1}} }(B^{\pi}_0), \]
where $\Circle$ is the function composition operator.
Then, knowing that $\widehat{O}_H = \widehat{o}_H$ 
with $\widehat{o}_H = \set{o_1,\ldots,o_H} \in \mathcal{O}^H$ a sequence of observations, 
$B^{\pi}_{H}$ is known: it is denoted by $\beta^{\delta,\widehat{o}_H}_{\beta_0}$,
and is called the belief state generated by the observation sequence $\widehat{o}_H$ and the strategy $(\delta_t)_{t=0}^{H-1}$. 
\begin{theorem}[$\pi$-POMDP Criteria Rewritings -- Terminal Preference Case]
\label{piPOMDPrewriting}
%Given , let ut consider $B_{H}^{\pi} =  \nu \bigg( \nu\Big( \ldots \nu \paren{ \beta_0,\delta_0(\beta_0),O_1}, \ldots \Big), \delta_t(I_t), O_H \bigg)$ \textit{i.e.}
%the $H^{th}$ possibilistic belief state seen as a variable.
Let $\widehat{o}_H = \set{o_1,\ldots,o_H}$ a sequence of observations,
and $(\delta) = (\delta)_{t=0}^{H-1}$ be a strategy 
such that $\delta_{t+1}$ is a function of the information $i_{t+1}=\set{ \delta_t(i_t), o_{t+1} }$.
The possibility distribution over the possible sequences of observations is denoted by\\
\\ 
$\pi \Big( \widehat{o}_H \Big\vert (\delta), \beta_0 \Big) = \Pi \paren{ \widehat{O}_{H} = \widehat{o}_H \sachant (\delta), \beta_0 }$
 \[ = \max_{(s_0,\ldots,s_H) \in \mathcal{S}^{H+1}} \min \bigg\{ \pi_t \Big( o_{t+1} \Big\vert s_{t+1}, \delta_t(i_t) \Big), \pi_t \Big( s_{t+1} \Big\vert s_t, \delta_t(i_t) \Big), \beta_0(s_0) \bigg\}. \]

The optimistic $\pi$-POMDP criterion is equal to the Sugeno integral 
of the belief-based optimistic preference $\overline{\Psi}(B_H^{\pi}) = \max_{s \in \mathcal{S}} \min \set{ \Psi(s), B^{\pi}_H(s) }$ 
with respect to the possibility measure over the observation sequences.
That is, denoting by $\beta^{\delta,\widehat{o}_H}_{\beta_0}$
the belief state generated by the observation sequence $\widehat{o}_H$ and the strategy $(\delta_t)_{t=0}^{H-1}$,
the \textbf{optimistic criterion can be rewritten as}
\begin{align}
\label{piPOMDPoptrewriting}
\overline{U_H}\Big(\beta_0,(\delta)_{t=0}^{H-1}\Big) &= \max_{\widehat{o}_H} \min \set{ \overline{\Psi}(\beta^{\delta,\widehat{o}_H}_{\beta_0}),  \pi \Big( \widehat{o}_H \Big\vert (\delta), \beta_0 \Big) } \\
\nonumber &= \max_{\widehat{o}_H} \min \set{  \max_{s \in \mathcal{S}} \min \set{ \Psi(s),\beta^{\delta,\widehat{o}_H}_{\beta_0}(s) },  \pi \Big( \widehat{o}_H \Big\vert (\delta), \beta_0 \Big) }.
\end{align}
%$\mathbb{S}_{\Pi} \Big[ \Psi(S_H) \Big\vert B^{\pi}_0 = \beta_0, (\delta) \Big] = \mathbb{S}_{\Pi_{\delta},S_0 \sim \beta_0} \Big[ \mathbb{S}_{\Pi_{\delta},S \sim B^{\pi}_H} \croch{ \Psi(S)  } \Big] = \mathbb{S}_{\Pi} \Big[ \max_{s \in \mathcal{S}} \min \set{\Psi(s), B^{\pi}_H(s) } \Big], $

Likewise, the pessimistic criterion is equal to the Sugeno integral of the belief-based pessimistic preference
$\underline{\Psi}(B^{\pi}_H) = \min_{s \in \mathcal{S}} \max \set{ \Psi(s), 1 - B^{\pi}_H(s) }$,
with respect to the necessity measure over the observation sequences.
The $\pi$-POMDP \textbf{pessimistic criterion can be rewritten as}
\begin{align}
\label{piPOMDPpessrewriting}
\underline{U_H}\Big(\beta_0,(\delta)_{t=0}^{H-1}\Big) &= \min_{\widehat{o}_H} \max \set{ \underline{\Psi}(\beta^{\delta,\widehat{o}_H}_{\beta_0}), 1-\pi \Big( \widehat{o}_H \Big\vert (\delta), \beta_0 \Big) } \\
\nonumber &= \min_{\widehat{o}_H} \max \set{  \min_{s \in \mathcal{S}} \max \set{ \Psi(s), 1 - \beta^{\delta,\widehat{o}_H}_{\beta_0}(s) }, 1-\pi \Big( \widehat{o}_H \Big\vert (\delta), \beta_0 \Big) }.
\end{align}
\end{theorem}
The proof is given in Annex \ref{piPOMDPrewriting_RETURN}.

Written as a Sugeno integral, the optimistic criterion $\mathbb{S}_{\Pi} \Big[ \Psi(S_H) \Big\vert \beta_0, (\delta) \Big]$,
which is based, as in Definition \ref{def_piPOMDPvaluefunction}, on $\pi \Big( s_H \Big\vert \beta_0, (\delta) \Big)$ (the possibility distribution over the last system state $s_H$), 
becomes in the previous theorem
\[  \mathbb{S}_{\Pi} \Big[ \max_{s \in \mathcal{S}} \min \set{\Psi(s), B^{\pi}_H(s)  } \Big\vert \beta_0, (\delta) \Big] = \mathbb{S}_{\Pi} \Big[ \overline{\Psi}(B^{\pi}_H) \Big\vert \beta_0, (\delta) \Big]. \]
These two equal Sugeno integrals are based on the possibility distribution 
over the observation sequence $\pi \Big( \widehat{o}_H \Big\vert (\delta), \beta_0 \Big)$.
As well, in Theorem \ref{piPOMDPrewriting}, 
the pessimistic criterion $\mathbb{S}_{\mathcal{N}} \Big[ \Psi(S_H) \Big\vert \beta_0, (\delta) \Big]$
is rewritten 
\[  \mathbb{S}_{\mathcal{N}} \Big[ \min_{s \in \mathcal{S}} \max \set{\Psi(s), 1 - B^{\pi}_H(s)  } \Big\vert \beta_0, (\delta) \Big] 
= \mathbb{S}_{\mathcal{N}} \Big[ \underline{\Psi}(B^{\pi}_H) \Big\vert \beta_0, (\delta) \Big]. \]
Note that the belief-based preferences $\overline{\Psi}$ and $\underline{\Psi}$,
are the $\pi$-POMDP counterparts of the POMDP belief-based reward $r(b,a) = \sum_{s \in \mathcal{S}} r(s,a) \cdot b_t(s)$.
As well, these rewritings are the possibilistic counterpart of 
the rewriting $\mathbb{E} \croch{ r\Big(S_t, d_t(i_t) \Big) } = \mathbb{E} \croch{ \sum_{s \in \mathcal{S}} B_t(s) \cdot r\Big(s,d_t(i_t)\Big) }$.

This theorem assures us that the criteria
can be expressed as Sugeno integrals 
of a function of the possibilistic belief state $B^{\pi}_H$: 
this result leads to the definition of $\pi$-MDPs whose states are the qualitative possibilistic belief states:
these $\pi$-MDPs are denoted by $\langle \tilde{S}^{\pi}, \mathcal{A}, \tilde{T^{\pi}}, \tilde{\Psi} \rangle$.
The state space $\tilde{ \mathcal{S}}^{\pi} $ 
is the finite set of possibilistic belief states
$\Pi^{\mathcal{S}}_{\mathcal{L}} = \set{ \beta \sachant \beta: \mathcal{S} \rightarrow \mathcal{L}, \max_{s \in \mathcal{S}} \beta(s) = 1 }$.

Let $\beta_t$ a given qualitative possibilistic belief, 
\textit{i.e.} a possibility distribution in $\Pi^{\mathcal{S}}_{\mathcal{L}}$.
The sequence of variables $(B_t^{\pi})_{t \in \mathbb{N}}$ is the sequence of the belief functions seen as random variables.
As highlighted by the possibilistic belief update (\ref{possbeliefupdate}),
if $B^{\pi}_t=\beta_t$, and the selected action is $a_t$,
the value of the next variable $B^{\pi}_{t+1}$ 
is a deterministic function of the observation $O_{t+1}$. 

A belief $\pi$-MDP is defined since 
the qualitative possibilistic belief updating process is shown to be a possibilistic Markov process \textit{i.e.}
$\forall a \in \mathcal{A}, \forall \beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}$,
$B^{\pi}_{t+1}$ is M-independent from all previous variables conditional to the current belief $B^{\pi}_{t}$ and the selected action $a_t \in \mathcal{A}$:
\begin{theorem}
\label{theorem_qualpossMarkov}
The qualitative possibilistic belief updating process is a Markov process, \textit{i.e.}
\begin{equation}
\label{possibilistic_belief_markov}
\Pi \paren{ B^{\pi}_{t+1} = \beta' \sachant I_t=i_t, a_t } = \Pi \paren{ B^{\pi}_{t+1} = \beta' \sachant B^{\pi}_t=\beta_{b_0}^{i_t}, a_t },
\end{equation}
where $\beta_{b_0}^{i_t}$ is the qualitative belief state reached starting with $\beta_0$ and with the information $i_t = \set{ a_0,o_1,a_1,o_2, \ldots, a_{t-1},o_t }$.
\end{theorem}
The proof is given in Annex \ref{theorem_qualpossMarkov_RETURN}.

As highlighted by the equation (\ref{possibilistic_belief_trans}) in the proof, if $B^{\pi}_t=\beta$
and the selected action is $a \in \mathcal{A}$, 
the possibility degree that the next belief $B^{\pi}_{t+1}$ is $\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}$, 
is the maximum of all the possibility degrees of observations $o'$
such that $\nu \paren{\beta,a,o'}=\beta'$: 
it defines the transition probability distributions of the belief process,
\textit{i.e.} elements of $\tilde{T}$, as follows: $\forall t \geqslant 0$,
\begin{align}
\label{transition_possibilistic_belief} \pi_t \paren{ \beta' \sachant \beta,a} &= \max_{ \substack{ o' \in \mathcal{O} \mbox{ \tiny s.t. } \\ \nu(\beta,a,o') = \beta'} } \pi_t \paren{ o' \sachant \beta, a },
\end{align}
where $\pi_t \paren{ o' \sachant \beta, a } = \max_{(s,s') \in \mathcal{S}^2} \min \Big\{ \pi_t \paren{ o' \sachant s', a_t }, \pi_t \paren{ s' \sachant s, a_t }, \beta(s) \Big\}$,
is the possibility degree of observing $o'$ conditional on all the previous information.

Finally, the preference functions associated with the possibilistic belief $\beta_H$ are defined as
highlighted by Theorem \ref{piPOMDPoptrewriting}: for an optimistic $\pi$-POMDP, the preference function is, $\forall \beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$,
\begin{equation}
\label{opt_preference}
 \overline{\Psi}(\beta) = \max_{s\in\mathcal{S}} \min \set{ \Psi(s), \beta(s) } 
\end{equation}
and for a pessimistic one, $\forall \beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$,
\begin{equation}
\label{pess_preference}
 \underline{\Psi}(\beta) = \min_{s\in \mathcal{S}} \max \set{ \Psi(s), 1 - \beta(s) }.
\end{equation}
As for each belief $\beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$, 
the possibility and necessity measure conditional on the information $i$ 
don't vary if $i \in \set{ i \sachant \beta_{\beta_0}^{i} = \beta }$,
\textit{i.e.} if the information $i$ leads to $\beta$ (see for instance the proof of Theorem \ref{theorem_qualpossMarkov}),
it is sufficient to look for a belief-based strategy $(\delta_t)_{t=0}^{H-1}$,
such that $\forall t \geqslant 0$, $\delta_t: \beta_t \mapsto \delta_t(\beta_t) \in \mathcal{A}$.

The $\pi$-MDP $\langle \tilde{\mathcal{S}}^{\pi}, \mathcal{A}, \tilde{T^{\pi}}, \overline{\Psi} \mbox{ or } \underline{\Psi} \rangle$
built from of a $\pi$-POMDP $\langle \mathcal{S}, \mathcal{A}, \mathcal{O}, T^{\pi}, O^{\pi}, \beta_0 \rangle$ is finally: 
\begin{itemize}
\item $\tilde{\mathcal{S}}^{\pi} = \Pi_{\mathcal{L}}^{\mathcal{S}}$, the set of all qualitative possibilistic beliefs;
\item $\tilde{T^{\pi}}$ contains all transition possibility distributions
of the possibilistic beliefs: $\forall a \in \mathcal{A}$, $\forall \beta \in  \Pi_{\mathcal{L}}^{\mathcal{S}}$, 
the belief transition possibility distribution defined by the equation (\ref{transition_possibilistic_belief}),
$\pi_t \paren{ . \sachant \beta, a }$ is in $\tilde{T^{\pi}}$; 
\item preference functions $\overline{\Psi}$ 
if the computed criterion is optimistic, see equation (\ref{opt_preference}),
or $\underline{\Psi}$ if it is pessimistic, equation (\ref{pess_preference}).
\end{itemize}

Note now that, using the belief state transition definition (\ref{transition_possibilistic_belief}), and the equation (\ref{equationmaxmin3}) of Property \ref{property_minmax},
for each function from the belief space to $\mathcal{L}$, $U: \Pi^{\mathcal{S}}_{\mathcal{L}} \rightarrow \mathcal{L}$,
\begin{align*}
\max_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \min \set{ \pi_t \paren{ \beta' \sachant \beta, a}, U(\beta') } &= \max_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \min \set{ \max_{ \substack{ o' \in \mathcal{O} \mbox{ \tiny s.t. } \\ \nu(\beta,a,o') = \beta'} } \pi_t \paren{ o' \sachant \beta, a } , U(\beta') }\\
&=\max_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \max_{ \substack{ o' \in \mathcal{O} \mbox{ \tiny s.t. } \\ \nu(\beta,a,o') = \beta'} } \min \set{  \pi_t \paren{ o' \sachant \beta, a } , U(\beta') }\\
&= \max_{o' \in \mathcal{O}} \min \set{ \pi_t \paren{ o' \sachant \beta, a}, U\Big(\nu(\beta,a,o') \Big) }, 
\end{align*}
This observation leads to Algorithm \ref{PIPOMDP_algo_opt} which is the $\pi$-MDP algorithm (\ref{dynamic_programming_pimdp})
with terminal preference criteria (\ref{MDPtermprefcritopt}), 
applied to the $\pi$-MDP $\langle \tilde{\mathcal{S}}^{\pi}, \mathcal{A}, \tilde{T^{\pi}}, \overline{\Psi} \rangle$.

\begin{algorithm} \caption{Dynamic Programming Algorithm for Optimistic $\pi$-POMDP\hspace{20cm} with Terminal Preference Only} \label{PIPOMDP_algo_opt}
$\overline{U^*_0} \gets \overline{\Psi}$;\\
\For{$i \in \set{1,\ldots,H}$}{
	\For{$\beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$}{
		$\displaystyle \overline{U^*_i}(\beta) \gets \max_{a \in \mathcal{A}} \max_{o' \in \mathcal{O}} \min \set{ \pi_t \paren{ o' \sachant \beta,a } , \overline{U^*_{i-1}}\Big(\nu(\beta,a,o')\Big) }$;\\
		$\displaystyle \overline{\delta_{H-i}}(\beta) \in \operatorname*{argmax}_{a \in \mathcal{A}} \max_{o' \in \mathcal{O}} \min \set{ \pi_t \paren{ o' \sachant \beta,a }, \overline{U^*_{i-1}} \paren{ \nu(\beta,a,o') } }$;\\
	}
}
\Return $\overline{U^*_H}$, $(\overline{\delta^*})$;
\end{algorithm}

As well, the equations (\ref{equationmaxmin1}) and (\ref{equationmaxmin2}) of Property \ref{property_minmax}
leads to
\[  \displaystyle \min_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \max \set{ 1 - \pi_t \paren{ \beta' \sachant \beta, a}, U(\beta') } = 
1 - \max_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \min \set{ \pi_t \paren{ \beta' \sachant \beta, a}, 1- U(\beta') }, \]
for each function $U: \Pi^{\mathcal{S}}_{\mathcal{L}} \rightarrow \mathcal{L}$.
Thus, using the belief state transition definition (\ref{transition_possibilistic_belief}), 
\[ \min_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \max \set{ 1 - \pi_t \paren{ \beta' \sachant \beta, a}, U(\beta') } = \min_{o' \in \mathcal{O}} \max \set{ 1 - \pi_t \paren{ o' \sachant \beta, a}, U\Big( \nu(\beta,a,o')\Big) } . \]
It leads to Algorithm \ref{PIPOMDP_algo_pess},
which is the $\pi$-MDP algorithm (\ref{dynamic_programming_pimdppess}),
with terminal preference criteria (\ref{MDPtermprefcritpess}),
applied to the $\pi$-MDP $\langle \tilde{\mathcal{S}}^{\pi}, \mathcal{A}, \tilde{T^{\pi}}, \underline{\Psi} \rangle$.

\begin{algorithm} \caption{Dynamic Programming Algorithm for Pessimistic $\pi$-POMDP\hspace{20cm} with Terminal Preference Only} \label{PIPOMDP_algo_pess}
$\underline{U^*_0} \gets \underline{\Psi}$;\\
\For{$i \in \set{1,\ldots,H}$}{
	\For{$\beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$}{
		$\displaystyle \underline{U^*_i}(\beta) \gets \max_{a \in \mathcal{A}} \min_{o' \in \mathcal{O}} \max \set{ 1 - \pi_t \paren{ o' \sachant \beta,a } , \underline{U^*_{i-1}}\Big(\nu(\beta,a,o')\Big) }$;\\
		$\displaystyle \underline{\delta_{H-i}}(\beta) \in \operatorname*{argmax}_{a \in \mathcal{A}} \min_{o' \in \mathcal{O}} \max \set{ 1 - \pi_t \paren{ o' \sachant \beta,a }, \underline{U^*_{i-1}} \paren{ \nu(\beta,a,o') } }$;\\
	}
}
\Return $\underline{U^*_H}$, $(\underline{\delta^*})$;
\end{algorithm}

In this first chapter, probabilistic and qualitative possibilistic POMDPs have been built one after the other
shedding some light on the similarities between both models:
with probabilistic POMDPs, system state dynamics and observation uncertainty 
are described with probabilities $\textbf{p} \paren{ s' \sachant s,a } \in \mathbb{R}$
and $\textbf{p} \paren{ o' \sachant s',a } \in \mathbb{R}$
while they are defined by possibility distributions $\pi \paren{ s' \sachant s,a } \in \mathcal{L} = \set{ 0, \frac{1}{k}, \ldots, 1}$ (with $k\geqslant1$) 
and $\pi \paren{ o' \sachant s',a } \in \mathcal{L}$ 
in the $\pi$-POMDP framework.
Moreover, the probabilistic framework measures the benefit from passing through a system state $s \in \mathcal{S}$ and using action $a \in \mathcal{A}$
with the additive reward functions $r(s,a) \in \mathbb{R}$ (and $R(s)$ for the last state in case of finite-horizon problem);
the possibilistic framework uses qualitative preferences $\rho(s,a) \in \mathcal{L}$ and $\Psi(s) \in \mathcal{L}$.
Thus, the probabilistic criterion (the value function) for a given strategy 
is the expectation of the rewards written $\mathbb{E} \croch{ rewards\Big( (S_t)_{t\geqslant0} \Big) } \in \mathbb{R}$, 
and the possibilistic framework has two criteria (value functions) which are Sugeno integrals of preferences written 
$\mathbb{S}_{\Pi} \croch{ preferences\Big( (S_t)_{t\geqslant0} \Big) } \in \mathcal{L}$ for the optimistic one,
and $\mathbb{S}_{\mathcal{N}} \croch{ preferences\Big( (S_t)_{t\geqslant0} \Big) } \in \mathcal{L}$ for the pessimistic one.
A POMDP (resp. $\pi$-POMDP), is redefined in terms of fully observable MDP (resp. $\pi$-MDP)
where the system states are the belief states $b_t \in \mathbb{P}^{\mathcal{S}}_{b_0}$ (resp. $\beta_t \in \Pi^{\mathcal{S}}_{\mathcal{L}}$), 
\textit{i.e.} probability (resp. possibility) distributions over the system states of the initial POMDP:
the belief-based reward has to be defined $r(b,a) = \mathbb{E}_{S \sim b} \croch{ r(S,a) } = \sum_{s \in \mathcal{S}} r(s,a) \cdot b(s)$
in the probabilistic case. In the possibilistic case, the belief-based preference 
can be written $\overline{\rho}(b,a) = \mathbb{S}_{\Pi, S \sim \beta} \croch{ \rho(S,a) } = \max_{s \in \mathcal{S}} \min \set{ \rho(s,a), \beta(s) }$
for the optimistic criterion,
and $\underline{\rho}(b,a) = \mathbb{S}_{\mathcal{N}, S \sim \beta} \croch{ \rho(S,a) } = \min_{s \in \mathcal{S}} \max \set{ \rho(s,a), 1 - \beta(s) }$
for the pessimistic one.

The next chapter proposes some improvements of the qualitative possibilistic model: 
first, criteria are discussed, concerning the preference aggregation, 
and the impact of the choice of the (optimistic or pessimistic) criterion.
Next, the Mixed-Observability property is defined:
as for the probabilistic model,
the complexity of solving $\pi$-POMDPs having this property is reduced.
Finally the infinite horizon problem is formally defined
and the proposed solving algorithm is shown to return an
optimal strategy for a given criterion.
