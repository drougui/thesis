While the previous chapters dealt with purely qualitative possibilistic models,
this one proposes to use the strength of both probabilistic and possibilistic approaches,
in order to solve fully defined factored POMDPs, or partially defined ones.
This idea comes from the analysis of the results of the experiments of Chapter \ref{chap_symb}:
qualitative modeling may lead to poor strategies 
for risky problems, or when the frequentist information defining the POMDP 
is at the heart of planning the problem. 
Here, a new translation from Partially Observable MDP into Fully 
Observable MDP is described. 
Unlike the classical
translation (see Section \ref{section_POMDPisbeliefMDP}), 
the resulting problem state space is finite, 
making MDP solvers able to solve this simplified version 
of the initial partially observable problem: this approach encodes 
agent beliefs with fuzzy measures over states, 
leading to an MDP whose system state space is a finite set of 
epistemic states. %After a short description of the POMDP
%problem as well as notions of Possibility Theory,
The translation is described in a formal manner with
semantic arguments. Then actual computations of this transformation
are detailed, in order to highly benefits from the factorized 
structure of the initial POMDP in the the final MDP problem 
size reduction and structure.
Finally size reduction and tractability of the resulting MDP is 
illustrated on a simple POMDP problem.


\section{Introduction}
The approach proposed here simplifies the belief space of a POMDP problem before solving it.
The transformation described leads to a fully observable MDP on a finite
number of epistemic states, \textit{i.e.} a problem 
modeling an agent acting under uncertainty in a fully
observable environment \cite{puterman94}. As such a finite state space 
MDP problem is P-complete \cite{Papadimitriou:1987} 
this transformation qualifies as a simplification, and any MDP solver 
can return a policy for this translated POMDP.

More than only a simplification of the initial POMDP problem,
the theoretical framework used here for belief states representation
formally models an agent's knowledge about the system state.
Indeed the proposed translation defines the belief states as possibility distributions
over system states $s \in \mathcal{S}$
which represents the fuzzy set of possible system states, 
as done with $\pi$-POMDP models.

The major originality of this work comes from the finiteness of the scale $\mathcal{L}$:
indeed it follows that the number of possible belief states over the system state 
is, as well, finite (smaller than $\# ( \mathcal{L}^{\mathcal{S}} ) = 
(\# \mathcal{L})^{\# \mathcal{S}}$, see Equation \ref{equation_numberOfPossDistrib}). 
What very clearly distinguishes this approach
from the classical one is that the classical translation leads to an infinite set of 
belief states (the continuous set of all probability distributions over $\mathcal{S}$, 
or the sequence of reachable belief states from an initial one, see Section \ref{section_POMDPisbeliefMDP}).
The translation described here leads to an MDP whose system state space
is the set of possible possibilistic belief states, or \textit{epistemic states},
that is why its state space is finite. 

In addition to POMDP simplification and knowledge modelling, this qualitative
possibilistic framework offers some interesting properties: the possibilistic counterpart 
of the Bayes rule leads to a special belief state behaviour. Indeed the 
agent can possibly change their mind radically and rapidly, 
as described in Section \ref{EXPE_CHAP1}, 
experimental section of Chapter \ref{chap_Updates}.% \cite{Drougard13}. 
Moreover, under some conditions, the increased specificity of the belief state distribution is enforced, \textit{i.e.} 
the knowledge about the current state is non decreasing with time steps (see Section \ref{EXPE_CHAP1}).
Finally, in order to fully define the resulting MDP, the translation has to attach 
a reward function to its states: as a possibilistic belief state distributions 
constitute the new (epistemic) states of the problem, the definition of the rewards
uses the Choquet integral adapted to fuzzy measures. 
This integral is used with the dual measure of the possibility measure defined by the belief state. 
The dual measure of a possibility measure, called the \textit{necessity measure} (see Definition \ref{DEF_necessity}), 
and the use of this integral makes the rewards values pessimistic about the potential lack of knowledge
described by the associated belief state.

However the number of possibilistic belief distributions, or \textit{fuzzy epistemic states},
grows exponentially with the number of initial POMDP system states. 
The so called simplification of the problem does not transform the PSPACE POMDP 
problem into a polynomial one: as the new state space size is exponential in the 
previous one, the resulting problem is EXPTIME. The proposed translation 
tries to generate as few epistemic states as possible taking carefully into account
potential factorized structures of the initial POMDP.

This chapter begins with the description of 
the first contribution of this work, which is the translation itself, 
presented in a formal way.
As the resulting state space of the built MDP is too big to make this problem
tractable without factorization tricks in practice,
the next section details the proper way to preprocess its attributes.
Finally, the last section illustrates the power of this approach,
describing the translation in practice, 
and applying it on a simple factored POMDP problem.

\section{Un PDMPO hybride}
As claimed by Zadeh, ``most information/intelligent
systems will be of hybrid type'' \cite{DBLP:journals/soco/Zadeh98}:
the idea developped here is to use a granulated representation of the agent knowledge 
using possibilistic belief states instead of probabilistic belief states
in the POMDP framework.
The first advantage of this granulation
is that strategy computation
is performed reasoning on
a finite set of possibilistic belief states 
called then epistemic states:
the set of all possibility distributions 
defined over $\mathcal{S}$, denoted by $\Pi^{\mathcal{S}}_{\mathcal{L}}$ is
$\# \Pi^{\mathcal{S}}_{\mathcal{L}} = \# \mathcal{L}^{\# \mathcal{S}} - (\# \mathcal{L}-1)^{\# \mathcal{S}}$,
due to the possibilistic normalization (see Equation \ref{equation_numberOfPossDistrib}),
while the set of probability distributions over $\mathcal{S}$ is infinite.
%Possibilistic counterparts of POMDP, named $\pi$-(PO)MDPs have been already defined 
%\cite{Sabbadin_1999_pipomdp, LIP61498} and efficiently used for planning under uncertainty problems \cite{DBLP:conf/aaai/DrougardTFD14}. 
The $\pi$-MDPs studied in the first chapters of this thesis 
are quite different from the model exposed in this chapter. 
For instance, Qualitative Possibilistic MDPs 
do not use quantitative data as probabilities or rewards. 
Dynamics is described in a purely qualitative possibilistic way. 
Frequentist information about the problem cannot be encoded: 
these frameworks are indeed dedicated to situations 
when the probabilistic dynamic of the studied system is lacking.
Moreover, possible values of the reward function are chosen 
among the degrees of the qualitative possibilistic scale.
A commensurability assumption between reward and possibility 
degrees, i.e. a meaning of why they share the same scale, 
is needed to use the criteria proposed in these frameworks.
Our model bypasses these demands: a real number is assigned 
to each possibilistic belief (epistemic state), 
instead of a qualitative utility degree: 
it represents the reward got by the agent 
when reaching this belief (in a MDP fashion)
as detailed in Section \ref{aggreg}. 
Moreover, the dynamics of our process 
is described with probability distributions: 
approximate probabilistic transition functions 
between current and next beliefs, or epistemic states, 
are given section \ref{setTrans}.
Finally, our model can be solved by any MDP solver in practice: 
it becomes eventually a classical probabilistic fully observable MDP
whose state space is the finite set $\Pi^{\mathcal{S}}_{\mathcal{L}}$. 

Here, the term hybrid is used because the beliefs only
are defined as possibility distributions, and all variables keep a probabilistic dynamic:
the agent reasons based on a possibilistic analysis of the system state (the possibilistic belief,
or epistemic state), and transition probability distributions are defined for its epistemic states. 
Such beliefs are formally defined in Section \ref{section_piPOMDP}.
As the set of the possibilistic beliefs is finite, 
they define the finite state space of an MDP, 
whose the probabilistic transitions are defined in the next section.
At this step, a Markov process based on epistemic states
is thus defined. 
Finally rewards are defined on epistemic states 
using the discrete Choquet integral
and leading to the definition of the resulting MDP.

Consider that possibility distributions similar to those used to define the initial
POMDP are available: a transition distribution, giving the possibility degree of
reaching $s' \in \mathcal{S}$ from $s \in \mathcal{S}$
using action $a \in \mathcal{A}$,  $\pi \paren{s' \sachant s,a} \in \mathcal{L}$; 
as well as an observation one, giving the possibility degree of observing 
$o \in \mathcal{O}$, in a system state $s \in \mathcal{S}$ 
after the use of action $a \in \mathcal{A}$, 
$\pi \paren{o' \sachant s',a} \in \mathcal{L}$. 
Indeed, this work
is devoted to two kinds of practical problems. On the one hand real problems
modeled as POMDPs are often intractable: our granulated approach is in this
case a simplification of the initial POMDP, and possibility distributions are computed 
from the POMDP probability distributions, using a possibility-probability
transformation \cite{Dubois93onpossibility/probability}. 
On the other hand, some problems lead to POMDPs with
partially defined probability distributions: some estimated probabilities have no
strong guarantees. 
A more faithful representation is given with possibility distri-
butions modeling the inherent imprecision, defining transition and observation
possibility distributions.
\subsection{Définition des transitions}
\label{setTrans}
First, we use here some notations:
the transition probability distribution is denoted by $T(s,a,s') = \textbf{p} \paren{s' \sachant s,a}$,
and the observation probability distribution by $O(s',a,o') = \textbf{p} \paren{ o' \sachant s',a}$.
If the agent selects action $a \in \mathcal{A}$ in the 
epistemic state $\beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$, 
the next epistemic state depends only on the next observation,
as highlighted by possibilistic belief update (see Theorem \ref{belief_process_recursif_poss}).
 The probability distribution over observations conditionned on the 
reached state is part of the POMDP definition via the observation function $O$. 
The probability distribution over observations conditionned on previous state is obtained using transition function $T$:
\[ \textbf{p} \paren{ o' \sachant s,a } = \sum_{s' \in \mathcal{S}} O(s',a,o') \cdot T(s,a,s') . \]
This distribution and the possibilistic belief $\beta$ about the system state, 
can lead to an approximate probability distribution over the next observations. Indeed,
a probability distribution over the system state, 
$\overline{\beta} \in \mathbb{P}_{\mathcal{S}}$, 
can be derived from $\beta$
using an extension of the Laplace principle.
Then approximate distribution over $o' \in \mathcal{O}$ is defined as
\begin{equation}
\label{obsProb}
\textbf{p} \paren{ o' \sachant \beta,a  } = \sum_{s \in \mathcal{S}} \textbf{p} \paren{ o' \sachant s, a } \cdot \overline{\beta}(s) .
\end{equation}
Finally, summing over concerned observations,
the transition probability distribution 
over epistemic states is defined as
\begin{equation}
\label{epistemTrans}
\textbf{p} \paren{ \tilde{\beta} \sachant \beta, a } = \sum_{o' | u(\beta,a,o')=\tilde{\beta }} \textbf{p} \paren{ o' \sachant \beta,a  }.
\end{equation}
A way to construct a probability distribution $\overline{\beta}$ from
a possibility one $\beta$ is the use of the pignistic transformation
\cite{Du2006.7} minimizing the arbitrariness
in the translation:
numbering system states with the order induced by distribution $\beta$,
$1 = \beta(s_1) \geqslant \beta(s_2) \geqslant 
\ldots \geqslant \beta(s_{\# \mathcal{S}+1}) = 0$, 
with $s_{\# \mathcal{S}+1}$ an artificial state such that $\pi(s_{\# \mathcal{S}+1})=0$ 
introduced to simplify the formula,
\begin{equation}
\label{transform} \overline{\beta}(s_i) = \sum_{j=i}^{\# \mathcal{S}} \frac{\beta(s_j) - \beta(s_{j+1})}{j}
\end{equation}
Note that this probability distribution corresponds to the center of gravity
of the probability distributions family induced by the possibility measure 
defined by distribution $\beta$ \cite{Dubois93onpossibility/probability}, and
respects the Laplace principle of Insufficient Reason (ignorance 
leads to uniform probability).

Although possibilistic belief states were so far defined in 
a qualitative way, degrees 
of $\mathcal{L}$ are considered as numerical in this section and
the following: the section about factorization will make it clear that
possibility distributions can be computed from $T$ and $O$ if the
sole purpose is to simplify the POMDP.
Numerical values are then used to compute the observation probability distribution
here, and in order to aggregate rewards according to the current epistemic state 
in the next section.

\subsection{Aggrégation des récompenses}
\label{aggreg}
After the transition function, it remains to assign a reward to each epistemic state:
in the classical probabilistic translation, the reward assigned to a belief state $b$ is
the reward expectation according to the probability distribution $b$: $\sum_{s \in \mathcal{S}} r(s,a) \cdot b(s)$.
Here, the agent knowledge is represented with a possibility distribution $\beta$: 
it sums up the frequentist uncertainty of the problem, 
and imprecision due to the possibilistic discretization 
and/or due to partial ignorance about actual probability distributions
defining the situation.
A way to define a reward being pessimistic about these imprecisions is to aggregate the
reward using the dual measure of the possibility distribution, and the \textit{Choquet integral}.

The dual measure of a possibility measure $\Pi:2^{\mathcal{S}} \rightarrow \mathcal{L}$
is called \textit{necessity measure} and is denoted by $N$. This measure is defined
by $\forall A \subseteq \mathcal{S}$, $N(A) = 1 - \Pi(\overline{A})$ where $\overline{A}$
is the complementary set of $A:$ $\overline{A} = \mathcal{S} \setminus A $.
We use now the notation $\mathcal{L} = \set{ l_1=1,l_2,l_3, \ldots, 0 }$.
For a given action $a \in \mathcal{A}$, reward values, 
$\set{ r(s,a) \sachant s \in \mathcal{S} }$ are denoted by
$\set{r_1,r_2,\ldots,r_k}$ with $r_1 > r_2 > \ldots > r_k$, 
with $k \leqslant \# \mathcal{S}$. An artificial value $r_{k+1}=0$ is also
introduced to simplify the formulae.

Discrete Choquet integral of the reward function against necessity measure $N$ \cite{DBLP:conf/ipmu/AmorFG10} is defined as follows:
\begin{eqnarray}
\label{choquet1}  Ch(r,N) = & \displaystyle \sum_{i=1}^{k} (r_i - r_{i+1}) \cdot N( \set{r(s,a) \geqslant r_i}) \\
\nonumber = & \displaystyle \sum_{i=1}^{k} (r_i - r_{i+1}) \cdot \croch{1 - \Pi( \set{r(s,a) < r_i}) } \\ 
\nonumber = & \displaystyle \sum_{i=1}^{k} (r_i - r_{i+1}) \cdot \paren{1 - \max_{s | r(s,a)<r_i} \pi(s)} \\ 
\nonumber = & \displaystyle r_1 - r_{k+1} - \sum_{i=1}^{k} (r_i - r_{i+1}) \cdot \max_{s | r(s,a)<r_i} \pi(s) \\
\nonumber = & \displaystyle r_1 - (r_1 - r_2) \cdot \max_{s | r(s,a)<r_1} \pi(s) - \ldots \\
\nonumber & \displaystyle - (r_{k-1} - r_{k}) \cdot \max_{s | r(s,a)<r_{k-1}} \pi(s) - r_k \cdot \max_{s | r(s,a) < r_k} \pi(s) \\
\label{choquet2} = & \displaystyle \sum_{i=1}^{\# \mathcal{L}-1} (l_i - l_{i+1}) \cdot \min_{s | \pi(s) \geqslant l_i } r(s)  
\end{eqnarray}
%with the convention $\displaystyle \max_{ \emptyset } \pi(s) = \Pi(\emptyset) = 0$. 
More on possibilistic Choquet integrals can be found in \cite{DBLP:journals/amai/Cooman01,DuPr2002.20}.

This reward aggregation using the necessity measure leads to a pessimistic estimation of the reward:
as an example, the reward $\displaystyle \min_{s \in \mathcal{S}} r(s,a)$ is assigned to the total ignorance.  

Note that, if the necessity measure $N$ is replaced by a probability measure $\mathbb{P}$, 
\textit{e.g} as the one induced by probability distribution $\beta$ using \ref{transform}, 
Choquet integral coincides with the expected reward based on $\overline{\beta}$.
This could be a good aggregation choice as well, but more optimistic than the one described above.
The most optimistic way to aggregate the reward  
is to compute the Choquet integral with the possibilistic
measure $\Pi$ induced by distribution $\beta$, rather than with
necessity one $N$, but this is not detailed here.

\subsection{PDM avec des états épistémiques flous}
This section summarizes the complete translation using final
equations of the previous sections.
This translation takes for input a POMDP: $\langle \mathcal{S},\mathcal{A},T,\mathcal{O},O,r \rangle$
and returns an epistemic states based MDP: $\langle \tilde{\mathcal{S}},\mathcal{A},\tilde{T},\tilde{r} \rangle$ with
\begin{itemize}
\item $\tilde{\mathcal{S}} = \Pi^{\mathcal{S}}_{\mathcal{L}}$;
\item $\tilde{T}$, such that $\forall (\beta, \tilde{\beta}) \in (\Pi^{\mathcal{S}}_{\mathcal{L}})^2$, $\forall a \in \mathcal{A}$ \\
$\tilde{T}(\beta,a,\tilde{\beta}) = \textbf{p} \paren{ \tilde{\beta} \sachant \beta,a }$ 
 using \ref{obsProb} and \ref{epistemTrans};
\item $\tilde{r}(a,\beta) = Ch(r(a,.),N_{\beta})$, using Equation \ref{choquet2} and where $N_{\beta}$ is the necessity
measure computed from $\beta$.
\end{itemize}
Finally, as in the probabilistic framework (see Section \ref{subsectionIHMDP}),
the criterion of this MDP is the expected total reward: 
\[ \mathbb{E}_{ (\beta_t) \sim \tilde{T}} \croch{ \sum_{t=0}^{+ \infty} \gamma^t \tilde{r}(\beta_t,d_t) }.\]

While the resulting state space is finite, 
only really small POMDP problems can be solved 
with this translation without computation tricks.
Indeed, $\Pi^{\mathcal{S}}_{\mathcal{L}}$ grows exponentially 
with the number of system states (see Equation \ref{equation_numberOfPossDistrib}), 
which makes the problem intractable even for state of the art MDP solvers.

\section{Factorisation}
\label{factorizationSection}
This section carefully derives a tractable MDP problem from a factored POMDP: 
the resulting MDP is equivalent to the former translation, but some factorization
and computational tricks are described here to reduce its size and to fit to the
factorized structure. First, the definition of a factored POMDP is quickly exposed,
followed by some dependency notations helpful for describing how distributions are dealt
with. Next, a classification of the state variables is made
to strongly adapt computations according to the nature of the state.
Then follows the definition of possibility distributions, 
and the description of the use of the possibilistic Bayes rule 
in practice ends this section.
\subsection{PDMPO factorisé}
Partially Observable Markov Decision Processes 
can be defined in a factorized way.
\begin{itemize}
\item state space $\mathcal{S} = s_1 \times \ldots \times s_m$ with 
$\forall j \in \set{ 1, \ldots, m}$, $s_j$ boolean variable. 
The set of boolean state variables is denoted by 
$\mathbb{S} = \set{ s_1, \ldots, s_m }$;
\item observation space $\mathcal{O} = o_1 \times \ldots \times o_n$ 
with $\forall i \in \set{ 1, \ldots, n}$, $o_i$ boolean variable.
In the same way as to state variables, the set of observation variables is denoted by 
$\mathcal{O}=\set{ o_1, \ldots, o_n }$;
\item action space $\mathcal{A}$, a finite set of actions $a \in \mathcal{A}$.
\end{itemize}
Note that a problem with non boolean variables can be easily reduced to such a problem
with the boolean variables assumption. 
For simplicity, and as state $s_j \in \mathcal{S}$ and observation $o_i \in \mathcal{O}$ notations are no longer reused 
in this chapter, only variables are denoted with these letters from now: $s \in \mathbb{S}$ and $o \in \mathbb{O}$.
 
Non-primed variables correspond to the
current time step, and primed variables to the next time-step.
This notation is also used for sets of variables: $\mathbb{S}'$ is the set
of next state variables and $\mathbb{O}'$ the set of next observable ones.
The factorized description continues with following probability distributions:
\begin{itemize}
\item $\forall j \in \set{1, \ldots, m}$, $\forall a \in \mathcal{A}$, a transition function is defined:
\end{itemize} 
\[ T_j^a(s_1, \ldots, s_m, s'_j) = \textbf{p} \paren{ s'_j \sachant s_1,\ldots,s_m, a}; \]
\begin{itemize}
\item One observation function is also given for each observation variable: $\forall i \in \set{1, \ldots, n}$, $\forall a \in \mathcal{A}$,
\end{itemize}
\[ O_i^a(s_1', \ldots, s_m', o'_i) = \textbf{p} \paren{ o'_i \sachant s_1',\ldots,s_m', a}; \]
\begin{itemize}
\item and reward function $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. 
\end{itemize}
These definitions lead to the following observations: $\set{ s'_j }_{j \in \set{1, \ldots, m}}$ 
are post-action independent, and $\set{ o'_i }_{j \in \set{1, \ldots, n}}$ post-transition independent. 
\subsection{Notations et fonction d'observation}
Transitions of the final MDP make it more handy if each variable depends
on only few previous variables: the procedure to avoid blocking such simplifications brought by the
structure of the initial POMDP during the translation, needs the following notations.
In practice, for each $i \in \set{1, \ldots, n}$ not all state variables influence 
observation variable $o_i'$; similarly, for each $j \in \set{1, \ldots, m}$, 
not all current state variables influence next state variable $s_j'$:
\begin{itemize}
\item for each action $a \in \mathcal{A}$, observation variable $o_i'$ 
depends on some state variables which are denoted by
\end{itemize}
\begin{eqnarray*} 
\mathcal{P}_a(o_i') = \set{ s_j' \in \mathbb{S}' \mbox{ s.t. } o_i' \mbox{ depends on } 
s_j' \mbox{ when } a \mbox{ applied} } 
\end{eqnarray*}
 %with related indices $\mathcal{J}^a_i = \set{ j \in 1,\ldots,m \mbox{ s.t. } s_j' \in parents_a(o_i') }$. 
They are called \textit{parents} as they appears as ``parents nodes'' in a dynamic Bayesian network \cite{Dean:1989:DBN} 
illustrating dependencies of the process.
\begin{itemize}
\item as well, for each action $a \in \mathcal{A}$, probability distribution 
of next state variable $s_j'$ 
depends on some current ones, denoted by
\end{itemize}
\begin{eqnarray*}
\mathcal{P}_a(s_j') = \set{ s_k \in \mathbb{S} \mbox{ s.t. } s_j' \mbox{ depends on } s_k \mbox{ when } a \mbox{ applied} }	
\end{eqnarray*}
and illustrated in Figure \ref{stateVarDep}.
%and related indices are $\mathcal{K}^a_j = \set{ k \in 1,\ldots,m \mbox{ s.t. } s_k \in parents_a(s_j	') }$. 
\begin{figure}
\begin{tikzpicture}
\node (rien) at (-5,0) {};
\node (a) at (2,0) {$a \in \mathcal{A}$ chosen};
\node (ra) at (2,-0.5) [rotate=270] {$\Rightarrow$};
\node (s1) at (0,-0.5) {$s_1$};
\node (dots1) at (0,-1) {$\vdots$};
\node (sk1) at (0,-1.5) {$s_{k_1}$};
\node (dots2) at (0,-2) {$\vdots$};
\node (sk2) at (0,-2.5) {$s_{k_2}$};
\node (dots3) at (0,-3) {$\vdots$};
\node (sk3) at (0,-3.5) {$s_{k_3}$};
\node (dots4) at (0,-4) {$\vdots$};
\node (sm) at (0,-4.5) {$s_m$};
\node (spj) at (3.5,-2.5) {$s_j'$};
\draw[->,>=latex,thick] (sk1) to (spj);
\draw[->,>=latex,thick] (sk2) to (spj);
\draw[->,>=latex,thick] (sk3) to (spj);
\coordinate (sk1bis) at (-0.7,-1.5) {};
\coordinate (sk2bis) at (-0.7,-2.5) {};
\coordinate (sk3bis) at (-0.7,-3.5) {};
\draw[thick,dashed] (sk1) to (sk1bis);
\draw[thick,dashed] (sk2) to (sk2bis);
\draw[thick,dashed] (sk3) to (sk3bis);
\draw[thick,dashed] (sk2bis) to (sk3bis);
\draw[thick,dashed] (sk1bis) to (sk2bis);
\node (parents) at (2,-5.5) {$\mathcal{P}_a(s_j') = \set{ s_{k_1},s_{k_2},s_{k_3} }$};
\coordinate (pa4) at (-0.7,-3) {};
\coordinate (pa3) at (-1.2,-3) {};
\coordinate (pa2) at (-1.2,-5.5) {};
\coordinate (pa) at (0,-5.5) {};
\draw[thick,dashed] (pa) to (pa2);
\draw[thick,dashed] (pa3) to (pa2);
\draw[thick,dashed] (pa3) to (pa4);
\end{tikzpicture}
\caption[Parents of a state variable given an action]{For action $a \in \mathcal{A}$, only tree variables influence variable $s'_j$ 
in this Bayesian network: $s_{k_1}$, $s_{k_2}$ and $s_{k_3}$, 
which constitute $\mathcal{P}_a(s_j')$.}
\label{stateVarDep}
\end{figure}

Now, let us define parents whatever the chosen action: $\forall$  $i=1,\ldots,n$,
\[ \mathcal{P}(o_i') = \cup_{a \in \mathcal{A}} \mathcal{P}_a(o_i') \subseteq \mathbb{S}' \]
and $\forall j=1,\ldots,m$,
\[ \mathcal{P}(s_j') = \cup_{a \in \mathcal{A}} \mathcal{P}_a(s_j') \subseteq \mathbb{S} \]
It leads to the following rewriting of probability distributions:
\[ T_j^a(\mathcal{P}(s'_j), s'_j) = \textbf{p} \paren{ s'_j \sachant \mathcal{P}(s'_j), a} \]
and
\[ O_i^a(\mathcal{P}(o'_j), o'_i) = \textbf{p} \paren{ o'_i \sachant \mathcal{P}(o'_j), a}. \]
Following subset of $\mathbb{S}$ is useful to specify observation dynamic:
\begin{eqnarray*}
\mathcal{Q}(o_i') = \set{ s_k \in \mathbb{S} \mbox{ s.t. } \exists s_j' \in \mathcal{P}(o_i') \mbox{ s.t. } s_k \in \mathcal{P}(s_j') } = \displaystyle \cup_{s_j' \in \mathcal{P}(o'_i)} \mathcal{P}(s_j') \subseteq \mathbb{S} 
\end{eqnarray*}
and is illustrated in Figure \ref{obsVarDep}. \\

In order to simplify notations, and as it causes no confusion, $\mathbb{S}$, $\mathcal{P}(o'_i)$, 
$\mathcal{P}(s'_i)$ and $\mathcal{Q}(o'_i)$ designate as well a set of variables, 
or a vector comprised of these variables (with an arbitrary order).
Distribution over $\mathcal{P}(o_i')$ assignments benefits from previous rewritings:
\begin{eqnarray}
\textbf{p} \paren{ \mathcal{P}(o_i') \sachant \mathbb{S},a} =  \hspace{-0.2cm} \prod_{s'_j \in \mathcal{P}(o_i')} T_j^a(\mathcal{P}(s'_j), s'_j) = \hspace{-0.2cm} \prod_{ s'_j \in \mathcal{P}(o_i')} \textbf{p} \paren{ s_j' \sachant \mathcal{P}(s_j'),a }
\label{distribPOP} =  \textbf{p} \paren{ \mathcal{P}(o_i') \sachant \mathcal{Q}(o_i') ,a}
\end{eqnarray} 
\begin{figure}
\begin{tikzpicture}
\node (rien) at (-4.6,0) {};
\node (a) at (1,1) {$\forall a \in \mathcal{A}$};
\node (s1) at (0,-0.6) {$s_1$};
\node (dots1) at (0,-1) {$\vdots$};
\node (sk1) at (0,-1.5) {$s_{k_1}$};
\node (dots2) at (0,-1.9) {$\vdots$};
\node (sk2) at (0,-2.4) {$s_{k_2}$};
\node (dots3) at (0,-2.8) {$\vdots$};
\node (sk3) at (0,-3.3) {$s_{k_3}$};
\node (dots4) at (0,-3.7) {$\vdots$};
\node (sk4) at (0,-4.2) {$s_{k_4}$};
\node (dots4) at (0,-4.6) {$\vdots$};
\node (sk5) at (0,-5.1) {$s_{k_5}$};
\node (dots4) at (0,-5.5) {$\vdots$};
\node (sm) at (0,-6) {$s_m$};

\node (sp1) at (3.5,-0.5) {$s_1'$};
\node (dotsp1) at (3.5,-1.2) {$\vdots$};
\node (spj1) at (3.5,-1.9) {$s'_{j_1}$};
\node (dotsp2) at (3.5,-2.6) {$\vdots$};
\node (spj2) at (3.5,-3.3) {$s'_{j_2}$};
\node (dotsp3) at (3.5,-4) {$\vdots$};
\node (spj3) at (3.5,-4.7) {$s'_{j_3}$};
\node (dotsp4) at (3.5,-5.4) {$\vdots$};
\node (smp) at (3.5,-6.1) {$s'_m$};
\draw[->,>=latex,thick] (sk1) to (spj1);
\draw[->,>=latex,thick] (sk2) to (spj1);
\draw[->,>=latex,thick] (sk3) to (spj1);
\draw[->,>=latex,thick] (sk2) to (spj2);
\draw[->,>=latex,thick] (sk4) to (spj2);
\draw[->,>=latex,thick] (sk1) to (spj3);
\draw[->,>=latex,thick] (sk5) to (spj3);
\node (opi) at (6,-3.3) {$o'_{i}$};
\draw[->,>=latex,thick] (spj1) to (opi);
\draw[->,>=latex,thick] (spj2) to (opi);
\draw[->,>=latex,thick] (spj3) to (opi);
\coordinate (sk1bis) at (-0.7,-1.5) {};
\coordinate (sk2bis) at (-0.7,-2.4) {};
\coordinate (sk3bis) at (-0.7,-3.3) {};
\coordinate (sk4bis) at (-0.7,-4.2) {};
\coordinate (sk5bis) at (-0.7,-5.1) {};
\draw[thick,dashed] (sk1) to (sk1bis);
\draw[thick,dashed] (sk2) to (sk2bis);
\draw[thick,dashed] (sk3) to (sk3bis);
\draw[thick,dashed] (sk4) to (sk4bis);
\draw[thick,dashed] (sk5) to (sk5bis);
\draw[thick,dashed] (sk1bis) to (sk5bis);
\node (parents) at (2.6,-7) {$\mathcal{Q}(o_i') = 
\set{ s_{k_1},s_{k_2},s_{k_3},s_{k_4},s_{k_5} }$};
\coordinate (qpa4) at (-0.7,-4.6) {};
\coordinate (qpa3) at (-1.2,-4.6) {};
\coordinate (qpa2) at (-1.2,-7) {};
\coordinate (qpa) at (0,-7) {};
\draw[thick,dashed] (qpa) to (qpa2);
\draw[thick,dashed] (qpa3) to (qpa2);
\draw[thick,dashed] (qpa3) to (qpa4);

\coordinate (h2) at (4,-1.2) {};
\coordinate (m2) at (4,-2.6) {};
\coordinate (b2) at (4,-4) {};
\draw[thick,dashed] (spj1) to (h2);
\draw[thick,dashed] (spj2) to (m2);
\draw[thick,dashed] (spj3) to (b2);
\draw[thick,dashed] (h2) to (b2);
\node (h2) at (4,0.5) {$\mathcal{P}(o_i') = \set{ s'_{j_1}, s'_{j_2}, s'_{j_3} }$};
\coordinate (pa4) at (4,-1.5) {};
\coordinate (pa3) at (4.5,-1.5) {};
\coordinate (pa2) at (4.5,0.2) {};
\draw[thick,dashed] (pa3) to (pa2);
\draw[thick,dashed] (pa3) to (pa4);
\end{tikzpicture}
\caption[Grand-parents of an observation variable]{Whatever the action $a \in \mathcal{A}$, only five state variables 
influence variable $o'_i$ in this Bayesian network: 
$s_{k_1}$, $s_{k_2}$, $s_{k_3}$, $s_{k_4}$, $s_{k_5}$ 
which constitute $\mathcal{Q}(o_i')$.}
\label{obsVarDep}
\end{figure}

Observation probability distributions, knowing previous state variables, 
are then defined $\forall i=1,\ldots,n$
\begin{equation}
\label{obsDistrib} \textbf{p} \paren{ o_i' \sachant \mathcal{Q}(o_i'), a } = \sum_{ v \in 2^{\mathcal{P}(o'_i)}} \textbf{p} \paren{ o'_i \sachant v ,a } \cdot \textbf{p} \paren{ v \sachant \mathcal{Q}(o_i') ,a}
\end{equation}
Therefore a possibilistic belief defined on $2^{ \mathcal{Q}(o_i')}$ is enough to
get the approximate probability distribution of an observation variable, Equation \ref{obsProb}: 
such an epistemic state
leads via transformation \ref{transform} to a probability distribution $\overline{\beta}$
over $2^{\mathcal{Q}(o_i')}$. 
Finally, the approximate probability distribution of the observation variable $i$,
factored counterpart of former equation \ref{obsProb}, is:
\begin{equation}
\label{probObel}
\textbf{p} \paren{ o_i' \sachant \beta,a } = \sum_{ v \in 2^{\mathcal{Q}(o_i')}} \textbf{p} \paren{ o_i' \sachant v,a } \cdot \overline{\beta}(v).
\end{equation}

\subsection{Classification des variables d'état}
\label{classif}
State variables $s \in \mathbb{S}$ do not play the same role
in the process: as already studied in the literature \cite{OngShaoHsuWee-IJRR10},
some variables can be visible for the agent, and namely this \textit{mixed-observability}
leads to important computational simplifications. Moreover, some variables do not
affect observation variables, and factorization of the POMDP is then easily transmitted
to the epistemic state based MDP. Finally, using rewrittings of previous sections,
useless computations are highlighted.
\begin{itemize}
\item A state variable $s_j$ is said to be \textbf{visible}, if $\exists o_i \in \mathbb{O}$, 
observation variable, such that $\mathcal{P}(o_i') = \set{ s_j' }$ 
and $\forall a \in \mathcal{A}$, $\textbf{p} \paren{ o_i' \sachant s_j',a } = \mathds{1}_{ \set{o_i' = s_j'}}$
\textit{i.e.} if $o_i' = s_j'$ almost surely. The set of visible state variables is denoted by 
$\mathbb{S}_v = \set{s_{v,1}, s_{v,2}, \ldots, s_{v,m_{v}}}$;
\end{itemize}

Observation variables corresponding to visible state variables can be removed 
from the set of observation variables: the number of observation variables becomes 
$\tilde{n}$, and remaining observation variables are denoted by $o_1,\ldots,o_{\tilde{n}}$.
\begin{itemize}
\item \textbf{Inferred hidden variables} are simply $\cup_{i=1}^{\tilde{n}} \mathcal{P}(o_i')$, 
\textit{i.e.} all hidden variables influencing (remaining) observation variables. The set of
inferred hidden variables is $\mathbb{S}_h = \set{s_{h,1},s_{h,2}, \ldots, s_{h,m_h}}$ and 
contains possibly visible variables.
\item \textbf{Non-inferred hidden variables} or \textbf{fully hidden variables}, denoted by $\mathbb{S}_f$, 
consists of hidden state variables which do not influence any observation, 
\textit{i.e.} all remaining state variables. The fully hidden variables are denoted by 
$s_{f,1},s_{f,2}, \ldots, s_{f,m_f}$, and the corresponding set is $\mathbb{S}_{f}$.
\end{itemize}
Of course, this classification leads to a partition of the initial set of state variables 
if potential visible variables are removed from
inferred hidden variables: denoting purely inferred hidden variables by $\overline{\mathbb{S}}_h = \mathbb{S}_h \setminus \mathbb{S}_v$, 
and $\overline{m}_h = \# \overline{\mathbb{S}}_h$ the state variables partition is 
$\mathbb{S} = \mathbb{S}_v \sqcup \overline{\mathbb{S}}_h \sqcup \mathbb{S}_f$ and $m = m_v + \overline{m}_h + m_f$.

The classification defined here is used to avoid some computations for visible variables:
if $s_{v} \in \mathbb{S}_v$ is visible, and $o_{v} \in \mathbb{O}$ is the associated observation ($s_v=o_v$ almost surely), 
computations of the distribution over $\mathcal{P}(o'_v)$, Equation \ref{distribPOP}, and
of the distribution over $o'_v$, Equation \ref{obsDistrib}, are unnecessary: the distribution 
over $s'_v$ $(=o'_v)$ needed is simply given by $\textbf{p} \paren{ s'_v \sachant \mathcal{P}(s'_v), a } $, 
data of the original problem. The counterpart of Equation \ref{probObel} is then
\begin{equation}
\label{probSVbel}
\textbf{p} \paren{ s'_v \sachant \beta, a } = \sum_{2^{\mathcal{P}(s'_v)}} \textbf{p} \paren{ s'_v \sachant \mathcal{P}(s'_v), a } \cdot \overline{\beta}(\mathcal{P}(s'_v)),
\end{equation}
where $\overline{\beta}$ is the probability distribution over $2^{\mathcal{P}(s'_v)}$ 
extracted from the possibilistic belief over the same space, using transformation (\ref{transform}).

\subsection{Mise à jour et manipulation de l'état de croyance}
This section is meant to define marginal belief distributions instead of a global one,
in order to benefit from the factorized structure of the initial POMDP. Indeed,
possibilistic belief distributions have different definitions according to which class of state variables they concern:
\begin{itemize}
\item as visible state variables are directly observed, there is no uncertainty over
 these variables. Two epistemic states (possibilistic belief distribution) 
are possible for visible state variable 
$s'_{v,j}$: 
$b_{v,T}'(s'_{v,j}) = \mathds{1}_{ \set{s'_{v,j}=\top}}$ and 
$b_{v,F}'(s'_{v,j}) = \mathds{1}_{\set{s'_{v,j}=\bot}}$. As a consequence, one boolean variable 
$\beta_{v,j}' \in \set{ \top, \bot }$ per visible state variables is enough to represent this 
belief distribution in practice: if $s'_{v,j}=\top$, then next belief is $b'=b_{v,T}'$ represented 
by belief variable assignment $\beta'_{v,j} = \top$, otherwise, next belief is $b'=b_{v,F}'$, and $\beta'_{v,j} = \bot$.
A belief variable of a visible state variable is denoted by $\beta_v$.
\item for each $i \in 1,\ldots,\tilde{n}$, each inferred hidden variable constituting 
$\mathcal{P}(o'_i)$ is an input of the same possibilistic belief distribution: 
non-normalized belief is, $\forall i=1,\ldots,\tilde{n}$
\end{itemize}
\begin{equation}
\label{inferredBel}
\tilde{b}'(\mathcal{P}(o_i')) = \max_{v \in 2^{\mathcal{Q}(o_i')}} \min \set{ \pi \paren{ o_i', \mathcal{P}(o_i') \sachant v,a}, b(v) }.
\end{equation}
where joint possibility distributions over $o_i' \times \mathcal{P}(o'_i)$, 
needed for belief process definition (possibilistic belief update, Theorem \ref{belief_process_recursif_poss}), 
are computed in the following way:
\begin{eqnarray*}
\pi \paren{ o_i', \mathcal{P}(o'_i) \sachant \mathcal{Q}(o'_i), a } & = & \min \set{ \pi \paren{ o_i' \sachant \mathcal{P}(o'_i), a}, \pi \paren{ \mathcal{P}(o'_i) \sachant \mathcal{Q}(o'_i), a  } } \\
& = & \min \set{ \pi \paren{ o_i' \sachant \mathcal{P}(o'_i), a}, \min_{s'_j \in \mathcal{P}(o'_i)} \pi \paren{ s'_j \sachant \mathcal{P}(s'_j), a  } }.
\end{eqnarray*}


A possibilistic normalization finalizes the belief update: for $w \in 2^{\mathcal{P}(o_i')}$,
\begin{eqnarray}
\label{possNorm} b'(w') & = & \left \{ \begin{array}{ccc} 1 \mbox{ if } w' \in \operatorname*{argmax}_{v' \in 2^{\mathcal{P}(o_i')}} \tilde{b}'(v'); \\
 \tilde{b}'(w') \mbox{ otherwise} . \end{array} \right.
\end{eqnarray} 
In practice, if $l=\# \mathcal{L}$ is the size of the possibility scale, and $p_i = \# \mathcal{P}(o_i')$, the number of belief states
is $l^{2^{p_i}} - (l-1)^{2^{p_i}}$, and then the number of belief variables is $n_{h,i} = \lceil \log_2(l^{2^{p_i}} - (l-1)^{2^{p_i}}) \rceil$.
A belief variable of an inferred hidden state variable is denoted by $\beta_h$.
\begin{itemize}
\item for each $j \in 1,\ldots,m_f$, non-normalized belief defined on fully hidden 
variable $s_{f,j}$ is 
\end{itemize}
\begin{equation}
\label{fullyHiddenBel}
\tilde{b}'(s_{f,j}') = \max_{v \in 2^{\mathcal{P}(s_{f,j}') }} \min \set{ \pi \paren{ s_{f,j}' \sachant v,a}, b(v) },
\end{equation}
which leads to the actual new belief state $b'$ after normalization (\ref{possNorm}).
In practice, as each fully hidden variable is considered independently from the others, 
following the previous reasoning for vector of inferred hidden s.v., 
the number of belief variables is $n_f = \lceil \log_2(l^2 - (l-1)^2) \rceil = \lceil \log_2(2l-1) \rceil$.
A belief variable of a fully hidden state variable is denoted by $\beta_f$. \\

Finally the actual global epistemic state $b'(\mathbb{S}')$ is upper bounded by 
\begin{equation}
b'(\mathbb{S}') = \min \set{ \min_{j=1}^{m_v} b'(s'_{v,j}), \min_{i=1}^{\tilde{n}} b'(\mathcal{P}(o_i')), \min_{k=1}^{m_f} b'(s_{f,k}') },
\end{equation}
where $\mathbb{S}$ has to be seen as a vector composed of all state variables.\\
The latter is considered as the agent belief
to make the final MDP factorized.

\subsection{Construction et utilisation des variables de croyance}
Starting with initial belief states defined for each visible state variable $s_{v,j}$,
for each vector of inferred hidden variables $\mathcal{P}(o'_i)$ and for each fully 
hidden variable $s_{f,j}$, these belief states are updated at each time step according to the
transformations described above.

However previous formulae \ref{inferredBel} 
and distribution over observation variable $o'_i \in \mathbb{O}$, Equation \ref{probObel}, 
depend on belief distribution
over $\mathcal{Q}(o'_i) \subseteq \mathbb{S}$.
They can be computed from the available belief states as follow: $\forall i=1,\ldots,\tilde{n}$,
\begin{eqnarray}
\label{bq}
b(\mathcal{Q}(o_i')) = \max_{v \in 2^{\mathcal{K}_i}}  \min \set{ \min_{s_{v} \in \mathcal{Q}(o_i') \cap \mathbb{S}_v} b(s_{v}), \min_{j \in \mathcal{J}_i }  b(\mathcal{P}(o_j)), \min_{s_{f} \in \mathcal{Q}(o_i') \cap \mathbb{S}_f} b(s_{f}) },
\end{eqnarray}
where
\begin{itemize}
\item $\mathcal{J}_i = \set{ j \in \set{1,\ldots,\tilde{n}} \mbox{ s.t. } \mathcal{P}(o_j) \cap \mathcal{Q}(o_i') \neq \emptyset}$, \textit{i.e.} $\mathcal{J}_i$ is the set of indices $j$ 
for which $\mathcal{P}(o_j)$ shares (inferred hidden) state variables with $\mathcal{Q}(o_i')$,
and
\item $\mathcal{K}_i = \set{\cup_{j \in \mathcal{J}_i} \mathcal{P}(o_j)} \setminus \mathcal{Q}(o_i') \subseteq \mathbb{S}_h$, \textit{i.e.} $\mathcal{K}_i$ 
is the set of (inferred hidden) state variables which are not present in 
$\mathcal{Q}(o_i')$, but are present in a set $\mathcal{P}(o_j)$ 
sharing state variables with $\mathcal{Q}(o_i')$. 
\end{itemize}
As well, belief update for fully hidden state variables, Equation \ref{fullyHiddenBel}, 
needs a belief distribution over variables $\mathcal{P}(s_{f,j}')$:
$\forall j=1,\ldots,m_f$, 
\begin{eqnarray} 
\label{bp}
b(\mathcal{P}(s_{f,j}')) =  \max_{v \in 2^{\mathcal{N}_j}}  \min \set{ \min_{s_{v} \in \mathcal{P}(s_{f,j}') \cap \mathbb{S}_v} b(s_{v}),
\nonumber\min_{k \in \mathcal{M}_j }  b(\mathcal{P}(o_k)), \min_{s_{f} \in \mathcal{P}(s_{f,j}') \cap \mathbb{S}_f} b(s_{f}) },
\end{eqnarray}
where
\begin{itemize}
\item $\mathcal{M}_j= \set{ k \in \set{1,\ldots,m_f} \mbox{ s.t. } 
\mathcal{P}(o_k) \cap \mathcal{P}(s_{f,j}') \neq \emptyset}$, \textit{i.e.} $\mathcal{M}_j$ 
is the set of indices $k$ for which $\mathcal{P}(o_k)$ shares (inferred hidden) state variables with $\mathcal{P}(s_{f,j}')$, and
\item $\mathcal{N}_j = \set{\cup_{k \in \mathcal{M}_j} \mathcal{P}(o_k)} 
\setminus \mathcal{P}(s_{f,j}') \subseteq \mathbb{S}_h$, \textit{i.e.} $\mathcal{N}_j$ 
is the set of (inferred hidden) state variables which are not present in 
$\mathcal{P}(s_{f,j}')$, but are present in a set $\mathcal{P}(o_k)$ 
sharing state variables with $\mathcal{P}(s_{f,j}')$. 
\end{itemize}
Finally, a belief distribution over $\mathcal{P}(s_{v,i}')$ needed to define an
approximate probability distribution over visible state variables (Equation \ref{probSVbel}),
can be defined in the same way, marginalizing ($\max$) over unused variables.

\section{Résoudre un PDMPO avec un algorithme pour PDM}
The previous section leads to a factored MDP, whose the version used in practice is defined here.
A concrete POMDP problem and its resulting MDP are then described in order to highlight
the power in state space size reduction of the possibilistic structured translation.
\subsection{PDM factorisé résultant}
Section \ref{classif} classifies state variables in order to define epistemic states $b$
over sets of state variables (respectively $\forall j=1,\ldots,m_v$, $\set{s^v_j}$, 
$\forall i=1,\ldots,\tilde{n}$, $\mathcal{P}(o'_i)$, and $\forall k=1,\ldots,m_f$, 
$\set{ s^f_j }$) and set of variables encoding them (respectively $\beta_v$, $\beta_h$ and  
$\beta_f$) independently to each other. As belief updates are deterministic knowing the
observation, a simple trick is used to keep this determinism in the final MDP:
a $flipflop$ boolean variable is introduced, changing its state at each step, denoted
by $f$. It
artificially divides a classical time step of the POMDP into two phases.
During the first phase, called \textit{the observation generation phase}, 
non-identity transition functions 
(\textit{i.e.} which do not let the variable remain the same)
are the probability distributions over observation variables \ref{probObel}
and visible state variables \ref{probSVbel}.

During the second phase, called \textit{the belief update phase}, 
non-identity transition functions are
the deterministic transitions of the belief variables:
\begin{itemize}
\item variable $\beta_v$ is updated knowing value of the corresponding visible variable $s_v$;
\item variables $\beta^{1}_{h}, \ldots, \beta^{n_{h,i}}_h$ are updated knowing value of observation variable $o_i$, 
and using update \ref{inferredBel}, \ref{possNorm};
\item variables $\beta^{1}_{f}, \ldots, \beta^{n_{f}}_f$ using update \ref{fullyHiddenBel}.
\end{itemize}

The state space is then defined as:  \\
$\mathcal{S} = f \times s^1_v \times \ldots \times s^{m_v}_v 
\times o^1 \times \ldots \times o^{\tilde{n}}  \times \beta^1_v \times \ldots 
\times \beta^{m_v}_v \times \beta^1_h  \times \ldots \times \beta^{\tilde{n}}_h 
\times \beta^1_f \times \ldots \times \beta^{m_f}_f $, where $\forall i=1,\ldots,\tilde{n}$, 
$\beta^i_h$ represents boolean variables $\beta^{1,i}_{h}, \ldots, \beta^{n_{h,i},i}_h$, and
$\forall k=1,\ldots,m_f$, $\beta^{j}_f$ represents boolean variables $\beta^{1,j}_{f}, \ldots, \beta^{n_{f},j}_f$.

\begin{figure}\centering
\begin{tikzpicture}[scale=1.2,transform shape]

%TIME/BACKGROUND
\coordinate (middleTop) at (5.7,3.6);
\coordinate (middleBot) at (5.7,-1.7);
\draw[thick,color=black!20] (middleTop) -- (middleBot);
\node [font=\huge] (statet) at (3.2,-0.8) {$t$};
\coordinate (middleTop2) at (9.5,3.6);
\coordinate (middleBot2) at (9.5,-1.7);
\draw[thick,color=black!20] (middleTop2) -- (middleBot2);
\node [font=\huge] (statetplus1) at (7.2,-0.8) {$t+1$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%vertex
%vars
\tikzstyle{vertex}=[circle,fill=black!30,minimum size=20pt,inner sep=0pt]
\tikzstyle{vertex2}=[circle,fill=black!15,minimum size=20pt,inner sep=0pt]
%r
\tikzstyle{vertex3}=[ draw, inner sep=0pt, minimum size=15pt]

%1
\node[vertex] (state1) at (3.3,3) {$v_t$};
%\node[vertex] (state13) at (3.3,2) {$s^v_{t}$};
\node[vertex2] (state12) at (3.3,1) {$\beta_{t-1}$};
%1bis
\node[vertex2] (state1bis) at (5,3) {$v_t$};
%\node[vertex2] (state13bis) at (5,2) {$s^v_{t}$};
\node[vertex] (state12bis) at (5,1) {$\beta_t$};
%R1
\node[vertex3] (R1) at (4.4,-0.1) {$r_t$};

%2
\node[vertex] (state2) at (7,3) {$v_{t+1}$};
%\node[vertex] (state23) at (7,2) {$s^v_{t+1}$};
\node[vertex2] (state22) at (7,1) {$\beta_{t}$};

%2bis
\node[vertex2] (state2bis) at (8.7,3) {$v_{t+1}$};
%\node[vertex2] (state23bis) at (8.7,2) {$s^v_{t+1}$};
\node[vertex] (state22bis) at (8.7,1) {$\beta_{t+1}$};
%R2
\node[vertex3] (R2) at (8.1,-0.1) {$r_{t+1}$};

%0
\node (state02) at (2,1) {};
\node (state03) at (2,2) {};
%3
\node (state3) at (10.5,3) {};
\node (state33) at (10.5,2) {};
\node (state32) at (10.5,1) {};

\node (state32bis) at (10.5,0) {};

%action
\node[fill=black!30] (action) at (5,-1.2) {$a_t$};
\node (action0) at (2,0) {};
\node (action02) at (2,-0.7) {};
\node[fill=black!30] (action3) at (8.7,-1.2) {$a_{t+1}$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS

%1->2
\draw[->,>=latex] (state1) -- (state1bis);
\draw[->,>=latex,very thick] (state1) -- (state12bis);
\draw[->,>=latex, very thick] (state12) -- (state12bis);
%\draw[->,>=latex] (state13) -- (state13bis);
%1->2bis
\draw[->,>=latex, very thick] (state12bis) -- (state2);
\draw[->,>=latex] (state12bis) -- (state22);
%\draw[->,>=latex, very thick] (state13bis) -- (state2);
%\draw[->,>=latex, very thick] (state13bis) -- (state23);
%\draw[->,>=latex, very thick] (state12bis) -- (state23);
%\draw[->,>=latex, very thick, color=red] (state13bis) -- (state22);
%0->1
\draw[->,>=latex,dashed] (state02) -- (state1);
\draw[->,>=latex,dashed] (state02) -- (state12);
%\draw[->,>=latex,dashed] (state02) -- (state13);
%\draw[->,>=latex,dashed] (state03) -- (state13);
%2->3
\draw[->,>=latex] (state2) -- (state2bis);
\draw[->,>=latex, very thick] (state2) -- (state22bis);
\draw[->,>=latex, very thick] (state22) -- (state22bis);
%\draw[->,>=latex] (state23) -- (state23bis);
%2->3bis
\draw[->,>=latex,dashed, very thick] (state22bis) -- (state3);
\draw[->,>=latex,dashed] (state22bis) -- (state32);
%\draw[->,>=latex,dashed, very thick] (state22bis) -- (state33);
%\draw[->,>=latex,dashed, very thick] (state23bis) -- (state33);
%\draw[->,>=latex,dashed, very thick] (state23bis) -- (state3);
%\draw[->,>=latex,dashed, very thick, color=red] (state23bis) -- (state32);
%action
%1
\draw[->,>=latex] (action) -- (state2);
\draw[->,>=latex] (action) -- (state22bis);
%\draw[->,>=latex] (action) -- (state23);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (action) -- (R1); 
%0
\draw[->,>=latex,dashed] (action0) -- (state1);
%\draw[->,>=latex,dashed] (action0) -- (state13);
\draw[->,>=latex,dashed] (action02) -- (state12bis);
%3
\draw[->,>=latex,dashed] (action3) -- (state3);
%\draw[->,>=latex,dashed] (action3) -- (state33);
\draw[->,>=latex,dashed] (action3) -- (state32bis);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (action3) -- (R2); 

% R
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (state12bis) -- (R1); 
%\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (state1) -- (R1); 
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (state22bis) -- (R2); 
%\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (state2) -- (R2); 
\end{tikzpicture}
\caption[Practical DBN of the resulting MDP]{Practical DBN of the resulting MDP: thickest arrows illustrate transitions which are not
identity transitions.}
\label{DBN}
\end{figure}

The resulting MDP is illustrated in Figure \ref{DBN} where $\beta_t$ represents all belief variables,
and $v_t$ the visible variables: flipflop variable $f$, observations $o_i$ and visible state variables $s_v$.

This trick makes the belief update phase deterministic. Each belief variable
transition can be then deterministically defined, and independently from
each other: as visible state and observation variables are already post-action independent, 
the resulting MDP is a factored MDP.
\subsection{Résultats pour un PDMPO concrêt}
A problem inspired by the RockSample problem \cite{Smith:2004:HSV:1036843.1036906} is described in this section
to illustrate the factorized possibilistic discretization of the agent belief, from a factored POMDP:
a rover is navigating in a place described by a finite number of locations $l_1, \ldots, l_n$,
and where stand $m$ rocks. Some of these $m$ rocks have an interest in the scientific
mission of the rover, and it has to sample them. However, sampling a rock is an expensive
operation. The rover is thus fitted with a long range sensor making him able to estimate if
the rock has to be sampled. Finally operating time of the rover is limited, but its battery
level is available.

Variables of this problem can now be set, and classified as in Section \ref{classif}:
as the battery level is directly observable by the agent (the rover), the set of
visible state variables consists of the boolean variables encoding it: $\mathbb{S}_v = \set{B_1, B_2, \ldots, B_k}$.
The agent knows the different locations of the rocks, however the nature of a rock is estimated.
The set of inferred hidden state variables consists of $m$ boolean variables $R_i$ encoding the nature
of the $i^{th}$ rock, $\top$ for ``scientifically good'' and $\bot$ otherwise: 
$\mathbb{S}_h = \set{R_1, R_2, \ldots, R_m}$. When the $i^{th}$ rock is observed using the sensor, it returns a noisy 
observation of the rock in $\set{\top,\bot}$, modeled by the boolean variable $O_i$: the set of observation
variables is then $\mathbb{O} = \set{O_1, O_2, \ldots, O_m}$.
Finally, no localization equipment is provided: the agent estimates its location from its initial information,
and its dynamics. Each location of the rover is formally described by a variable $L_j$, which equals $\top$
if the rover is at the $j^{th}$ location, and $\bot$ otherwise. The set of fully hidden variables consists thus
of these $n$ variables: $\mathbb{S}_f = \set{L_1, L_2, \ldots, L_{n}}$. 

Initial location is known, described by variable $L_1$, and leading to a deterministic initial belief state: 
$\beta_0(\mathcal{S}_h) = 1$ if $L_1=\top$ and $L_j = \bot$ $\forall j \neq 1$, $0$ otherwise. However
the initial nature of each rock is not known. Instead of a uniform probability distribution over the
rocks nature (``rock has to be sampled'', or ``rock is not interesting''), Possibility Theory allows to represent this initial ignorance with the
marginal belief $\beta_0(\mathcal{S}_h) = 1$, for each assignment of the hidden inferred state variables
modelling nature of the rocks. 

Finally, the factorization trick leads to a reduction of the domain size: 
with a flat translation of this POMDP, the size of the resulting state space is described with
$\lceil \log_2( \# \mathcal{L}^{2^{n+m+k}} - (\# \mathcal{L}-1)^{2^{n+m+k}}) \rceil$ boolean variables.
Taking advantage of the POMDP structure, the resulting state space is encoded with 
$1+2 \cdots k+m+(m+n).\lceil \log_2(2 \# \mathcal{L} -1 )  \rceil$ Boolean variables:
the flipflop variable, the visible variables and associated beliefs variables, 
the observation variables, and the belief variables 
associated to the fully hidden and inferred hidden variables.

Moreover, the dynamic of the resulting MDP is factorized: all variables are independent post-action,
and lots of them are deterministic, thank to the flipflop variable trick.
These structures are beneficial to the MDP solvers,
leading to faster computations.

%preprocessing with ADDs \cite{Bahar:1997:ADD}
%\cite{SannerIPPC11}
\section{Conclusion}
This chapter described a hydrid translation 
of a POMDP problem into a finite state space MDP one:  
Qualitative Possibility Theory
is used here to maintain an epistemic state during the process. 
The MDP problem, result of this translation, 
is entirely built defining transition and reward functions
over these epistemic states. 
Definitions of these functions use respectively the pignistic transformation,
used to recover a probability distribution 
from an epistemic state, 
and the Choquet integral with respect to the necessity, 
making the agent pessimistic about the potential ignorance 
described by its epistemic state.
A practical way to implement this translation 
is then described: 
with these computations, 
a factored POMDP leads to a factored and tractable MDP problem. 
The essential particularity of this translation 
is the granular modeling of the agent belief 
using a qualitative fuzzy knowledge representation 
Finally this promising approach will be tested on RDDL files 
of the IPPC competition \cite{SannerIPPC1111}
using a state of the art MDP planner like PROST 
\cite{DBLP:conf/aips/KellerE12}. Indeed these files describe
factored POMDP problems as introduced in Section \ref{factorizationSection}.
