\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\section*{Contexte}
%%% CONTEXT
\malettrine{L'}{autonomie} décisionnelle d'un robot provient, 
entre autre, 
du calcul d'une fonction 
appelée \textit{stratégie} ou \textit{politique}: 
celle-ci retourne 
le symbole de l'action à exécuter
en fonction de l'historique 
des données des capteurs du robot.
Les caractéristiques d'intérêt du robot
et de son environnement 
forment un \textit{système}.
En général, 
pour une séquence donnée 
d'actions exécutées par le robot,
l'évolution de ce système n'est pas déterministe, 
mais son comportement peut être étudié 
en effectuant de nombreux tests 
sur le système robotique, 
ou en utilisant les connaissances d'un expert.
De même, 
les données provenant des capteurs, 
brutes ou traitées,
sont généralement 
des éléments incertains:
cependant, 
le comportement de ces données,
appelées aussi \textit{observations} du système, 
dépend des actions du robot 
et des états successifs du système.
Les relations entre les observations, 
les états du système et les actions 
peuvent aussi être obtenues 
à l'aide de tests des capteurs 
dans de nombreuses situations, 
ou en utilisant la description de ces même capteurs,
du traitement des données, 
ou n'importe quelle information experte.
Par exemple, 
dans le cas d'un robot 
utilisant une vision artificielle,
la sortie de l'algorithme 
de traitement d'image utilisé
est considérée 
comme une observation du système
puisque résultat 
d'un traitement des données des capteurs,
et entrée du modèle de décision:
ici, les données sont les images provenant de la camera.
Pour une camera données, et un algorithme de vision donné,
le comportement de l'observation
est lié à l'action
et à l'état du système
lors du processus de prise des images.

Ainsi, dans le but de rendre un robot autonome
pour une \textit{mission} donnée, %%%%%%
nous cherchons une stratégie, \textit{i.e.} 
une fonction précisant les actions à exécuter
conditionnellement à la séquence d'observations du système,
tenant compte de l'incertitude à propos
de l'évolution du système et de ses observations. 
Le domaine de recherche associé 
à ce type de problème,
\textit{i.e.} le calcul de stratégies,
n'est pas restreint à la robotique
et est appelé \textit{décision séquentielle dans l'incertain}:
dans le cas général, l'entité qui doit effectuer l'action est appelée
\textit{l'agent}.
Dans cette thèse, bien que les résultats fournis sont principalement théoriques
et assez généraux pour concerner des applications plus variées,
le problème du calcul de stratégie
est étudié ici dans le contexte de la robotique autonome,
et l'agent est la partie décisionnelle du robot.
Calculer une stratégie pour une mission robotique donnée
nécessite un cadre adapté:
le modèle le plus connu décrit le comportement de l'état du système 
et des observations en utilisant la théorie de probabilités.

\subsection*{Un modèle probabiliste pour le calcul de stratégies}

Les processus décisionnels de Markov (PDMs)
expriment aisément les problèmes de décision séquentielle
sous incertitude probabiliste \cite{Bel}.
Ils sont adaptés au calcul des stratégies
si l'état du système est connu par l'agent 
à chaque moment de la mission.
Dans le contexte robotique,
cette hypothèse signifie
que la mission considérée
nous permet de faire l'hypothèse
que le robot a une connaissance parfaite
des caractéristiques d'intérêt via ses capteurs.
Dans ce modèle,
l'état du système est noté $s$,
et l'ensemble fini de tous les états possibles du système 
est $\mathcal{S}$.
L'ensemble fini $\mathcal{A}$
est l'ensemble de toutes les actions disponibles pour l'agent.
Le temps est discrétisé en étapes de décision
représentées par les entiers $t \in \mathbb{N}$.

Il est supposé que la dynamique de l'état du système
est \textit{Markovienne}:
à chaque étape de temps $t$,
l'état suivant $s_{t+1} \in \mathcal{S}$,  
ne dépend que de l'état courant $s_t \in \mathcal{S}$
et de l'action choisie $a_t \in \mathcal{A}$.
Cette relation est décrite par la fonction de transition
$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t}$,
définie comme la distribution de probabilité
sur l'état suivant $s_{t+1}$ 
conditionnellement à l'état courant $s_t \in \mathcal{S}$ 
lors de l'exécution de l'action $a_t \in \mathcal{A}$.

La mission de l'agent est décrite en terme de \textit{récompenses}:
une fonction de récompense $r: (s,a) \mapsto r(s,a) \in \mathbb{R}$ 
est définie pour chaque action $a \in \mathcal{A}$
et chaque état du système $s \in \mathcal{S}$.
Elle modélise le but de l'agent.
Chaque valeur de récompense $r(s,a) \in \mathbb{R}$
est une motivation locale pour l'agent.
En effet, plus l'agent récupère de récompenses pendant l'exécution du processus,
mieux il a réalisé sa mission:
une mission est considéré bien remplie
si la séquence d'états du système $s_t \in \mathcal{S}$ rencontrés
et d'actions $a_t \in \mathcal{A}$ sélectionnées
mènent à des récompenses $r(s_t,a_t)$ grandes.
La résolution d'un PDM correspond au calcul
d'une stratégie optimale,
\textit{i.e.} d'une fonction prescrivant les actions $a \in \mathcal{A}$
qu'il faut exécuter au cours du temps 
afin de maximiser la moyenne
de la somme des récompenses obtenues durant une exécution:
cette moyenne est calculée 
en prenant compte le comportement probabiliste des états du système
décrit par les fonctions de transition
$\textbf{p} \paren{s_{t+1} \sachant s_t,a_t}$.
Par exemple, une bonne stratégie
peut être une fonction $d$ définie sur $\mathcal{S}$
et à valeurs dans $\mathcal{A}$, 
puisque l'on suppose ici que l'agent connaît l'état du système
durant le processus.

%%% ROBOT POMPIER
\begin{figure} \centering
\definecolor{ggreen}{rgb}{0.3,0.7,0.4}
\begin{tikzpicture}
\node (rpomp) at (-1,4.7) {\includegraphics[scale=0.9]{robot_pompier}};
\node (bli) at (0,6.5) {};
\node (pomdp3) at (3.9,6) {\color{orange}{$s \in \mathcal{S}$}: \color{black}{\textbf{état du système}}};
\node (t1) at (2,6) {};
\node (r1) at (-2.3,5.5) {};
\node (r11) at (1,4.5) {};
\draw[->,>=latex,color=orange!60,line width=1mm] (t1) to (r1);
\draw[->,>=latex,color=orange!60,line width=1mm] (t1) to (r11);
\node (pomdp4) at (5.8,4.45) {\color{blue!60}{$o \in \mathcal{O}$:} \color{black} \textbf{observations du système}};
\node (t2) at (3.3,4.4) {};
\node (r2) at (-2.3,5.05) {};
\draw[->,>=latex,color=blue!40,line width=1mm] (t2) to[bend left] (r2);
\node (pomdp5) at (1.8,3) {\color{red}{$a \in \mathcal{A}$:} \color{black} \textbf{actions de l'agent}};
\node (t3) at (-0.3,3) {};
\node (r3) at (-2,4.2) {};
\draw[->,>=latex,color=red!50,line width=1mm] (t3) to (r3);
\node (pomdp6) at (-5.5,5.5) {\color{ggreen}{$b \in \mathbb{P}^{\mathcal{S}}$:} \color{black} }; %%% ORANGE?
\node (pomdp7) at (-5,5) {\color{black} \textbf{état de croyance} }; %%% ORANGE?
\node (pomdp6) at (-3.5,6) {\color{ggreen}{\Huge \textbf{?}}}; %%% ORANGE?
\end{tikzpicture}
\caption[Utilisation d'un PDMPO pour la modélisation du robot pompier]{
Utilisation d'un PDMPO pour la modélisation du robot pompier:
dans cet exemple, la mission du robot est la prévention des incendies.
L'état du système $s \in \mathcal{S}$ décrit par exemple la position du robot, 
l'orientation du jet d'eau, la quantité d'eau utilisée,
la position du feu et son niveau sur une échelle ``petit feu'' et ``feu important'', etc.
En utilisant une vision artificielle et des capteurs de chaleur,
le robot reçoit des \textbf{observations} $o \in \mathcal{O}$
qui sont les données brutes ou traitées provenant des capteurs:
la sortie d'un classifieur dont l'entrée est une image de la scène
(\textit{cf.} Figure \ref{observation_robot}), 
et qui renvoie le niveau ou la position du feu,
peut être modélisé par une observation.
Finallement, les \textbf{actions du robot} $a \in \mathcal{A}$ 
sont par exemple la mise en marche des moteurs
impactant la rotation des roues du robot, 
le débit de pompage,
l'orientation du jet d'eau ou des capteurs, etc.
%Uncertainty dynamics is described by conditional probability distributions:
La \textbf{fonction de récompense} $r(s,a)$
décroît avec le niveau de l'incendie.
Afin de ne pas gaspiller d'eau,
un coût proportionnel à la quantité d'eau
est soustrait à cette récompense:
puisque une stratégie optimale maximise la moyenne de la somme des récompenses,
le but du robot est donc d'attaquer les incendies sans gaspiller d'eau.
Cette moyenne peut-être calculée à l'aide des probabilités décrivant la dynamique stochastique du système.
Les actions du robot $a \in \mathcal{A}$ ont un effet probabiliste sur le système,
décrit par la \textbf{fonction de transition} $\textbf{p} \paren{s' \sachant s,a}$: 
par exemple, l'activation des roues du moteur modifie la position du robot,
et la probabilité sur chacune des positions suivantes possibles,
étant donné la position courante,
prend part à la définition du PDMPO.
Un autre exemple est l'action modifiant l'orientation du jet d'eau,
qui redéfinit la probabilité du nouveau niveau de feu étant donné l'état actuel du système.
Les actions du robot $a \in \mathcal{A}$
et les états suivants $s' \in \mathcal{S}$ 
peuvent aussi impacter les observations des capteurs:
cette influence est définie par la \textbf{fonction d'observation} $\textbf{p} \paren{o' \sachant s',a}$: 
par exemple, l'orientation du capteur de vision peut modifier
la probabilité de détection du feu, 
ou de l'évaluation de son intensité, 
qui font partie des observations $o' \in \mathcal{O}$. 
Finalement, l'état de croyance est la distribution de probabilité sur l'état courant du système 
conditionnellement à l'ensemble des observations et actions successives depuis le début du processus:
c'est la meilleure estimation possible puisque le robot n'a accès qu'aux actions et observations
lors de l'exécution.}
\label{robot_pompier}
\end{figure}

Il a été montré que de telles stratégies markoviennes sont optimales
pour certain critères
tels que celui basé sur la somme des récompenses décomptées:
en effet, un critère bien connu mesurant 
les performances d'une stratégie $d$
est l'espérance de la somme actualisée des récompenses: 
\begin{equation}
\label{criterion}
\mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,d_t) },
\end{equation}
où $d_t=d(s_t) \in \mathcal{A}$ 
et $0<\gamma<1$ est un facteur d'actualisation
assurant la convergence de la somme.

L'hypothèse que l'agent a une connaissance parfaite de l'état du système
est assez forte:
en particulier, dans le cas des robots réalisant des taches avec des capteurs conventionnels,
ces derniers sont souvent incapable de fournir au robot
toutes les caractéristiques d'intérêt pour la mission.
Ainsi, un modèle plus flexible, 
\textit{i.e.} qui tient compte de \textit{l'observation partielle} du système par l'agent, a été construit.
%%% ROBOT POMPIER FIN

%%% POMDP
En effet, les MDP Partiellement Observables 
(PDMPO) \cite{Smallwood_Sondik} 
ont une puissance de modélisation plus importante,
car ils peuvent représenter des situations 
dans lesquelles l'agent
ne connaît pas directement l'état courant du système:
ils modélisent de manière plus fine 
un agent exécutant des actions 
sous incertitude dans un environnement
partiellement observable. 

L'ensemble des états du système $\mathcal{S}$,
l'ensemble des actions $\mathcal{A}$, 
la fonction de transition $\textbf{p} \paren{ s_{t+1} \sachant s_t,a_t}$ 
et la fonction de récompense $r(s,a)$ 
restent les même que pour la définition des PDM.
Dans ce modèle, puisque l'état courant du système
$s \in \mathcal{S}$ 
ne peut pas être considéré comme une information accessible pour l'agent,
la connaissance de l'agent à propos de l'état du système 
provient des observations $o \in \mathcal{O}$, 
où $\mathcal{O}$ est un ensemble fini. 
%A full definition of this process 
%includes as well the set of 
%possible observations of the system, 
%$o \in \mathcal{O}$. 
La fonction d'observation 
$\textbf{p} \paren{ o_{t+1} \sachant s_{t+1},a_t }$
donne pour chaque action $a_t \in \mathcal{A}$
et état atteint $s_{t+1} \in \mathcal{S}$, 
la probabilité sur les observations possibles $o_{t+1} \in \mathcal{O}$. 
Enfin, \textit{l'état de croyance initial} $b_0(s)$
définit la distribution de probabilité \textit{a priori}  
sur l'état du système.
Un exemple d'usage des PDMPO est illustré 
dans la figure \ref{robot_pompier}.

Résoudre un POMDP consiste à calculer une stratégie 
qui renvoie une action adéquate à chaque étape du processus, 
et dépendente des observations reçues et des actions sélectionnées
\textit{i.e.} de toutes les données disponibles pour l'agent:
un critère pour la stratégie peut aussi être
l'espérance de la somme actualisée des récompenses (\ref{criterion}).

%
% belief
%
La plupart des algorithmes raisonnent sur \textit{l'état de croyance},
défini comme la distribution de probabilité sur l'état du système
conditionnellement à toutes les observations du système 
et les actions choisies par l'agent depuis le début du processus.
Cet état de croyance est mis à jour à chaque étape de temps en utilisant la règle de Bayes,
l'action courante,
et la nouvelle observation.
\`A une étape donnée $t \in \mathbb{N}$, 
$b_t(s)$ est défini
comme la probabilité que le $t^{ième}$ état du système soit 
$s \in \mathcal{S}$, connaissant les observations et actions précédentes,
ainsi que l'état de croyance initial $b_0$:
c'est une estimation de l'état du système
qui utilise uniquement les données disponibles
puisque l'état n'est pas directement observable.

Il peut être facilement calculé de manière récursive avec la règle de Bayes: 
à l'étape de temps $t$, 
si l'état de croyance est $b_t$, 
l'action choisie $a_t \in \mathcal{A}$ 
et la nouvelle observation 
$o_{t+1} \in \mathcal{O}$, 
l'état de croyance suivant
\begin{eqnarray}
\label{probBayesRule}
b_{t+1}(s')  \propto \textbf{p} \paren{ o_{t+1} \sachant s', a_t } \cdot \sum_{s \in \mathcal{S}} \textbf{p} \paren{s' \sachant s,a_t} \cdot b_t(s).
\end{eqnarray}
comme illustré par le réseau Bayesien de la figure \ref{BayesNetPOMDP}.
%
% BAYESNET
%
\begin{figure}\centering
\begin{tikzpicture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%vertex
\tikzstyle{vertex}=[circle,fill=black!40,minimum size=30pt,inner sep=0pt,draw=black,thick]
\tikzstyle{avertex}=[rectangle,fill=red! 60,minimum size=25pt,inner sep=0pt,draw=black,thick]
\definecolor{darkgreen}{rgb}{0.3,0.8,0.5}
\tikzstyle{overtex}=[circle,fill=blue!50,minimum size=30pt,inner sep=0pt,draw=black,thick]
%nodes
\node[vertex] (state1) at (0,1.8) {$S_t$};
\node[vertex] (state2) at (7,1.8) {$S_{t+1}$};
\node[overtex] (obs) at (13,0) {$O_{t+1}$};
\node[avertex] (action) at (2.5,0) {$a_t$};
%%bels
\node (bel1) at (1,2.5) {$b_t$};
\node (bel2) at (8.5,2.5) {$b_{t+1}$};
%probas
\node (trans) at (4.5,1.5) {$\textbf{p} \paren{ s_{t+1} \sachant s_t, a_t }$};
\node (observ) at (9.6,0.3) {$\textbf{p} \paren{ o_{t+1} \sachant s_{t+1}, a_t }$};
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ARROWS
\draw[->,>=latex, thick] (state1) -- (state2);
\draw[->,>=latex, thick] (state2) -- (obs);
\draw[->,>=latex, thick] (action) -- (state2);
\draw[->,>=latex, thick] (action) -- (obs);
\end{tikzpicture}
\caption[Réseau Bayesien illustrant la mise à jour de l'état de croyance]{Réseau Bayesien illustrant la mise à jour de l'état de croyance: 
les états sont représentés par des ronds gris, 
l'action est représentées par le losange rouge,
et l'observation par le rond bleu.
La variable aléatoire $S_{t+1}$
représentant l'état suivant $s_{t+1}$ 
dépend de l'état courant $s_t$ 
et de l'action courante $a_t$.
La variable aléatoire $O_{t+1}$ 
réprésentant l'observation suivante $o_{t+1}$ 
dépend de l'état suivant $s_{t+1}$
et de l'action courante $a_t$.
L'état de croyance $b_{t}$ (resp. $b_{t+1}$)
est l'estimation probabiliste du l'état courant (resp. suivant) du système, 
$s_t$ (resp. $s_{t+1}$).}
\label{BayesNetPOMDP}
\end{figure}%

Puisque les états de croyance successifs sont calculés avec les observations perçues par l'agent,
ils sont considérés visible par l'agent. 
%Moreover, it can be easily shown that
%the expected total reward can be rewritten 
%\begin{equation}
%\label{probCriterion}
%\mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,d_t) } = \mathbb{E} \croch{ \sum_{t=0}^{+ \infty} \gamma^t r(b_t,d_t) }, 
%\end{equation}
%defining $r(b_t,a) = \sum_s r(s,a) \cdot b_t(s)$ as the reward of belief $b_t$.
Notons $\mathbb{P}^{\mathcal{S}}$ 
l'ensemble continu des distributions de probabilité
sur $\mathcal{S}$.
Une stratégie optimale peut être cherchée parmi
les fonction $d$ définies sur $\mathbb{P}^{\mathcal{S}}$ 
telles que les $d_t = d(b_t) \in \mathcal{A}$ successifs 
maximisent l'espérance des récompenses (\ref{criterion}):
les décisions de l'agent sont basées sur l'état de croyance.

%
% POMDP ROBOTICS
%
Les PDMPO fournissent un cadre flexible pour la robotique autonome,
comme illustré par l'exemple du robot pompier, \textit{cf.} figure \ref{robot_pompier}:
ils permettent de décrire le système regroupant le robot et son environnement,
ainsi que la mission du robot.
Ils sont fréquemment utilisés en robotique
\cite{PineauG05,OngShaoHsuWee-IJRR10,Marthi12,ChanelTL12,ChanelTL13}.
En effet, ils prennent en compte le fait que le robot ne reçoit que les données des capteurs,
et doit estimer l'état du système, qui lui est caché,
en utilisant ces données, appelées alors observations,
afin de réaliser sa mission.
Cependant, le modèle PDMPO soulève quelques problèmes,
en particulier dans le contexte robotique.

%
% HIGH COMPLEXITY
%
\section*{Problèmes pratiques des PDMPO}
\subsection*{Complexité}
Résoudre un PDMPO \textit{i.e.} 
calculer une stratégie optimale,
est PSPACE-hard en horizon fini \cite{Papadimitriou1987} 
et même indécidable en horizon infini \cite{Madani1999UPP315149.315395}.
De plus, un espace exponentiel en la description du problème peut être requis
pour une spécification explicite d'une telle stratégie.
Le travail \cite{Mundhenk2000CPP867838} 
est un bon résumé des l'analyses de complexité
des PDMPO.

Cette forte complexité
est très bien connue des utilisateurs des PDMPO:
l'optimalité ne peut être atteinte que pour des petits problèmes,
ou bien des problèmes très structurés.
Les approches classiques essaient de résoudre ce problème
en utilisant la programmation dynamique
et des techniques de programmation linéaire
\cite{Cassandra97incrementalpruning}.
Sinon, seules des solutions approchées peuvent être calculées,
et donc la stratégie n'a pas de garantie d'optimalité.
Par exemple, les approches populaire
telles que les méthodes basées sur les points,
\cite{Pineau_2003_4826,Kurniawati-RSS08,Smith2004HSV1036843.1036906}, 
celles basées sur des grilles \cite{Geffner98solvinglarge,Brafman97aheuristic,Bonet_newgrid-based}
ou bien les approches de Monte Carlo approaches \cite{NIPS2010_4031},
utilisent des calculs approchés.

%
% VISION IN ROBOTICS
%
\begin{figure} \centering
\includegraphics[scale=0.75]{fig2}
\caption[Exemple d'une méthode d'observation dans le contexte robotique]{
Exemple d'une méthode d'observation dans le contexte robotique:
le robot, ici un drone, est équippé d'une caméra
et utilise un classifieur calculé à partir d'une base de données d'images
(comme NORB, \textit{cf.} figure \ref{NORB}).
Le classifieur est généré avant la mission (off-line)
avec une base de données d'images
(\textit{cf.} partie droite de l'illustration), 
et la sortie du classifieur 
est utilisée lors de la mission (online) 
comme une observation pour l'agent (\textit{cf.} la partie gauche).
Ici, les observations sont donc générées par un algorithme de vision artificielle.}
\label{observation_robot}
\end{figure}
%
% VISION IN ROBOTICS END
%


%
% COMPUTER VISION
%
\subsection*{Imprécision des Paramètres et Vision Artificielle}
Considérons maintenant des robots utilisant la perception visuelle,
et dont les observation proviennent d'algorithmes de vision
basé sur de l'apprentissage statistique.
(\textit{cf.} figure \ref{observation_robot}).
Dans cette situation, le robot utilise un \textit{classifieur}
pour reconnaître les objects dans les images:
le classifieur est censé renvoyer le nom de l'objet
qui se trouve dans l'image,
et fait quelquefois des erreurs avec une faible probabilité.
(\textit{i.e.} matrice de confusion de la figure \ref{confusion_matrix}).

Le classifieur est calculé en utilisant une \textit{base de donnée d'entraînement} 
(comme NORB, \textit{cf.} figure \ref{NORB}, 
mis en ligne par les auteurs à l'adresse \url{http://www.cs.nyu.edu/~ylclab/data/norb-v1.0/}).
Un algorithme utilisant des descentes de gradient 
utilisé pour calculer le classifieur
en utilisant la base de données d'images
est appelé Réseau Convolutionnel \cite{Lecun98gradient-basedlearning}.
Les figures (\ref{NORB}) et (\ref{confusion_matrix})
illustrent l'exemple d'un classifieur calculé pour une mission dronique
dans laquelle les caractéristiques d'intérêt
(les état du système)
sont liées à la présence (ou l'abscence)
d'animaux, voitures, humains, avions ou camions: 
le problème statistique du calcul d'un classifieur 
permettant de reconnaitre de tels objets dans les images
est appelé \textit{classification multi-classes}.

%%%
%%% NORB DATA SET
%%%
\newcounter{moncompteur} %define counter
\begin{figure} \centering
\begin{tikzpicture}
\node (notes) at (2.5cm,7cm) { 
base de données d'images NORB: \color{blue} $(\mbox{image}_i,\mbox{étiquette}_i)_{i=1}^N$
};

\def\names{{"bla","human","car","truck","truck","nothing","nothing","nothing","truck"}}%
\def\namess{{"bla","airplane","car","human","animal","car","human","animal","animal"}}%

\setcounter{moncompteur}{1}
	\coordinate (norb11) at (6.2cm,3.05cm); 
	\coordinate (norb12) at (14cm,3.05cm); 
	\coordinate (norb11lab) at (6.2cm,6.2cm); 
	\coordinate (norb12lab) at (14cm,6.2cm); 
	\coordinate (norb11lab2) at (6.2cm,1.9cm); 
	\coordinate (norb12lab2) at (14cm,1.9cm); 
	\foreach \y in {0.25,0.5,...,2} {
		\coordinate (weight) at (barycentric cs:norb11= \y,norb12=1-\y);
		\def\reptemp{photos/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics[scale=0.5]{\reptemp}};
		
		\coordinate (weight2) at (barycentric cs:norb11lab= \y,norb12lab=1-\y);
		\node[scale=0.7] (leslabels) at (weight2) {\pgfmathparse{\names[\themoncompteur]}\pgfmathresult};
		\coordinate (weight3) at (barycentric cs:norb11lab2= \y,norb12lab2=1-\y);
		\node[scale=0.7] (leslabels) at (weight3) {\pgfmathparse{\namess[\themoncompteur]}\pgfmathresult};

		\addtocounter{moncompteur}{1}
	}

	\coordinate (norb21) at (6.2cm,5cm); 
	\coordinate (norb22) at (14cm,5cm); 
	\foreach \y in {0.25,0.5,...,2} {
		\coordinate (weight) at (barycentric cs:norb21= \y,norb22=1-\y);
		\def\reptemp{photos/} %define string
		\appto\reptemp{\themoncompteur} %access the string of a counter
		\appto\reptemp{.png} % concatenate two trings
		\node (image) at (weight) {\includegraphics[scale=0.5]{\reptemp}};
		\addtocounter{moncompteur}{1}
	}

\end{tikzpicture}
\caption[Exemple de base de donnée pour la vision artificielle]{Exemple de base de donnée pour la vision artificielle: 
la base de données d'images étiquetées NORB \cite{LeCun.2004},
destinée à l'apprentissage statistique.
La taille de NORB est supérieure à $3.10^5$,
et les images de cette base de données représentent des objets de ces $5$ classes:
``animal'', ``car'', ``human'', ``nothing'', ``plane'' et ``truck''.
Chaque élément d'une base de donnée d'images est composé d'une image (par exemple une image montrant une voiture)
et une étiquette correspondant à la classe de l'objet représenté par l'image 
(dans l'exemple précédent, l'étiquette est ``car'').
Cette base de données peut être utilisée afin de calculer un classifieur par apprentissage supervisé.
Dans le but de discerner les positions des cibles,
une image est étiquetée avec le nom de l'objet qui est au centre
(``nothing'' si il n'y a rien au centre de l'image).}
\label{NORB}
\end{figure}
%%%%%
%%%%% NORB DATASET END
%%%%%

%
% IMPRECISION OBSERVATION
%
Puisque le classifieur est appris à partir d'une base de données d'images,
son comportement, et donc ses performances,
(\textit{i.e.} sa fréquence de bonne prédictions) 
est inévitablement dépendent de la base de données.
C'est un problème si la variabilité de la base de donnée
est trop faible:
dans ce cas, le comportement probabiliste du classifieur
sera dépendent de ces images en particulier
et le système robotique aura des mauvaises capacités d'observations
lorsque la mission implique des images trop différentes de celles présentes
dans la base de données.

Certaines bases de données à grande variabilité existent
(par exemple NORB, figure \ref{NORB}, bien que la variabilité pourrait être idéalement supérieure):
notons cependant qu'avec ces bases de données,
la performace de vision est réduite,
ou bien, au moins, de bonne performances
sont difficilement atteignables

Une matrice de confusion peut être calculée (\textit{cf.} figure \ref{confusion_matrix})
en utilisant une telle base de données étiquetées,
qui n'est pas utilisée pour l'entraînement,
et qui est appelée \textit{base de données de test}:
la fréquence des observations peut être déduit de cette matrice,
en normalisant les lignes en distributions de probabilité.
Une ligne correspond à un objet de la scène
et les probabilités de cette ligne sont les probabilité d'observation,
\textit{i.e.} chaque valeur de probabilité est la fréquence avec laquelle
le classifieur renvoie le nome de l'objet de la colonne correspondante.
Ces probabilités peuvent être utilisées pour définir la fonction d'observation
$\textbf{p} \paren{o' \sachant s',a}$ introduite au-dessus.
Cette approche soulève le problème de la représentativité
de la base de données pour la mission voulue.
Si la base de donnée de test n'est pas représentative,
ces probabilités d'observation risque de ne pas être fiables,
et le PDMPO mal défini:
cependant, comme montré par l'équation (\ref{probBayesRule})
la mise à jour de l'état de croyance nécessite la connaissance parfaite
de la fonction d'observation.

Finalement, si les bases de données considérée sont étiquetées plus précisément,
(comme NORB, qui inclut des informations telles que la luminosité, ou l'échelle de l'objet),
nous pouvons imaginer que les probabilités d'observation calculées (à partir de la matrice de confusion)
serait plus fiable, ou la performance de vision améliorée
(puisque la séparation demandée au classifieur 
est plus simple avec cette précision).
Cependant, comme plus d'observations ou d'états sont impliquées,
et le POMDP est plus dur à résoudre.

En guise de conclusion, l'utilisation du modèle PDMPO fait l'hypothèse
que les distributions de probabilité régissant le problème doivent être toutes connues parfaitement:
malheureusement ces fréquences ne sont pas connue précisément en pratique.
L'imprécision à propos de ces probabilités,
par exemple l'imprécision associée au comportement des sorties
des algorithmes de vision artificielle,
lorsque les images utilisées sont celles de la mission d'intérêt,
doit être prise en compte pour rendre le robot autonome en toute circonstances.
En général, le calcul des distributions de probabilité d'un PDMPO nécessite
assez de tests pour chaque paire état-action, ce qui est dur à effectuer en pratique.

\begin{figure}[b!] \centering
\begin{tabular}{c|c|c|c|c|c!{\vrule width 2pt}c!{\vrule width 2pt}c}%!{\vrule width 2pt}}
animal & human & plane & truck & car & nothing \\ \specialrule{.2em}{.0em}{.0em}  
$3688$ & $575$ & $256$ & $48$ & $144$ & $149$ &   animal & $75.885\%$ \\ \specialrule{.05em}{.0em}{.0em}  
$97$ & $4180$ & $81$ & $20$ & $225$ & $257$ & human & $86.008\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$292$ & $136$ & $3906$ & $237$ & $202$ & $87$ & plane & $80.370\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$95$ & $1$ & $44$ & $4073$ & $514$ & $133$ & truck & $83.807\%$  \\  \specialrule{.05em}{.0em}{.0em}  
$129$ & $3$ & $130$ & $1283$ & $3283$ & $32$ &  car & $67.551\%$ \\  \specialrule{.05em}{.0em}{.0em}  
$154$ & $283$ & $36$ & $63$ & $61$ & $4263$ & nothing & $87.716\%$  \\ %\specialrule{.2em}{.0em}{.0em}  
\end{tabular}
\caption[Exemple de matrice de confusion illustrant les performances d'un classifieur multi-classes]{
Exemple d'une matrice de confusion pour la classification multi-classe:
cette matrice est calculée avec une base de donnée de tests,
différente de la base de données d'apprentissage.
Chaqye ligne ne considère que les images d'un certain objet,
et les nombres représentent les réponses du classifieur:
par exemple, $3688$ images d'animaux sont bien reconnus, 
mais $575$ sont confondus avec un humain.
La moyenne de réponses correctes pour cet objet et de $80.223\%$.
L'environnement Torch7 \cite{Collobert_NIPSWORKSHOP_2011}
a été utilisé pour obtenir un classifieur, 
et pour calculer cette matrice à partir de ce dernier
et de la base de donnée de test.}
\label{confusion_matrix}
\end{figure}


%
% IMPRECISION POMDP WORKS
%
Quelques variations du cadre PDMPO
a été constuit dans le but de prendre en compte
l'imprécision sur les distributions de probabilité du modèle,
aussi appelé \textit{l'imprécision des paramètres}.
\subsubsection{Travaux Tenant Compte de l'Imprécision des Paramètres} 
Ici, les fonctions de transition et d'observation,
\textit{i.e.} $\textbf{p} \paren{ s' \sachant s,a }$ 
et $\textbf{p} \paren{ o' \sachant s',a }$,
$\forall (s,s',o',a) \in \mathcal{S}^2 \times \mathcal{O} \times \mathcal{A}$,
sont appelés \textit{paramètres} du PDMPO, 
ou aussi les \textit{paramètres} du modèle. 
A notre connaissance,
le premier modèle construit dans le but de gérer l'imprécision
des paramètres est nommé PDMPOPI, 
pour \textit{PDMPD à paramètres imprécis} \cite{Itoh2007453}. 
Dans ce travail, chacun des paramètres du PDMPO
est remplacé par un ensemble de paramètres possibles.
Dans ce travail, une \textit{croyance du second ordre}
est introduite: elle est définie comme étant
la distribution de probabilité sur les paramètres du modèles.

Un autre travail, appelé \textit{PDMPO à Paramètres Bornés} 
(PDMPOPB) \cite{NiYaLiaZhi},
traite aussi de l'imprécision des paramètres:
dans ce travail, l'imprécision sur chaque paramètre
est décrit à l'aide d'une borde supérieure et inférieure
sur les distributions possibles.
Aucune croyance du second ordre n'est introduite ici.
Cependant, résoudre un PDMPOPB est similaire, dans l'esprit, 
à la résolution des PDMPOPI \cite{Itoh2007453}:
la flexibilité amenée par l'imprécision de paramètres
est utilisée pour rendre les calculs les plus faciles possible,
et le critère utilisé n'est pas explicite.
Le problème majeur de ces approches (PDMPOPI et PDMPOPB)
est que l'imprécision des paramètres n'est pas géré dans un but de robustesse,
comme une approche pessimiste (pire cas),
mais dans un but de simplification. 

Un travail plus récent traite le problème de manière pessimiste 
et est donc appelé \textit{PDMPO robuste} \cite{Osogami15}.
Inspiré par le travail correspondant dans le cas complètement observable
(appelé \textit{PDM uncertain} \cite{NE05}),
ce travail utilise le critère dit \textit{maximin},
ou du \textit{pire cas},
qui vient de la \textit{Théorie des jeux}: 
dans ce cadre, une stratégie optimale
maximise le plus petit critère
parmi ceux induits par chaque paramètre possible.
Si l'imprécision des paramètres n'est pas stationnaire,
\textit{i.e.} peut changer à chaque étape de temps,
la stratégie optimale associée (au sens du maximin)
peut être facilement calculé en utilisant la
\textit{Programmation Dynamique} \cite{bellman54}.
Cependant, lorsque l'imprécision des paramètres est stationnaire,
les choses se compliquent:
les calculs proposés mènent à une approximation de la stratégie optimale,
puisque le critère utilisé est une borne inférieur
du critère désiré.
Pourtant, une hypothèse stationnaire pour l'imprécision
des paramètres semble mieux adaptée lorsque le PDMPO est stationnaire.

Bien que l'utilisation d'ensembles de distributions de probabilité 
rend le modèle plus adapté à la réalité du problème
(les paramètres sont imprécis en pratique),
les prendre en compte augmente violemment 
la complexité du calcul d'une politique optimale
(par exemple, lors de l'utilisation du critère maximin).
Comme expliqué précédemment,
résoudre un PDMPO est déjà une tâche très ardue,
donc l'utilisation d'un cadre menant à des calculs plus complexes
ne semble pas être une approche satisfaisante.
%and optimization may require linear programs 
%As discussed during this thesis 
%(for instance see Section \ref{section_expe_PPUDD} of Chapter \ref{chap_symb}),
En effet, modéliser le problème de manière trop fine
mène à l'utilisation de nombreuse approximations
dans les calculs en pratique,
sans réel contrôle ou estimation de ces approximations
comme dans le cas des PDMPOPI et des PDMPOPB.
Il est peut-être plus judicieux de commencer avec un modèle plus simple
qui peut être résolu plus facilement en pratique.

Un autre problème pratique du modèle PDMPO
peut aussi être mentionné: 
cela concerne la définition de l'état de croyance
durant les premières étapes du processus,
et plus généralement, la manière avec laquelle
la connaissance de l'agent est représentée.
%%
%%  FULL IGNORANCE/ KNOWLEDGE OF THE AGENT
%%
\subsection*{Modéliser l'Ignorance de l'Agent}
L'état de croyance initial $b_0$, 
ou distribution de probabilité \textit{a priori} 
sur l'état du système, prends part dans la définition
du PDMPO. 
\'Etant donné un état du système $s \in \mathcal{S}$, 
$b_0(s)$ est la fréquence de l'évènement ``l'état initial est $s$''.
Cette quantité peut être dure à calculer rigoureusement,
surtout lorsque le nombre d'expériences passées est limité: 
cette raison a déjà été invoquée au-dessus, 
menant à l'imprécision des fonctions de transition et d'observation.

Considérons l'exemple d'un robot qui est pour la première fois dans une salle
dont la position de la sortie est inconnue 
(état de croyance initial)
et qui doit trouver la sortie et l'atteindre.
En pratique, aucune expérience ne peut être
répétée dans le but d'extraire une fréquence de position
pour cette sortie.
Dans ce genre de situation, l'incertitude n'est pas due
à un événement aléatoire, mais à un manque de connaissance:
aucun état de croyance initial fréquentiste ne peut être utilisé pour définir le modèle.

L'agent peut aussi croire fortement
que la sortie est positionnée dans un mur,
comme dans la plupart des salles,
mais il attribue quand-même une très petite probabilité $p_{\epsilon}$ 
au fait que la sortie peut être un escalier au plein milieu de la salle.
Même si il est très peu probable que ce soit le cas,
cette seconde option doit être prise en compte
dans l'état de croyance,
sinon la règle de Bayes (\textit{cf.} équation \ref{probBayesRule}) 
ne peut pas être le mettre à jour correctement
si la sortie est vraiment au centre de la pièce.
Quantifier $p_{\epsilon}$ sans expérience passée
n'est pas une chose facile du tout,
et ne se repose sur aucune justification rationnelle,
mais peut impacter fortement la stratégie de l'agent.

L'état initial du système peut être délibérément
défini comme étant inconnu, avec strictement aucune
information probabiliste:
considérons une mission robotique
pour laquelle une partie de l'état du système,
décrivant un fait que le robot est censé
découvrir par lui-même, est
initialement complètement inconnu.
Dans un contexte d'exploration robotique,
la position ou la nature d'une cible,
ou encore la position initiale du robot,
peut être défini comme étant absent de la connaissance de l'agent.
Les approches classiques initialisent l'état de croyance
par une distribution de probabilité uniforme.
(\textit{i.e.} sur toutes les position possible du robot/cible, 
ou sur toutes les natures de cible possibles), 
mais cette réponse provient de l'interprétation subjective
des probabilités \cite{de1974theory,Dubois96representingpartial}.
En effet, les probabilités sont les même
puisque aucun événement n'est plus plausible qu'un autre:
cela correspond à des paris égaux.
Cependant, les mises à jour de l'état de croyance (\textit{cf.} équation \ref{probBayesRule}) 
mène fatalement à un mélange de probabilités fréquentistes, \textit{i.e.}
les fonction de transition et observation,
avec cet état de croyance initial qui est une distribution de probabilité subjective:
cela n'a pas toujours de sens, et dans tous les cas cette approche est douteuse.
Ainsi, l'utilisation des PDMPO dans ces contextes fait face à la difficulté de représenter l'ignorance de l'agent.


%it may make the  actually the real  the POMDP is in practice, a belief state may have an high entropy
%not because of the imprecision of the parameters, 
%may be not related to the agent's lack of knowledge in the probabilistic about the ,
%but rather to the 
%(fully known) 
%variance of the system state:
%hence the agent does not know 
%which is the actual system state 
%since its  is far from being deterministic,
%but it may perfectly know how the state behaves,
%and then nothing can be improved concerning its knowledge.

%and knowledge of the agent CARO \\

%
% WHAT WE WOULD LIKE TO DO
%
\section*{Problème Général}
Les sections précédentes ont présenté certain problèmes rencontrés en pratique
lors de l'utilisation des PDMPO pour calculer des stratégies,
en particulier dans le cadre robotique.
La très grande complexité
du calcul d'une stratégie optimale
est un premier problème:
les missions robotiques 
sont souvent des problèmes à grandes dimensions,
dont le calcul d'une stratégie suffisamment proche de l'optimal
est impossible, 
du fait d'un trop grand temps de calcul
ou d'un manque de mémoire.
Ensuite, il a été mis en évidence
que les distributions de probabilité
définissant le PDMPO 
ne sont pas toujours connues précisément:
par exemple, la fonction d'observation
peut être difficile à définir 
lorsque les observations proviennent
d'un algorithme de vision artificielle complexe.
Enfin, le problème de la gestion de la connaissance et de l'ignorance de l'agent
a été discuté: il n'y a pas de réponse formelle
concernant la manière de représenter 
le manque de connaissance initial de l'agent
à propos du monde dans lequel il évolue.
La difficulté vient du fait que
les PDMPO classiques (probabilistes)
autorisent uniquement l'usage de distribution de probabilité fréquentiste,
alors qu'un autre outil mathématique semble nécessaire.

Ces problèmes forment le point de départ de ce travail.
En effet, ce dernier consiste à contribuer
au problème du calcul d'une stratégie
pour des domaines partiellement observables.
Les stratégies calculées
doivent permettre au robot
de remplir sa mission
aussi bien que possible,
dès la première exécution
\textit{i.e.} le calcul de stratégie 
est opéré avant toute réelle exécution
de la mission. 

Le chalenge général guidant ce travail
est de procéder aux calculs de stratégies
en utilisant seulement les données et les connaissances
vraiment disponibles en pratique,
au lieu d'utiliser les PDMPO classiques,
très complexes et difficiles à définir.
En d'autre mots, cela revient à prêter attention
aux problèmes soulevés au-dessus:
la complexité de calcul, l'imprécision du modèle,
et la gestion de la méconnaissance de l'agent. 
Comme expliqué par la suite, la théorie des possibilités qualitatives \cite{Dubois95possibilitytheory}
semble répondre aux problèmes soulevés.

\section*{Une Théorie Qualitative}
Cette théorie est généralement
introduite en définissant une
échelle qualitative $\mathcal{L}$,
qui peut être définie par 
$\set{0,\frac{1}{k}, \frac{2}{k}, \ldots,1 }$, avec $k>1$,
ou tout autre ensemble fini totalement ordonné.
Les valeurs de cette échelle
ne sont pas importantes 
car elle servent seulement 
à matérialiser un ordre:
nous utilisons donc
le terme de \textit{degré de possibilité}
afin d'expliciter cette remarque.
Le plus petit élément de l'échelle qualitative est noté $0$,
et le plus grand, $1$.
La section suivante clarifie pourquoi
l'utilisation de ce cadre qualitatif 
est bénéfique en terme de complexité
et de modélisation.

Notons tout d'abord les similarités entre
la théorie des probabilités et des possibilités:
une distribution de possibilité sur $\mathcal{S}$ 
est une fonction $\pi: \mathcal{S} \rightarrow \mathcal{L}$
telle que $\max_{s \in \mathcal{S}} \pi(s) = 1$.
De plus, l'opérateur $\min$ est utilisé pour calculer 
une distribution de possibilité jointe 
$\pi(s,o)$, $\forall (s,o) \in \mathcal{S} \times \mathcal{O}$
à partir d'une distribution marginale $\pi(s)$, $\forall s \in \mathcal{S}$
et d'une distribution conditionnelle $\pi \paren{ o \sachant s }$, $\forall (o,s) \in \mathcal{S} \times \mathcal{O}$:
$\pi(s,o) = \min \set{ \pi (s), \pi \paren{ o \sachant s }  }$.
Ainsi, la théorie des possibilités peut donc s'apprivoiser 
en remplaçant l'opérateur $+$ de la théorie des probabilités
par l'opérateur $\max$, et l'opérateur $\times$ par $\min$.

%%% PIPOMDP
\subsection*{PDMPO Qualitatifs Possibilistes}
Une alternative possibiliste qualitative du modèle PDMPO 
a été proposé dans \cite{Sabbadin1999pipomdp}:
ce modèle s'appelle PDMPO qualitatif possibiliste,
et est noté $\pi$-PDMPO. 
Un $\pi$-PDMPO est simplement un PDMPO
avec des distributions possibilistes qualitatives comme paramètres,
au lieu de distributions de probabilités.
Comme les $\pi$-PDMPO sont qualitatifs,
l'homogue de la fonction de récompense,
appelée \textit{fonction de préférence},
est aussi qualitative:
en effet, la fonction de préférence 
est à valeurs dans l'échelle possibiliste qualitative finie $\mathcal{L}$,
et donc n'est pas additive.

%%%% COMPLEXITY
L'une des propriétés les plus intéressantes
des $\pi$-PDMPO est la simplification du calcul de la stratégie.
En effet, les algorithmes proposés
pour résoudre les PDMPO probabilistes 
sont souvent basés sur l'ensemble des états de croyance
appelé \textit{espace des croyances}.
L'espace des croyances est infini dans le cas général:
chaque étape de temps mène à une nombre fini d'états de croyance suivants,
donc cet espace est dénombrable.
Dans le but d'obtenir des propriétés utiles
et des moyens de calculer un stratégie,
l'ensemble de toutes les distributions de probabilité
sur l'espace d'état $\mathcal{S}$ est souvent considéré, 
\textit{i.e.} le simplex continu
$\mathbb{P}^{\mathcal{S}} 
= \set{ \textbf{p}:\mathcal{S} \rightarrow [0,1] \sachant \sum_{s \in \mathcal{S}} \textbf{p}(s) 
= 1, \mbox{ and } \textbf{p}(s)\geqslant 0, \forall s \in \mathcal{S} }$.
La taille infinie de l'espace des croyances
explique en partie
pourquoi les PDMPO probabilistes
sont vraiment difficiles à résoudre.
Au contraire, les $\pi$-PDMPO ont un espace de croyances fini.
En effet, le nombre de distributions de possibilités qualitatives
sur les états du système $\mathcal{S}$
est inférieur à $\mathcal{L}^{\# \mathcal{L}}$,
puisque l'échelle qualitative $\mathcal{L}$ est finie.
La version complètement observable
d'un $\pi$-PDMPO est appelé $\pi$-PDM:
comme expliqué dans la section \ref{section_piPOMDP} du chapitre \ref{chap_SOTA}, 
tout $\pi$-PDMPO se réduit à un $\pi$-MDP 
dont l'espace d'état est l'ensemble des états de croyance possibilistes qualitatifs,
et dont la taille est exponentielle en fonction du nombre d'états du système.
Dans les travaux \cite{abs-1202-3718,Garcia20081018,Sabbadin1999pipomdp}, 
il est montré que
la complexité d'un $\pi$-PDM
est plus faible que la complexité d'un MDP probabiliste, 
qui est polynomiale \cite{Papadimitriou1987}:
la complexité d'un $\pi$-PDMPO est akirs
au pire exponentiel en la description du problème,
tandis qu'un PDMPO probabiliste peut être indécidable \cite{Madani1999UPP315149.315395}.
%The approach of using a $\pi$-POMDP 
%instead of a probabilistic POMDP
%in order to simplify the resolution, 
%can be compared to 


%%% IMPRECISION MODELISATION
En plus de la simplification des calculs,
le $\pi$-PDMPO peut être vraiment intéressant
pour nos problèmes de modélisation. 
En effet, dans le cas d'un robot 
utilisant un algorithme de vision artificielle
(\textit{cf.} figure \ref{observation_robot}),
nous avons précédemment mis en évidence la difficulté
pour définir rigoureusement 
les fonction d'observation probabilistes:
les probabilités 
des réponses
des algorithmes de vision
dans le contexte d'une mission robotique
sont mal connus et difficiles à définir en pratique.
Trouver des estimations qualitatives
de ses performances de reconnaissance
est plus facile:
le modèle $\pi$-PDMPO
ne requiert que des données qualitatives, 
donc il permet de construire un modèle
sans l'utilisation d'informations 
différentes 
de celles vraiment disponibles.
Par exemple, la matrice de confusion 
de la figure \ref{confusion_matrix}
peut mener à une fonction d'observation
qualitative possibiliste
qui ne tient compte uniquement 
de la manière dont les fréquences
de réponses sont classées:
en présence d'un humain (\textit{i.e.} deuxième ligne),
la réponse la plus fréquente est ``human'',
la deuxième réponse est ``nothing'',
la troisième est ``car'', etc.
Donc, la distribution de possibilité correspondante
est telle que,
conditionnellement à la présence d'un humain, 
le degré de possibilité de la réponse ``human''
est plus grand que le degré de possibilité de la réponse ``nothing'',
qui est plus grand que le degré de possibilité
de la réponse ``car'', etc.
Au lieu d'attribuer des fréquences
qui ne sont pas vraiment
fiable en pratique,
le modèle possibiliste qualitatif
exprime naturellement ces imprécisions
à propos du problem.

%%%IGNORANCE
Enfin, notons que la distribution de possibilité 
constante, dont les degrés sont tous égaux à $1$
(élément maximal de $\mathcal{L}$),
représente l'ignorance totale:
cette distribution peut être utilisé 
pour définir l'état de croyance initial
lorsqu'il doit représenter
un agent qui ignore initialement
une situation donnée.
Donc, le modèle $\pi$-PDMPO
permet une modélisation formelle du manque de connaissance de l'agent. 

%%CORRESPONDS TO OUR ISSUES 
L'utilisation de la théorie de possibilités qualitatives \cite{DuboisPS01}
est donc étudié dans ce travail,
puisqu'il semble être capable
d'à la fois simplifier un PDMPO,
et de modéliser l'imprécision des paramètres
et l'ignorance associées aux missions robotiques.
Ce cadre, en effet, simplifie les calculs,
est capable de représenter le problème avec seulement les données disponibles,
et modélise le manque de connaissance:
donc cette théorie offre ses solutions aux
trois problèmes mis en évidence précédemment.
Cependent, notons qu'un cadre qualitatif
ne permet pas de manipuler de l'information fréquentiste.

%%%NOT WIDELY STUDIED
A notre connaissance,
une étude plutôt limitée du modèle $\pi$-PDMPO existe dans la littérature jusqu'à aujourd'hui:
en fait, le travail \cite{Sabbadin1999pipomdp}
semble être le seul, proposant
à la fois une définition des $\pi$-PDMPO 
et un exemple jouet pour illustrer ce modèle.
La version complètement observable ($\pi$-PDM)
a généré plus de travaux \cite{Sabbadin2001287,Sabbadi00,LIP61498}.

\section*{Description de notre étude}
% Le sujet de la thèse
Cette thèse contribue à déterminer
dans quelle mesure la thérie de possibilités qualitatives
peut contribuer à la \textit{planification dans l'incertain dans des domaines partiellement observables}, 
et plus généralement à la gestion séquentielle de l'incertitude, 
en termes de simplification des calculs et de modélisation. 
Elle présente de récentes contributions
dans l'utilisation de cette théorie pour la planification dans l'incertain
et la représentation des connaissances,
avec une utilisation quasi systématique des modèles graphiques 
\cite{Koller2009PGM1795555,Be2002.7,Borgelt02graphicalmodels}.

La fin de cette introduction 
décrit la structure de cette cette thèse.
En effet, chacune des sections suivantes
correspond à un chapitre de notre travaille,
et détaille son contenu.

%%%% SOTA
\subsection*{\'Etat de l'Art}
Les PDMPO qualitatifs et possibilistes 
constituent l'objet central de cette thèse:
le \emph{premier chapitre} est consacré à ce modèle.
Une présentation rapide de la théorie des possibilités qualitatives
est suivie de la présentation du modèle entièrement observable ($\pi$-PDM),
puis du modèle partiellement observable ($\pi$-PDMPO).
Comme noté précédemment, à notre connaissance,
seulement un article de dix pages a déjà traité des $\pi$-POMDPs.

%%% CHAP1
\subsection*{Mises \`a jour naturelles du modèle possibiliste qualitatif}
Le \emph{deuxième chapitre} propose quelques extensions
au travail \cite{Sabbadin1999pipomdp}.
Tout d'abord, est construite une version possibiliste qualitative
des PDM à observabilité mixte \cite{OngShaoHsuWee-IJRR10,AraThoBufCha-ICTAI10}
dans lesquels quelques variables d'état sont complètement observables. 
Elle est appelée $\pi$-PDMMO et généralise à la fois les $\pi$-PDM et les $\pi$-PDMPO.
Cette contribution réduit considérablement 
la complexité de résolution des $\pi$-POMDPs, 
en manipulant de manière plus fine l'information des environnements
dont certaines variables sont complètement observables.
Par exemple, le niveau de batterie d'un robot peut être considéré comme
une information directement observable
pour la prise de décision, menant à des calculs plus simples.
Plus généralement, l'existence de variables visibles est très courant 
en robotique \cite{OngShaoHsuWee-IJRR10}.

Ensuite, un critère qualitatif est proposé
pour pouvoir traiter des missions 
dont la durée n'est pas connue à l'avance:
l'algorithme faisant le calcul de la stratégie optimale 
associée est alors présenté.
Cet algorithme est utilisé pour calculer une stratégie
pour une mission de reconnaissance de cible:
les résultats experimentaux comparent
les exécutions utilisant cette stratégie
à celles utilisant la stratégie d'un algorithme pour PDMPO probabiliste,
dans des situations où la dynamique probabiliste des observations
n'est en fait mal définie.

Notons que ces résultats expérimentaux 
constituent, à notre connaissance, 
la première utilisation du cadre $\pi$-PDMPO.
Ils mettent aussi en évidence que l'état de croyance possibiliste qualitatif
a un comportement intéressant, 
dans certaines situations détaillées par la suite.
Les principale contributions de ce chapitre
ont été publiés dans \cite{Drougard13}.
Il est souligné que les résultats expérimentaux
n'auraient pas pu être effectués
sans les deux premières contributions,
qui permettent de ne pas fixer un horizon arbitraire
et d'alléger les calculs.

Cependant, ces contributions ne sont pas suffisantes
pour atteindre un temps de calcul compétitif,
ou pour pouvoir traiter des problèmes robotiques réels:
l'orientation du second chapitre est dictée par cette remarque.

%%% CHAP2
\subsection*{Modèles factorisés et algorithmes symboliques}

%% INTRO
Les problèmes robotiques traités dans le chapitre précédent
sont assez petits pour permettre à l'algorithme proposé
de calculer une stratégie
en un temps raisonnable.
Les contributions du \emph{troisième chapitre}
permettent la résolution de problèmes structurés de planification
plus importants.

%% PPUDD and factored models
La première partie de ce chapitre
se propose d'introduire
les \textit{$\pi$-PDMOM} factorisés:
il sont définis par des hypothèses d'indépendance supplémentaires.
Les grands problèmes de planification 
satisfaisant ces hypothèses 
peuvent être résolus plus facilement:
inspiré par l'algorithme pour PDM probabilistes, 
SPUDD \cite{Hoey99spuddstochastic},
nous avons conçu un algorithme nommé PPUDD
pour résoudre les $\pi$-PDMOMs factorisés 
en utilisant des \textit{arbres de décision algébriques} (ADD).
L'intuition motivant cette contribution,
est que le calcul entre ADDs 
est moins coûteux en temps et en mémoire
dans le cadre qualitatif possibiliste %%%
que dans le cadre probabiliste:
les opérations qualitatives
devraient mener à des ADDs plus petits
puisque la somme et le produit
sont remplacés par les opérateurs $\min$ et $\max$.
Ces derniers produisent des ADDs
avec potentiellement moins de feuilles,
car ils ne créent pas de nouvelles valeurs.

%% belief factorization
Les hypothèses d'indépendance définissant
le modèle $\pi$-PDMOM factorisé 
concerne les variables 
représentant les croyances successives.
De plus, les variables définissant un
$\pi$-PDMOM sont celles représentant
les états du système et les observations.
C'est pourquoi la section qui suit dans ce chapitre
propose des conditions suffisantes
sur les variables d'état et d'observation
menant aux indépendances désirées
entre les variables de croyances.
Un exemple robotique est utilisé en guise
d'illustration de ces conditions.
Puisque les preuves utilisent le concept
graphique appelé \textit{$d$-Séparation} \cite{pearl88},
ces conditions mènent aussi à l'indépendance des 
variables de croyance dans le cadre des
PDMOM probabilistes.
 
%% first tests, larger robotic missions  
Les performances de notre solver PPUDD
sont ensuite comparées à ceux des homologues probabilistes,
en termes de temps de calcul
et avec quelques critères mesurant 
la réussite de la mission.
Enfin, la dernière partie de ce chapitre
décrit les résultats de PPUDD
lors de la compétition internationale
de planification probabiliste\footnote{\url{https://cs.uwaterloo.ca/~mgrzes/IPPC_2014/}}.
Nous avons participé à la compétition
dans le but de tester les performances de l'algorithme
contre celles des algorithmes probabilistes,
en terme d'espérances de la somme des récompenses,
sur de nombreux problèmes de planification.

Certaines contributions de ce chapitre
ont été publiées dans \cite{DrougardTFD14}.
Les nombreux problèmes de planification de la compétition
mettent aussi en évidence 
quelques problèmes liés à notre algorithme,
lorsqu'il est utilisé pour approximer
le calcul d'une stratégie
pour un problème probabiliste
dans le but de bénéficier de
calculs qualitatifs
qui sont plus simples.
De plus, bien que notre algorithme est meilleur que certains
algorithmes utilisant des ADDs (notamment sont homologue direct, SPUDD),
les algorithmes de la compétition utilisant des recherches dans l'espace d'état
\cite{KellerE12,KolobovMW12} obtiennent de meilleurs résultats.
L'approche proposée dans le dernier chapitre
prend en compte ces problèmes. 

%%% CHAP4
\subsection*{Approche probabiliste et possibiliste: une perspective hybride}
Le dernier chapitre de cette thèse
persiste à montrer les possibles améliorations
du cadre PDMPO, avec l'usage de la théorie de possibilités,
en particulier pour la gestion de l'état de croyance.
Il prend aussi en compte les problèmes soulevés dans le chapitre précédent.

En effet, le \emph{quatrième chapitre}
propose un PDMPO hybride
avec des données probabilistes et possibilistes 
regroupées de manière cohérente.
Une nouvelle manière de traduire un PDMPO en
PDM entièrement observable est décrite ici.
Contrairement à la traduction classique,
l'espace d'état résultant est fini,
permettant aux algorithmes de résolution des PDM
de résoudre cette version simplifiée
du problème partiellement observable initial.
En effet, cette approche représente les états de croyance de l'agent
avec des distributions de possibilité sur les états,
menant à un PDM dont l'espace d'état
est un ensemble fini d'états épistémiques.

Quelques simplifications des calculs
sont enfin décrites pour les \textit{PDMPO factorisés} \cite{Sim2008SHS1620163.1620241,Williams05factoredpartially,
Veiga14aaai,abs-1301-6719}, 
\textit{i.e.} les PDMPO avec des structures d'indépendance particulières. 
Ces dernières contributions ont été publiées dans
\cite{DrougardDFT15}.
