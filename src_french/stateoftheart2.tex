Le principal sujet de cette thèse est le processus décisionnel markovien partiellement observable (PDMPO),
ou plus précisément son homologue possibiliste qualitatif.
L'utilisation pratique du modèle probabiliste a été critiquée en introduction,
cependant il résume de manière appropriée les principales caractéristiques d'un système robotique:
son homologue possibiliste étant très similaire, son étude semble prometteuse.
Tout d'abord, la théorie des possibilités qualitative est présentée dans le but
d'introduire les \textit{processus décisionnels markoviens possibilistes qualitatifs} ($\pi$-PDM)
et enfin les \textit{processus décisionnels markoviens partiellement observables} ($\pi$-PDMPO)
qui constituent le point de départ de notre travail.\\

\section{Théorie des possibilités}
\label{posspres}
Les \textit{ensembles flous} ont été introduits par Lotfi Zadeh \cite{Zadeh1965338},
et étudiés par Didier Dubois \cite{didiersgroundhogday} et Henri Prade:
leur contributions ont mené à la fondation de la théorie des possibilités \cite{DuPr1988.4}.

Comme dans la théorie des probabilités,
cette théorie est basée sur une mesure d'incertitude,
appelée \textit{mesure de possibilité}.
Contrairement à la mesure de probabilité mesure $\mathbb{P}$ 
qui est une mesure classique,
la mesure de possibilité, notée $\Pi$,
est une \textit{mesure floue}, ou \textit{capacité}.
Pour faire simple,
une mesure floue n'est pas supposée additive,
mais juste \textit{monotone},
\textit{i.e.} si $A \subset B$
alors la mesure de $A$ est plus petite que la mesure de $B$.
Dans cette thèse, les mesures de possibilité vont concerner uniquement
des ensembles finis comme l'ensemble des états $\mathcal{S}$
et l'ensemble des observations $\mathcal{O}$.

Formellement, une mesure de possibilité est définie comme suit:
\begin{Def}[Possibility Measure]
\label{poss_measure}
Une mesure de possibilité sur l'ensemble fini $\Omega$ 
est une fonction $2^{\Omega} \rightarrow [0,1]$
telle que
\begin{itemize}
\item $\Pi(\Omega) = 1$ (\textit{normalization});
\item $\forall A,B \subset \Omega$, $\Pi \set{ A \cup B } = \max \big\{ \Pi(A), \Pi(B) \big\}$ (\textit{maxitivité}).
\end{itemize}
\end{Def}

La théorie des probabilité modélise l'incertitude 
due à la variabilité des événements:
en pratique, les probabilités sont les fréquences estimées des événements,
définis comme le vrai modèle de variabilité des événements.
Une autre interprétation de cette théorie est celle des probabilités subjectives 
définies par De Finetti \cite{de1974theory}:
la valeur de probabilité d'un événement 
est un pari échangeable, 
relatif aux connaissances d'une personne donnée.

La théorie des possibilités est dédiée à l'incertitude
due à un manque de connaissance, ou une imprécision à propos d'un événement.
La théorie des possibilités quantitatives est un cas particulier de probabilités imprécises
\textit{i.e.} une mesure de possibilité $\Pi$ représente un ensemble particulier
de mesures de probabilité définies sur $\Omega$.

Contrairement à la théorie des possibilités quantitatives,
la théorie des possibilités qualitatives utilise des mesures de possibilité
dont les valeurs sont définies sur n'importe quelle échelle ordonnée.
Cette théorie nous permet de raisonner, même lors d'un manque d'informations quantitatives:
la seule information donnée par une mesure de possibilité qualitative
est l'ordre de plausibilité entre les événements
\textit{i.e.} pour $A,B \subseteq \Omega$,
l'information ``l'événement $A$ est moins plausible que l'événement $B$'', 
qui est s'écrit $\Pi(A) \leqslant \Pi(B)$.
Ainsi les mesures de possibilité qualitatives $\Pi$
sont souvent définies 
comme des fonctions $2^{\Omega} \rightarrow \mathcal{L}$,
où $\mathcal{L}$ est un ensemble fini 
appelé \textit{échelle possibiliste}
associé à un ordre total. 
Dans ce travail, et plus spécifiquement dans les trois premiers chapitres de cette thèse,
l'échelle possibiliste est définie par $\mathcal{L} = \set{ 0, \frac{1}{k}, \ldots, 1 }$
pour simplifier les notations.

Nous pouvons définir la \textit{distribution de possibilité} 
$\pi$: $\pi(\omega) = \Pi(\set{\omega})$.
D'après la définition \ref{poss_measure},
la mesure de possibilité est entièrement 
définie par la distribution $\pi$.
Pour chaque distribution (ou mesure) de possibilité,
une mesure duale, appelée mesure de nécessité, peut être définie:
le degré de nécessité d'un événement augmente si le degré de possibilité de l'événement contraire décroît.
\vbox{
\begin{Def}[Mesure de nécessité associée à $\Pi$]
\label{DEF_necessity}
La mesure de nécessité associée à $\Pi$ est la mesure floue $\mathcal{N}:2^\Omega \rightarrow \mathcal{L}$ 
telle que $\forall A \subset \Omega$, \[ \mathcal{N}(A)=1-\Pi(\overline{A}), \]
où $\overline{A}$ est l'événement complémentaire $A$ dans $\Omega$. 
\end{Def}
}
\begin{figure} 
\center
\begin{tikzpicture}
\definecolor{ggreen}{rgb}{0.3,0.7,0.4};
\begin{axis}[xtick=\empty,ymin=-0.19]
\foreach \xx in {-2,-1.5,-1,-0.5,0,0.5,1,1.5,2} {
\addplot[black,dotted] coordinates {(\xx,0) (\xx,1)};
}
\addplot[mark=none,blue, ultra thick] coordinates { (-2, 0) (-1, 0) (-1, 0.6) (-0.5, 0.6) (-0.5, 1) (0, 1) (0,0.8) (1,0.8) (1,0.2) (1.5,0.2) (1.5,0) (2,0)  };
\addplot[mark=none,ggreen, very thick, densely dashed] coordinates { (-2, 0) (-1, 0) (-1, 0 )  (-0.5, 0)   (-0.5, 1) (0, 1) (0,0.6) (0.5, 0.6) (0.5,0.4) (1,0.4) (1,0.2) (1.5,0.2) (1.5,0) (2,0)  };
\end{axis}
\node (pi1) at (4.6,4.6) { {\color{blue} $\pi_1$} };
\node (pi1) at (3.9,3.7) { {\color{ggreen} $\pi_2$} };

\node (a) at (0.9,0.4) {$a$};
\node (b) at (1.63,0.45) {$b$};
\node (c) at (2.36,0.4) {$c$};
\node (d) at (3.09,0.45) {$d$};
\node (e) at (3.82,0.4) {$e$};
\node (f) at (4.55,0.45) {$f$};
\node (g) at (5.28,0.4) {$g$};
\node (h) at (6,0.45) {$h$};

\draw[red, thick] (2.85,0.7) -- (4.8,0.7) -- (4.8,0.2) -- (2.85,0.2) -- (2.85,0.7);
\draw[<->,>=latex,red, thick] (2.4,3.5) -- (2.4,5.2);
\draw[<->,>=latex,orange, thick, dashed] (5.1,1.75) -- (5.1,5.2);

\node (nec) at (1.2,4.5) {{\color{red} $\mathcal{N}_1(\set{ d, e, f })$}};
\node (nec) at (6,3.5) [rotate=300] {{\color{orange} $\mathcal{N}_2(\set{ d, e, f })$}};

\end{tikzpicture}
\caption[Distribution de possibilité, mesure de necessité et spécificité]{
Exemple de deux distributions de possibilité sur $\Omega=\set{a,b,c,d,e,f,g,h}$:
$\pi_1$ (ligne bleue continue) et $\pi_2$ (ligne verte pointillée), 
avec $\pi_2$ qui est plus spécifique que $\pi_1$.
La mesure de nécessité $\mathcal{N}_1$ 
associée à $\pi_1$
est évaluée sur l'événement $\set{d,e,f} \subset \Omega$: 
le degré de nécessité est égal à $0.4 = 1-0.6$, 
comme illustré par les flèches rouges continues.
La mesure de nécessité $\mathcal{N}_2$
associée à $\pi_2$ 
est évaluée sur le même événement:
le degré de nécessité est égal à $0.8 = 1 - 0.2$, 
comme illustré
par les flèches orange en pointillées. }  
\label{figure_necspec}
\end{figure}

L'ignorance totale est modélisée par une distribution de possibilité $\pi$ 
telle que $\forall \omega \in \Omega$,
$\pi(\omega)=1$ \textit{i.e.} 
n'importe quel événement élémentaire est possible.
Dans ce cas, 
$\forall A \subseteq \Omega$, 
$A \neq \Omega$, 
$\mathcal{N}(A)=1-\Pi(\overline{A})=1-\max_{\omega \in \overline{A}} \Pi(\omega)=0$:
contrairement à l'univers $\Omega$,
aucun événement n'est nécessaire. 

Au contraire, la connaissance parfaite que l'événement élémentaire $\omega_A \in \Omega$
est modélisé par un distribution de possibilité $\pi$
telle que $\pi(\omega_A) = 1$
et $\pi(\omega)=0$, $\forall \omega \neq \omega_A$. 
La nécessité du singleton $\set{\omega_A}$
est aussi égal à un: $\mathcal{N}(\set{\omega_A})=1-\Pi(\overline{\set{\omega_A }}) = 1$. 
L'événement élémentaire $\omega_A$
est nécessaire, et tous les autres ont un degré de nécessité nul:
si $\omega_B \neq \omega_A$, 
$\mathcal{N}(\set{\omega_B})=1-\Pi(\overline{\set{\omega_B }}) = 1 - \pi(\omega_A) = 0$.

Généralement, une distribution de possibilité est plus informative, ou plus \textit{spécifique}, que l'ignorance totale
et moins \textit{spécifique} que la connaissance parfaite:
\begin{Def}[Spécificité]
\label{def_specificity}
Une distribution de possibilité $\pi_2$
est plus spécifique que la distribution de possibilité $\pi_1$, 
si $\forall \omega \in \Omega$,
\[ \pi_2(\omega) \leqslant \pi_1(\omega).\]
\end{Def}
Les deux notions de nécessité et de spécificité sont illustrée en figure \ref{figure_necspec}.

Les principaux concepts de la théorie des possibilité ont été présentés.
L'homologue du critère basé sur espérance dans les modèles probabilistes,
nous présentons maintenant le critère qualitatif utilisé dans les modèles possibilistes qualitatifs.

\section{Critères Qualitatifs}
\label{subsection_qualcrit}

\begin{figure}[b!]
\center
\begin{tikzpicture}
\definecolor{ggreen}{rgb}{0.3,0.7,0.4};
\begin{axis}[xtick=\empty,ymin=-0.19]
\foreach \xx in {-2,-1.5,-1,-0.5,0,0.5,1,1.5,2} {
\addplot[black,dotted] coordinates {(\xx,0) (\xx,1)};
}
\addplot[mark=none, red, ultra thick] coordinates { (-2, 0) (-1, 0) (-1, 0.2) (-0.5, 0.2) (-0.5, 0.4) (0, 0.4) (0,0.6) (1,0.6) (1,0.8) (1.5,0.8) (1.5,1) (2,1)  };
\addplot[mark=none, blue, ultra thick, densely dashed] coordinates { (-2, 1) (-1, 1) (-1, 1)  (-0.5, 1)   (-0.5, 0.8) (0, 0.8) (0,0.6) (0.5, 0.6) (0.5,0.4) (1,0.4) (1,0.2) (1.5,0.2) (1.5,0) (2,0)  };
\addplot[mark=none, black, ultra thick, dotted]%densely dashed] 
coordinates { (-2, 0.6) (2,0.6)  };
\end{axis}
\node (fff) at (4.8,4.8) { {\color{red} $f(\omega_i)$} };
\node (mumu) at (1.5,4.7) { {\color{blue} $\mu(A_i)$} };
\node (sug) at (1.5,3) {$\mathbb{S}_{\mu}[f]$};

\node (a) at (0.9,0.45) {$\omega_1$};
\node (b) at (1.63,0.45) {$\omega_2$};
\node (c) at (2.36,0.45) {$\omega_3$};
\node (d) at (3.09,0.45) {$\omega_4$};
\node (e) at (3.82,0.45) {$\omega_5$};
\node (f) at (4.55,0.45) {$\omega_6$};
\node (g) at (5.28,0.45) {$\omega_7$};
\node (h) at (6,0.45) {$\omega_8$};
\draw[ggreen, thick] (3.5,0.7) -- (6.3,0.7) -- (6.3,0.2) -- (3.5,0.2) -- (3.5,0.7);
\node (h) at (4,1) {{\color{ggreen} $A_5$}};

\end{tikzpicture}
\caption[Résultat de l'intégrale de Sugeno]{ 
illustration du résultat de l'intégrale de Sugeno: 
l'axe des abscisses représente l'ensemble $\Omega = \set{ \omega_1, \ldots, \omega_{\# \Omega} }$,
où $\forall i \in \set{1,\ldots,\# \Omega-1}$, $f(\omega_i) \leqslant f(\omega_{i+1})$. 
L'axe des ordonnées est $\mathcal{L}$. 
La courbe rouge représente les degrés $f(\omega_i)$, 
la bleue en pointillées représente les degrés $\mu(A_i)$ 
avec $A_i = \set{ \omega_i, \ldots, \omega_{\# \Omega} }$,
et la noire est le résultat de l'intégrale de Sugeno. }  
\label{figure_sugeno}
\end{figure}

L'espérance utilisée comme critère dans les PDMPO probabilistes,
est finalement l'integrale de la la fonction de récompense
contre la mesure de probabilité.
Le concept de l'intégrale a été étendue aux mesure floues:
Quand la mesure est quantitative, l'extension est appelée \textit{intégrale de Choquet} \cite{Choquet1954}.
Dans le cas des mesure qualitatives, l'objet résultant est l'\textit{intégrale de Sugeno} \cite{Sugeno74}.
Nous définissons donc ici l'intégrale de Sugeno:
\begin{Def}[Integrale de Sugeno]
\label{sugeno_integral}
L'intégrale de Sugeno d'une fonction $f:\Omega \rightarrow \mathcal{L}$ 
contre la capacité (mesure floue) $\mu:2^{\Omega} \rightarrow \mathcal{L}$ est
\begin{align} 
\label{equation_sugeno1} \mathbb{S}_{\mu} \croch{ f } &= \max_{i=1}^{\# \Omega} \min \set{ f(\omega_i), \mu(A_i) } \\
\label{equation_sugeno2} &=  \min_{i=1}^{\# \Omega} \max \set{ f(\omega_i), \mu(A_{i+1}) }
\end{align}
où $f(\omega_1) \leqslant \ldots \leqslant f(\omega_{\# \Omega})$, 
 $A_i = \set{ \omega_i, \omega_{i+1}, \ldots, \omega_{\# \Omega} }$
et $A_{\#\Omega+1} = \emptyset$.
\end{Def}

Comme illustré dans la figure \ref{figure_sugeno},
l'intégrale de Sugeno de $f: \Omega \rightarrow \mathcal{L}$ 
contre la mesure floue $\mu$
est le plus grand degré $\lambda \in \mathcal{L}$ 
tel que la mesure $\mu$ of $\set{ \omega \sachant f(\omega) \geqslant \lambda}$ 
est plus grande ou égale à $\lambda$. 
Par exemple, le $h$-indice
(ou indice de Hirsh)
est l'intégrale de Sugeno de la fonction $paper \mapsto \# citations$
contre la mesure de comptage. 

L'intégrale de Sugeno contre une mesure de possibilité
et celle contre la nécessité,
mènent à deux critères pour la planification.
Ces intégrales se réécrivent comme suit:
\begin{theorem}[L'intégrale de Sugeno contre les mesures de possibilité et de nécessité]
\label{sugenoPossNec}
\begin{align}
\label{equation_sugenoposs} \mathbb{S}_{\Pi}[f] &= \max_{i=1}^{\# \Omega} \min \set{ f(\omega_i), \pi(\omega_i) },\\
\label{equation_sugenonec} \mathbb{S}_{\mathcal{N}}[f] &= \min_{i=1}^{\# \Omega} \max \set{ f(\omega_i), 1-\pi(\omega_i) } .
\end{align}
sont les réécritures des intégrales de Sugeno contre les mesures de possibilité et de nécessité.
\end{theorem}

Les critères possibilistes qualitatifs,
\textit{i.e.} les fonctions $\mathcal{A} \rightarrow \mathcal{L}$ 
mesurant la validité des actions étant donné un modèle possibiliste et de préférence,
a été proposé dans \cite{DBLPjournals/ijar/SabbadinFL98,DBLPjournals/eor/DuboisPS01,Dubois95possibilitytheory},
basé sur les intégrales de Sugeno (\ref{equation_sugenoposs}) and (\ref{equation_sugenonec}).
Rappelons que l'ensemble $\mathcal{S}$ (resp. $\mathcal{A}$) 
est comme précédemment l'ensemble fini des états du système $s$ (resp. des actions $a$).
La variable représentant l'état du système est $S \in \mathcal{S}$.
Soit $\big(\pi_a\big)_{a \in \mathcal{A}}$ 
une famille de distributions de possibilité sur $\mathcal{S}$,
\textit{i.e.} $\forall a \in \mathcal{A}$, $\pi_a(s) = \Pi_a( \set{S = s} )$
est le degré de possibilité de la situation $\set{S=s} \subset \Omega$ 
lorsque l'action $a \in \mathcal{A}$ est sélectionnée. 
Soit $\rho: \mathcal{S} \rightarrow \mathcal{L}$ la fonction de préférence,
définissant le degré de préférence de chaque état du système $s \in \mathcal{S}$.

\begin{Def}[Critère de décision qualitatif]
Soit $\pi_a$ la distribution de possibilité
décrivant l'incertitude à propos de l'état du système
étant donné que l'action $a \in \mathcal{A}$ a été sélectionnée, 
et $\rho(s)$ la préférence de l'état du système $s \in \mathcal{S}$.
En utilisant la formule (\ref{equation_sugenoposs}) avec $f=\rho(S)$, 
l'intégrale de Sugeno integral de la préférence contre la mesure de possibilité $\Pi_a$
mène à un critère optimiste pour $a \in \mathcal{A}$:
\label{def_qualcrit}
\begin{align}
\label{equation_critopt} \mathbb{S}_{\Pi_a}[\rho(S)] &= \max_{s \in \mathcal{S}} \min \set{ \rho(s), \pi_a(s) }.
\end{align}
De même, en utilisant la formule (\ref{equation_sugenonec}) avec $f=\rho(S)$, 
l'intégrale de Sugeno de la préférence contre la mesure de nécessité associée à $\Pi_a$, $\mathcal{N}_a$, 
mène au critère pessimiste pour $a \in \mathcal{A}$:
\begin{align}
\label{equation_critpess} \mathbb{S}_{\mathcal{N}_a}[\rho(S)] &= \min_{s \in \mathcal{S}} \max \set{ \rho(s), 1-\pi_a(s) }.
\end{align}
\end{Def}

Le critère pessimiste (\ref{equation_critpess}) a tendance à éviter les états non désirés,
tandis que le critère optimiste (\ref{equation_critopt}) souhaite rendre possible le fait d'atteindre les états préférés.
La figure (\ref{figure_criteria}) illustre le résultat du critère 
pour une action donnée $a\in\mathcal{A}$.

L'exemple suivant, illustré en figure \ref{figure_example},
montre bien la différence entre ces deux critères:
soit $\mathcal{S} = \set{ s_A , s_B , s_C }$ l'ensemble des états
et l'ensemble des actions $\mathcal{A} = \set{ a_1 , a_2 }$. 
Le modèle de préférence et le modèle d'incertitude
sont décrits respectivement par $\rho$ et $\paren{\pi_a}_{a \in \mathcal{A}}$:
\begin{itemize}
\item[$\bullet$] $1 = \rho(s_A) > \rho(s_B) > \rho(s_C) = 0$;
\item[$\bullet$] si l'action $a_1$ est sélectionnée, $\pi_{a_1}(s_A) = \pi_{a_1}(s_C) = 1$, et $\pi(s_B) = 0$;
\item[$\bullet$] si l'action $a_2$ est sélectionnée, $\pi_{a_2}(s_A) = \pi(s_C) = 0$, et $\pi(s_B) = 1$, \textit{i.e.}
le système est dans l'état $s_B$ de manière déterministe.
\end{itemize}

\begin{figure}
\center
\begin{tikzpicture} 
\tikzstyle{vertex}=[circle,fill=black!30,minimum size=20pt,inner sep=0pt]
\node[vertex] (G1) at (0,1) {$s_A$};
\node[vertex] (G2) at (0,0) {$s_B$};
\node[vertex] (G3) at (0,-1) {$s_C$};
\node (G1b) at (-3,1) {};
\node (G2b) at (-3,0) {};
\node (G3b) at (-3,-1) {};

\node (poss1) at (-1.7,1.3) {$\pi_{a_1}(s_A)=1$};
\node (poss2) at (-1.7,0.3) {$\pi_{a_1}(s_B)=0$};
\node (poss3) at (-1.7,-0.7) {$\pi_{a_1}(s_C)=1$};
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (G1b) to (G1);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (G2b) to (G2);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (G3b) to (G3);

\node[vertex] (GG1) at (5,1) {$s_A$};
\node[vertex] (GG2) at (5,0) {$s_B$};
\node[vertex] (GG3) at (5,-1) {$s_C$};
\node (GG1b) at (2,1) {};
\node (GG2b) at (2,0) {};
\node (GG3b) at (2,-1) {};

\node (poss1) at (3.3,1.3) {$\pi_{a_2}(s_A)=0$};
\node (poss2) at (3.3,0.3) {$\pi_{a_2}(s_B)=1$};
\node (poss3) at (3.3,-0.7) {$\pi_{a_2}(s_C)=0$};
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (GG1b) to (GG1);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (GG2b) to (GG2);
\draw[->,decorate,decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}] (GG3b) to (GG3);

\node (mu1) at (8,1.2) {$\rho(s_A)=1$};
\node (mu2) at (8.15,0.2) {$\rho(s_B)=0.5$};
\node (mu3) at (8,-0.8) {$\rho(s_C)=0$};
\end{tikzpicture}
\caption[Exemple d'une situation pour illustrer les critères qualitatifs]{Illustration de l'example de la section \ref{subsection_qualcrit}
à propos des critères qualitatifs. L'action $a_1$ maxmise le critère optimiste (\ref{equation_critopt}),
qui peut mener au meilleur état ($s_A$), 
mais aussi au pire ($s_C$).
Au contraire, l'action $a_2$ maximise
le critère pessimiste (\ref{equation_critpess})
puisque le pire cas n'est pas atteignable avec cette action.}  \label{figure_example}
\end{figure}

Le critère optimiste est maximisé par l'action $a_1$,
puisqu'avec cette action, le meilleur état du système, $s_A$, 
est entièrement possible.
Cependant, cette action rend le pire état du système, $s_C$,
entièrement possible, et l'état $s_A$ n'est pas du tout nécessaire: 
une action plus prudente est $a_2$, avec laquelle l'état devient $s_B$
avec certitude, mais avec une plus petite préférence.
On peut vérifier facilement que l'action $a_2$ maximises bien le critère pessimiste (\ref{equation_critpess}):

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\pi$-PDM}
\label{subsection_piMDPs}
Ce modèle, présenté dans \cite{Sabbadin2001287,Sabbadi00,Sabbadin1999pipomdp}, 
est la version possibiliste qualitative des PDM probabilistes décrits en introduction,
basée sur les critères (optimistes et pessimistes) présentés en section \ref{subsection_qualcrit}:
cette version est appelée \textit{processus décisionnel markovien possibiliste qualitatif}, ou $\pi$-PDM.

L'ensemble fini des états du système
décrivant l'agent et son environnement, 
reste noté $\mathcal{S}$, 
comme vu en introduction avec les modèles probabilistes.
L'ensemble fini des actions est toujours $\mathcal{A}$ 
et $\mathcal{L}$ est l'échelle possibiliste $\set{ 0, \frac{1}{k}, \ldots, 1 }$,
avec $k\geqslant2$.

Comme dans le cas probabiliste, 
ce modèle considère que
les états successifs du système,
représentés par la séquence de variables $(S_t)_{t \in \mathbb{N}}$
avec $S_t \in \mathcal{S}$ $\forall t \geqslant 0$, is markovien.
Dans ce cadre possibiliste qualitatif,
cela signifie que la séquence $(S_t)_{t \in \mathbb{N}}$ est telle que
$\forall t \geqslant 0,  \forall (s_0,s_1,\ldots,s_{t+1}) \in \mathcal{S}^{t+2}$
et pour chaque séquence d'actions $(a_t)_{t\geqslant 0} \in \mathcal{A}^{\mathbb{N}}$, 
$S_{t+1}$ est indépendante (au sens causal) des variables $\set{ S_0, \ldots, S_{t-1} }$,
conditionnellement à $\set{S_t = s}$ et $a_t$: 
\begin{equation}
\label{equation_possmarkov}
  \Pi \Big( \ S_{t+1}=s_{t+1} \ \Big\vert \ S_t=s_t, a_t \ \Big) = \Pi \Big( \ S_{t+1}=s_{t+1} \ \Big\vert \ S_t=s_t, S_{t-1}=s_{t-1}, \ldots, S_0=s_0, (a_t)_{t \geqslant 0} \ \Big).
\end{equation}
Cette indépendance possibiliste, ici causale, 
n'est pas la seule existante:
une présentation générale des indépendances et des conditionnements,
ainsi que de leurs conséquences dans les modèles graphiques
est disponible dans la thèse de N.Ben Amor \cite{Be2002.7}.

En utilisant cette propriété markovienne, 
la dynamique du système est entièrement décrite
avec les transitions possibilistes $\pi_t \paren{ s' \sachant s, a } = \Pi \Big( \ S_{t+1}=s' \ \Big\vert \ S_t=s, a \ \Big)  \in \mathcal{L}$:
$\forall t \geqslant 0$, $(s,s') \in \mathcal{S}^2$ et $a \in \mathcal{A}$,
$\pi_t \paren{ s' \sachant s, a }$ est le degré de possibilité, qu'à l'étape de temps $t$, 
le système atteigne l'état $s'$
lorsque l'agent choisit l'action $a$,
conditionné au fait que l'état courant est $s$.

Enfin, un $\pi$-PDM est entièrement défini avec la séquence de fonctions de préférence $(\rho_t)_{t=0}^{H-1}$,
où $\forall s \in \mathcal{S}$, $\forall a\in \mathcal{A}$, 
$\rho_t(s,a)$ est le degré de préférence lorsque l'état du système est $s$
et l'agent sélectionne l'action $a$
au temps $t$.
La fonction de préférence terminale, $\Psi$, 
donne pour chaque état du système $s \in \mathcal{S}$, 
le degré de préférence
si $S_H = s$: $\Psi(s)$.

Afin de définir les critères des $\pi$-PDM
à partir des critères possibilistes (\ref{equation_critopt}) and (\ref{equation_critpess}), 
nous introduisons, pour un horizon $H\geqslant0$, 
les \textit{trajectoires de longueur $H$} $\mathcal{T} = (s_1,\ldots,s_{H})$,
et $\mathcal{T}_H = \mathcal{S}^{H}$ l'ensemble de telles trajectoires.
Une règle de décision est notée $\delta: \mathcal{S} \rightarrow \mathcal{L}$, 
et une stratégie de longueur $H$ est une séquence de règles de décision $\delta_t$: $(\delta_t)_{t=0}^{H-1}$.
L'ensemble de toutes les stratégies de taille $H$ est noté $\Delta_H$.
Dans \cite{Sa1998.16}, 
pour une stratégie donnée $(\delta) \in \Delta_H$,
une séquence d'états du système $\mathcal{T} = (s_1,\ldots,s_H) \in \mathcal{T}_H$, 
et un état initial donné $s_0 \in \mathcal{S}$,
la \textit{préference d'une stratégie de taille $H$ commençant par $s_0$} 
est définie comme le degré de possibilité le plus petit de la trajectoire et de $s_0$:
\[ \rho \Big( \mathcal{T}, (\delta) \Big) = \min \set{ \min_{t=0}^{H-1} \rho\Big(s_t,\delta_t(s_t)\Big), \Psi(s_H)}.\]

En utilisant la propriété de Markov de ce processus d'états du système,
pour un état initial donné $s_0 \in \mathcal{S}$, 
un horizon $H \in \mathbb{N}$,
et une stratégie $(\delta_t)_{t=0}^{H-1}$, 
le degré de possibilité de la trajectoire $\mathcal{T} = (s_1, \ldots, s_H)$ est
\begin{equation}
\label{equation_possdistribtraj}
\Pi \paren{ S_H=s_h, S_{H-1}=s_{h-1}, \ldots, S_1 = s_1 \Big\vert S_0 = s_0, \big(\delta_t\big)_{t=0}^{H-1} } = \min_{t=0}^{H-1} \pi_{t+1} \Big( s_{t+1} \Big\vert s_{t}, \delta_{t}(s_{t}) \Big)
\end{equation}
noté $\pi \Big( \mathcal{T} \Big\vert s_0, (\delta) \Big)$.

L'intégrale de Sugeno de la préférence de la trajectoire
contre cette distribution est notée 
\[\mathbb{S}_{\Pi} \croch{ \rho \Big( \mathcal{T}, (\delta) \Big) \sachant S_0 = s_0, (\delta) } = \mathbb{S}_{\Pi} \croch{ \displaystyle \min \set{ \min_{t=0}^{H-1} \rho \Big(S_t,\delta_t(S_t) \Big), \Psi(S_H) } \sachant S_0 = s_0, (\delta) }\]
et correspond du critère optimiste definissant la stratégie optimale,
\textit{i.e.} une fonction valeur optimiste:
\begin{align} 
\overline{U_H} \Big( s_0,(\delta_t)_{t=0}^{H-1} \Big) &= \max_{\mathcal{T} \in \mathcal{T}_H} \min \set{ \rho \Big( \mathcal{T}, (\delta) \Big), \pi \Big( \mathcal{T} \Big\vert s_0, (\delta) \Big) }. \label{equation_optqualvalue} 
\end{align}
C'est équivalant du critère optimiste (\ref{equation_critopt}),
cependant, l'intégrale est sur les trajectoires $\mathcal{T}_H$, 
et la préférence dépend de la stratégie.
La \textit{stratégie optimale optimiste} $\overline{\delta^*}$ est la stratégie
maximisant la fonction valeur optimiste (\ref{equation_optqualvalue}),
et la \textit{fonction valeur optimiste optimale}
est la fonction valeur optimiste la plus grande en faisant varier $(\delta) \in \Delta_H$:
\begin{Def}[Fonction valeur et stratégie optimiste optimale]
$\forall s \in \mathcal{S}$,
\begin{align} 
\label{equation_optqualvaluestar} \overline{U_H^*}(s) &= \max_{(\delta) \in \Delta_H} \set{ \overline{U_H} \Big(s,(\delta)\Big) } & \mbox{ (fonction valeur optimale optimiste), } \\
\label{equation_optqualstratstar} \overline{\delta^*}(s) &\in \operatorname*{argmax}_{(\delta) \in \Delta_H} \set{ \overline{U_H} \Big(s,(\delta)\Big) } & \mbox{ (stratégie optimale optimiste). } 
\end{align}
\end{Def}

De même, le critère possibiliste qualitatif optimal (\ref{equation_critpess})
mène à un critère prudent pour les stratégies:
la fonction valeur pessimiste est
l'intégrale de Sugeno de la préférence de la trajectoire
contre la mesure de nécessité
qui vient de la distribution de possibilité
sur les trajectoires $\mathcal{T}_H$ (\ref{equation_possdistribtraj}) 
avec la stratégie $(\delta) \in \Delta_H$:
\begin{align} 
 \underline{U_H} \Big( s_0,(\delta_t)_{t=0}^{H-1} \Big) &= \min_{\mathcal{T} \in \mathcal{T}_H} \max \set{ \rho \Big( \mathcal{T},  (\delta) \Big), 1 - \pi \Big( \mathcal{T} \Big\vert s_0, (\delta) \Big) }. \label{equation_pessqualvalue}
\end{align}
notée $\mathbb{S}_{\mathcal{N}} \croch{ \rho \Big( \mathcal{T},  (\delta) \Big) \sachant S_0 = s, (\delta) }$.
Comme précédemment pour le cas optimiste,
la \textit{stratégie optimale optimiste} $\underline{\delta^*}$ 
est la stratégie maximisant
la fonction valeur pessimiste (\ref{equation_pessqualvalue}),
et la \textit{fonction valeur pessimiste optimale}
 est la fonction valeur maximale en faisant varier les stratégies $(\delta) \in \Delta_H$:
\\
\\
\vbox{
\begin{Def}[Fonction valeur et stratégie pessimiste optimale]
$\forall s \in \mathcal{S}$,
\begin{align} 
\label{equation_pessqualvaluestar} \underline{U_H^*}(s) &= \max_{(\delta) \in \Delta_H} \set{ \underline{U_H} \Big(s,(\delta)\Big) } & \mbox{ (fonction valeur optimale optimiste pessimiste), } \\
\label{equation_pessqualstratstar} \underline{\delta^*}(s) &\in \operatorname*{argmax}_{(\delta) \in \Delta_H} \set{ \underline{U_H} \Big(s,(\delta)\Big) } & \mbox{ (stratégie optimale pessimistic). } 
\end{align}
\end{Def}
}

Les fonctions valeur optimales et les stratégies peuvent être calculée par programmation dynamique:
\begin{theorem}[Programmation Dynamique pour $\pi$-PDM]
\label{DPpiMDP}
Le critère optimiste optimal et la stratégie optimale associée
peuvent être calculés comme suit: 
$\forall s \in \mathcal{S}$,
\begin{align}
\nonumber 
\overline{U_0^*}(s) & = \Psi(s), \mbox{\hspace{1cm} and, $\forall 1 \leqslant i \leqslant H$,} \\
\label{equation_recursiveopt} 
\overline{U_{i}^*}(s)& = \max_{a \in \mathcal{A}} \min \set{ \rho_{H-i}(s,a), \max_{s' \in \mathcal{S}} \min \set{ \pi_{H-i} \paren{ s' \sachant s, a  } , \overline{U^*_{i-1}}(s') }  }.
\end{align}
\begin{eqnarray}
\label{equation_recursiveoptstrat} 
\overline{\delta^*_{H-i}}(s) \in \operatorname*{argmax}_{a \in \mathcal{A}} \min \set{ \rho_{H-i}(s,a), \max_{s' \in \mathcal{S}} \min \set{ \pi_{H-i} \paren{ s' \sachant s, a  } , \overline{U^*_{i-1}}(s') }  }.
\end{eqnarray}
De même, le critère pessimiste optimal, et la stratégie associée
peut être calculé comme suit: $\forall s \in \mathcal{S}$,
\begin{align}
\nonumber 
\underline{U_0^*}(s) & = \Psi(s), \mbox{\hspace{1cm} and, $\forall 1 \leqslant i \leqslant H$,} \\
\label{equation_recursivepess} 
\underline{U_{i}^*}(s)& = \max_{a \in \mathcal{A}} \min \set{ \rho_{H-i}(s,a), \min_{s' \in \mathcal{S}} \max \set{ 1 - \pi_{H-i} \paren{ s' \sachant s, a  } , \underline{U^*_{i-1}}(s') }  }.
\end{align}
\begin{eqnarray}
\label{equation_recursivepessstrat} 
\underline{\delta^*_{H-i}}(s) \in \operatorname*{argmax}_{a \in \mathcal{A}} \min \set{ \rho_{H-i}(s,a), \min_{s' \in \mathcal{S}} \max \set{ 1 - \pi_{H-i} \paren{ s' \sachant s, a  } , \underline{U^*_{i-1}}(s') }  }.
\end{eqnarray}
\end{theorem}

Dans ce théorème, l'horizon $i$ est l'opposé modulo $H$
de l'étape du processus $t$:
durant l'éxécution, $\delta_{t} = \delta_{H-i}$ 
est utilisée à l'étape de temps $t$,
\textit{i.e.} lorsque il reste $i$ étapes. 

Notons qu'une classe plus lrge de modèles PDM,
incluant les PDM probabilistes et possibilistes,
est appelé \textit{PDM algébrique} \cite{LIP67955}. 

La section suivant est dédiée
à la présentation de l'homologue possibiliste qualitatif du PDMPO
noté $\pi$-PDMPO: 
le modèle $\pi$-PDMPO est la version partiellement observable du modèle $\pi$-PDM.
Ce modèle a été présenté dans \cite{Sabbadin1999pipomdp}
dans le cadre pessimiste.
L'algorithme pour le résoudre a été présenté dans le cas où
aucune préférence intermédiaire n'est impliquée 
\textit{i.e.} dans le cas où les fonctions de préférence $\rho_t$ ne sont pas prise en compte:
dans ce cas, seule la fonction de préférence terminale $\Psi$
modèle le but de la mission.
Enfin, on remarque qu'il suffit de considérer un $\pi$-PDM classique
avec $\rho_t(s,a) = 1$, $\forall s \in \mathcal{S}$, $\forall a \in \mathcal{A}$ and $\forall t \in \set{0,\ldots,H-1}$. 

\section{$\pi$-PDMPO}
\label{section_piPOMDP}
Le modèle PDMPO possibiliste qualitatif
($\pi$-PDMPO) a été présenté pour la première fois dans \cite{Sabbadin1999pipomdp}.
Comme avec le modèle probabiliste,
dans le cadre partiellement observable
l'état du système n'est plus considéré comme une donnée d'entrée pour l'agent: 
l'agent doit l'estimer à partir des observations $o \in \mathcal{O}$ 
reçues à chaque étape de temps, 
représentée par le processus d'observation $(O_t)_{t \in \mathbb{N}}$.
L'incertitude à propos des variables d'observation successives $O_t$
ne dépend que de l'action et de l'état atteint:
si l'agent choisit l'action $a \in \mathcal{A}$ au temps $t$,
et le système a atteint l'état $s' \in \mathcal{S}$ à l'étape de temps $t+1$,
l'observation $o' \in \mathcal{O}$
est reçue avec le degré de possibilité 
$\pi_t \paren{ o' \sachant s', a} = \Pi \paren{ O_{t+1} = o' \sachant S_{t+1}=s', a }$:
conditionnellement à l'état suivant $s'$ et à l'action courante $a$,
la variable d'observation suivante est
indépendante (au sens causal)
de toutes les autres variables jusqu'à l'étape $t+1$. 
La figure \ref{POMDP} illustre 
la dynamique et la structure d'un $\pi$-PDMPO.
Un PDMPO probabiliste a la même structure,
sauf que les distributions de possibilité de transition (resp. d'observation)
$\pi \paren{s' \sachant s,a}$ (resp. $\pi \paren{o' \sachant s',a}$).
doivent être remplacée par la distribution de probabilité 
$\textbf{p} \paren{s' \sachant s,a}$ 
(resp. $\textbf{p} \paren{o' \sachant s',a}$).

\begin{figure}[!t]
\input{src_french/fig-pomdp}
\caption[Diagramme d'Influence d'un $\pi$-PDMPO et de son processus d'états de croyance]{
Diagramme d'Influence d'un $\pi$-PDMPO et de son processus d'états de croyance:
les ronds noirs représentent les états successifs du système $S_t$,
les bleus représentent les observations successives $O_t$,
et les carrés rouges sont les actions sélectionnées $a_t$.
Les cercles verts en haut de la figure sont les états de croyance successifs
$B_t$ constituant le processus d'états de croyance,
calculé avec la mise à jour $B_{t+1} = \nu(B_t,a_t,O_{t+1})$.
Les lignes vertes en pointillé représentent une influence déterministe.}
\label{POMDP} 
\end{figure}


Comme avec le modèle probabiliste,
le calcul des stratégies
est effectué en traduisant le $\pi$-PDMPO
en un $\pi$-PDM entièrement observable.
L'espace d'état de ce dernier est l'ensemble
des \textit{états de croyance possibilistes qualitatifs}
$\beta: \mathcal{S} \rightarrow \mathcal{L}$ 
décrivant la connaissance à propos de l'état du système,
\textit{i.e.} l'ensemble de distributions de possibilité sur $\mathcal{S}$.
Cet ensemble est noté by $\Pi^{\mathcal{S}}_{\mathcal{L}} = \set{ \pi: \mathcal{S} \rightarrow \mathcal{L} \sachant \max_{s \in \mathcal{S}} \pi(s) = 1 }$. 
Notons tout d'abord que le nombre d'états de croyance à propos de l'état du système est
\begin{equation}
\label{equation_numberOfPossDistrib}
\paren{\# \mathcal{L}}^{\# \mathcal{S}} - \paren{\# \mathcal{L} -1}^{\# \mathcal{S}}.
\end{equation}
En effet, il y a $\# \mathcal{L}^{\# \mathcal{S}}$ 
fonctions différentes de $\mathcal{S}$ vers $\mathcal{L}$,
et $\paren{ \# \mathcal{L} - 1 }^{\# \mathcal{S}}$ fonctions non normalisées
\textit{i.e.} fonctions $f: \mathcal{S} \rightarrow \mathcal{L}$ 
telles que $\max_{s \in \mathcal{S}} f(s) <1$.
Le nombre de distributions de possibilité sur $\mathcal{S}$
est le nombre de fonctions normalisées de $\mathcal{S}$ vers $\mathcal{L}$,
qui est le nombre total de fonctions, moins le nombre de fonctions non normalisées.

Tout d'abord, définissons formellement un $\pi$-PDMPO comme le $7$-uplet 
$<\mathcal{S},\mathcal{A},\mathcal{O},T^{\pi},O^{\pi},\Psi,\beta_0>$:
\begin{itemize}
\item $\mathcal{S}$, un ensemble fini d'états cachés du système;
\item $\mathcal{A}$ un ensemble fini d'actions;
\item $\mathcal{O}$ un ensemble fini d'observations;
\item $T^{\pi}$ l'ensemble des distributions de possibilité de transition $\pi \paren{s' \sachant s, a}$;
\item $O^{\pi}$, l'ensemble des distribution de possibilité d'observation $\pi \paren{ o' \sachant s',a }$ is part of $O^{\pi}$.
\item $\Psi$ la fonction de préférence, définissant pour chaque état $s \in \mathcal{S}$,
la préférence associée à la situation où l'état du système termine dans l'état $s$.
\item $\beta_0$, \textit{l'état de croyance possibiliste initial},
est la distribution de possibilité définissant l'incertitude possibiliste
à propos de l'état initial:
$\forall s \in \mathcal{S}$, $\beta_0(s) = \Pi(S_0 = s)$. 
\end{itemize}

\`A chaque étape de temps, l'état de croyance possibiliste
est calculé à partir de ces objets: 
L'état de croyance initial $\beta_0 \in \Pi^{\mathcal{S}}_{\mathcal{L}}$ 
fait partie de la définition du $\pi$-PDMPO.

\`A l'étape de temps $t \geqslant 1$, 
l'état de croyance est la distribution de possibilité
sur l'état courant du système, 
conditionnellement à toutes les données accessibles pour l'agent. 
\begin{Def}[\'Etat de croyance possibiliste qualitatif]
\label{def_QualPossBel}
\begin{equation}
\beta_{t}(s) = \Pi \paren{ S_t = s \sachant O_1=o_1, \ldots, O_t=o_t, a_0, \ldots, a_{t-1} } = \Pi \paren{ S_t = s \sachant I_t = i_t }
\end{equation}
\end{Def}
où $i_t = \set{o_1,\ldots,o_t,a_0,\ldots,a_{t-1}}$ 
est l'information accessible pour l'agent à l'étape de temps $t$ ( $i_0 = \set{} = \emptyset$ ),
et $I_t$ la variable correspondante.

La mise à jour possibiliste de l'état de croyance est basée sur l'homologue possibiliste de la règle de Bayes:
\begin{theorem}[Mise à jour possibiliste qualitative de l'état de croyance]
\label{belief_process_recursif_poss}
Si l'état de croyance à l'étape de temps $t$ est $b_t$,
l'action choisie est $a_t \in \mathcal{A}$,
et l'état suivant est $o_{t+1}$, 
l'état suivant $b_{t+1}$
est calculé comme suit:
\begin{equation}
\label{possbeliefupdate}
\beta_{t+1}(s') = \left \{ \begin{array}{ccc}
1 & \mbox{ si } \pi_t \paren{ s', o_{t+1} \sachant \beta_t, a_t}= \pi_t \paren{ o_{t+1} \sachant \beta_t,a_t }, \\
\pi_t \paren{ s', o_{t+1} \sachant \beta_t, a_t} & \mbox{ sinon. } 
\end{array} \right.
\end{equation}
où la distribution de possibilité jointe sur la variable d'état $S_{t+1}$
et la variable d'observation $O_{t+1}$ 
conditionnellement à l'information courante,
est notée $\pi_t \paren{ s', o' \sachant \beta_t, a_t} 
= \min \bigg\{ \pi_{t} \paren{o' \sachant s', a_t},  \max_{s \in \mathcal{S}} \min \Big\{ \pi_t \paren{ s' \sachant s, a_t }, \beta_t(s) \Big\} \bigg\}$. 
La notation $\pi \paren{ o' \sachant \beta_t, a_t}$ est aussi utilisée
pour $\max_{s' \in \mathcal{S}} \pi_t \paren{ s', o' \sachant \beta_t, a_t}$. 

Cette formule est appelée la \textbf{mise à jour de la croyance possibiliste},
et puisque l'état de croyance $\beta_{t+1}$
est une fonction de $\beta_t$, $a_t$ and $o_{t+1}$,
nous notons cette mise à jour \[ \beta_{t+1} = \nu(\beta_t,a_t,o_{t+1}),\]
avec $\nu$ appelée \textit{fonction de mise à jour}.
\end{theorem}

La mise à jour possibiliste de l'état de croyance (\ref{possbeliefupdate}) 
est noté
\[ \beta_{t+1}(s') \propto^{\pi} \pi_t \paren{ s', o_{t+1} \sachant \beta_t, a_t} \]
puisqu'elle consiste seulement à normaliser
la fonction $s' \mapsto \pi \paren{ s',o_{t+1} \sachant \beta_t,a_t }$
au sens possibiliste ($\max_{s} \pi(s) = 1$).

Nous notons $B^{\pi}_t$ l'état de croyance lorsque considéré comme une variable,
\textit{i.e.} $B^{\pi}_0$ est déterministe égal à $\beta_0$
(mais $S_0$ est incertain, ce qui est décrit par la distribution de possibilité $\beta_0$)
et $B^{\pi}_{t+1} = \nu(B^{\pi}_t,a_t,O_{t+1})$
où $O_{t+1}$ est la variable d'observation à l'étape de temps $t+1$.

Afin de rendre les choses plus claires,
le modèle $\pi$-PDMPO est ici défini sans préférences intermédiaires $\rho_t$,
mais avec une préférence terminale $\Psi$.

Nous pouvons maintenant exprimer la distribution de possibilité
de transition du processus de croyance
\textit{i.e.} les éléments de $\tilde{T}$, 
comme suit: $\forall t \geqslant 0$,
\begin{align}
\label{transition_possibilistic_belief} \pi_t \paren{ \beta' \sachant \beta,a} &= \max_{ \substack{ o' \in \mathcal{O} \mbox{ \tiny s.t. } \\ \nu(\beta,a,o') = \beta'} } \pi_t \paren{ o' \sachant \beta, a },
\end{align}
où $\pi_t \paren{ o' \sachant \beta, a } = \max_{(s,s') \in \mathcal{S}^2} \min \Big\{ \pi_t \paren{ o' \sachant s', a_t }, \pi_t \paren{ s' \sachant s, a_t }, \beta(s) \Big\}$,
est le degré de possibilité d'observer $o'$
conditionnellement à toutes les informations précédentes.

Enfin, la fonction de préférence associée à l'état de croyance possibiliste $\beta_H$ 
est défini de manière pessimiste:
$\forall \beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$,
\begin{equation}
\label{pess_preference}
 \underline{\Psi}(\beta) = \min_{s\in \mathcal{S}} \max \set{ \Psi(s), 1 - \beta(s) }.
\end{equation}
Le cas optimiste donne une préférence maximale à l'ignorance totale,
ce qui ne semble pas pouvoir mener l'agent à estimer l'état du système 
afin d'essayer d'atteindre un état satisfaisant.

Il est possible de montrer qu'il est suffisant de chercher une stratégie basée sur les états de croyance
\textit{i.e.} parmi les suites de règles de décision $(\delta_t)_{t=0}^{H-1}$,
telles que $\forall t \geqslant 0$, $\delta_t: \beta_t \mapsto \delta_t(\beta_t) \in \mathcal{A}$.

Le $\pi$-PDM $\langle \tilde{\mathcal{S}}^{\pi}, \mathcal{A}, \tilde{T^{\pi}}, \overline{\Psi} \mbox{ or } \underline{\Psi} \rangle$
construit à partir d'un $\pi$-PDMPO 
$\langle \mathcal{S}, \mathcal{A}, \mathcal{O}, T^{\pi}, O^{\pi}, \beta_0 \rangle$ est finalement: 
\begin{itemize}
\item $\tilde{\mathcal{S}}^{\pi} = \Pi_{\mathcal{L}}^{\mathcal{S}}$, l'ensemble de tous les états de croyance (possibilistes qualitatifs) possibles;
\item $\tilde{T^{\pi}}$ contient toutes les distributions de possibilité de transition
sur les états de croyance: $\forall a \in \mathcal{A}$, $\forall \beta \in  \Pi_{\mathcal{L}}^{\mathcal{S}}$, 
ces distributions de possibilité sont définies par l'équation (\ref{transition_possibilistic_belief}),
$\pi_t \paren{ . \sachant \beta, a }$ is in $\tilde{T^{\pi}}$; 
\item la fonction de préférence est l'estimation pessimiste de la préférence: $\underline{\Psi}$, \textit{cf.} équation (\ref{pess_preference}).
\end{itemize}

Notons maintenant que,
en utilisant la définition de la fonction de transition des états de croyance  (\ref{transition_possibilistic_belief}),
pour chaque fonction de l'espace des états de croyance
vers $\mathcal{L}$, $U: \Pi^{\mathcal{S}}_{\mathcal{L}} \rightarrow \mathcal{L}$,
\begin{align*}
\max_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \min \set{ \pi_t \paren{ \beta' \sachant \beta, a}, U(\beta') } &= \max_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \min \set{ \max_{ \substack{ o' \in \mathcal{O} \mbox{ \tiny s.t. } \\ \nu(\beta,a,o') = \beta'} } \pi_t \paren{ o' \sachant \beta, a } , U(\beta') }\\
&=\max_{\beta' \in \Pi^{\mathcal{S}}_{\mathcal{L}}} \max_{ \substack{ o' \in \mathcal{O} \mbox{ \tiny s.t. } \\ \nu(\beta,a,o') = \beta'} } \min \set{  \pi_t \paren{ o' \sachant \beta, a } , U(\beta') }\\
&= \max_{o' \in \mathcal{O}} \min \set{ \pi_t \paren{ o' \sachant \beta, a}, U\Big(\nu(\beta,a,o') \Big) }, 
\end{align*}
Cette observation mène à l'algorithme \ref{PIPOMDP_algo_opt}
qui est l'application directe du schéma de programmation dynamique
pour $\pi$-PDM (\ref{DPpiMDP})
avec seulement une préférence terminale:
ce schéma est utilisé sur le $\pi$-PDM 
$\langle \tilde{\mathcal{S}}^{\pi}, \mathcal{A}, \tilde{T^{\pi}}, \underline{\Psi} \rangle$.

\begin{algorithm} \caption{Algorithme de programmation dynamique pour $\pi$-PDMPO optimiste avec une préférence sur l'état final seulement} \label{PIPOMDP_algo_opt}
$\overline{U^*_0} \gets \overline{\Psi}$;\\
\For{$i \in \set{1,\ldots,H}$}{
	\For{$\beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$}{
		$\displaystyle \overline{U^*_i}(\beta) \gets \max_{a \in \mathcal{A}} \max_{o' \in \mathcal{O}} \min \set{ \pi_t \paren{ o' \sachant \beta,a } , \overline{U^*_{i-1}}\Big(\nu(\beta,a,o')\Big) }$;\\
		$\displaystyle \overline{\delta_{H-i}}(\beta) \in \operatorname*{argmax}_{a \in \mathcal{A}} \max_{o' \in \mathcal{O}} \min \set{ \pi_t \paren{ o' \sachant \beta,a }, \overline{U^*_{i-1}} \paren{ \nu(\beta,a,o') } }$;\\
	}
}
\Return $\overline{U^*_H}$, $(\overline{\delta^*})$;
\end{algorithm}

L'algorithme \ref{PIPOMDP_algo_pess},
qui est la programmation dynamique pour $\pi$-PDM 
(\ref{dynamic_programming_pimdppess}),
avec une préférence terminale seulement:
elle est appliquée au $\pi$-MDP $\langle \tilde{\mathcal{S}}^{\pi}, \mathcal{A}, \tilde{T^{\pi}}, \underline{\Psi} \rangle$.

\begin{algorithm} \caption{Algorithme de programmation dynamique pour $\pi$-PDMPO pessimiste avec une préférence terminale seulement} \label{PIPOMDP_algo_pess}
$\underline{U^*_0} \gets \underline{\Psi}$;\\
\For{$i \in \set{1,\ldots,H}$}{
	\For{$\beta \in \Pi^{\mathcal{S}}_{\mathcal{L}}$}{
		$\displaystyle \underline{U^*_i}(\beta) \gets \max_{a \in \mathcal{A}} \min_{o' \in \mathcal{O}} \max \set{ 1 - \pi_t \paren{ o' \sachant \beta,a } , \underline{U^*_{i-1}}\Big(\nu(\beta,a,o')\Big) }$;\\
		$\displaystyle \underline{\delta_{H-i}}(\beta) \in \operatorname*{argmax}_{a \in \mathcal{A}} \min_{o' \in \mathcal{O}} \max \set{ 1 - \pi_t \paren{ o' \sachant \beta,a }, \underline{U^*_{i-1}} \paren{ \nu(\beta,a,o') } }$;\\
	}
}
\Return $\underline{U^*_H}$, $(\underline{\delta^*})$;
\end{algorithm}

Dans ce premier chapitre la théorie des possibilités qualitatives 
ainsi que les modèle $\pi$-PDM et $\pi$-PDMPO.

Dans le cadre probabiliste des PDMPO,
l'incertitude est décrite avec des probabilités $\textbf{p} \paren{ s' \sachant s,a } \in \mathbb{R}$
et $\textbf{p} \paren{ o' \sachant s',a } \in \mathbb{R}$
tandis qu'elle est définie par des distributions de possibilité $\pi \paren{ s' \sachant s,a } \in \mathcal{L} = \set{ 0, \frac{1}{k}, \ldots, 1}$ (with $k\geqslant1$) 
et $\pi \paren{ o' \sachant s',a } \in \mathcal{L}$ 
dans le cadre $\pi$-PDMPO.
De plus, le cadre probabiliste mesure l'intérêt de passer par un état $s \in \mathcal{S}$
et d'utiliser l'action $a \in \mathcal{A}$
avec la fonction de récompense $r(s,a) \in \mathbb{R}$ 
tandis que le cadre possibiliste qualitatif utilise des préférences $\rho(s,a) \in \mathcal{L}$ and $\Psi(s) \in \mathcal{L}$.
Ainsi, le critère probabiliste pour une stratégie donnée,
est l'espérance des récompenses, ce qui s'écrit $\mathbb{E} \croch{ rewards\Big( (S_t)_{t\geqslant0} \Big) } \in \mathbb{R}$:
dans le cadre possibiliste, deux critères sont possibles 
qui sont les intégrales de Sugeno de la préférence,
$\mathbb{S}_{\Pi} \croch{ preferences\Big( (S_t)_{t\geqslant0} \Big) } \in \mathcal{L}$ pour l'optimiste,
et $\mathbb{S}_{\mathcal{N}} \croch{ preferences\Big( (S_t)_{t\geqslant0} \Big) } \in \mathcal{L}$ pour la pessimiste.

A PDMPO (resp. $\pi$-PDMPO), est redéfini en termes de MDP (resp. $\pi$-MDP) entièrement observable
où l'état du système est l'état de croyance $b_t \in \mathbb{P}^{\mathcal{S}}_{b_0}$ 
(resp. $\beta_t \in \Pi^{\mathcal{S}}_{\mathcal{L}}$), 
\textit{i.e.} les distribution de probabilité (resp. possibilité)
sur les états du système du PDMPO:
la récompense basée sur la croyance est définie comme étant
$r(b,a) = \mathbb{E}_{S \sim b} \croch{ r(S,a) } = \sum_{s \in \mathcal{S}} r(s,a) \cdot b(s)$
dans le cas probabiliste. 
Dans le cas possibiliste qualitatif, 
la préférence basée sur l'état de croyance
est $\underline{\rho}(b,a) = \mathbb{S}_{\mathcal{N}, S \sim \beta} \croch{ \rho(S,a) } = \min_{s \in \mathcal{S}} \max \set{ \rho(s,a), 1 - \beta(s) }$
pour les pessimistes.

Le chapitre suivant propose certaines amélioration au modèle possibiliste qualitatif:
La propriété d'observabilité mixed est définie:
comme pour le modèle probabiliste,
la complexité de résolution des problèmes ayant cette propriété est réduite.
Enfin, un problème à horizon infini est formellement défini,
et il est prouvé que l'algorithme de résolution proposé pour le résoudre
renvoie une stratégie optimale.
